{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-04-23T20:29:33.693344Z",
     "iopub.status.busy": "2025-04-23T20:29:33.693085Z",
     "iopub.status.idle": "2025-04-23T20:29:47.767534Z",
     "shell.execute_reply": "2025-04-23T20:29:47.766968Z",
     "shell.execute_reply.started": "2025-04-23T20:29:33.693322Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import cv2\n",
    "import glob\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchvision import models\n",
    "from torchvision.models import efficientnet_b0, EfficientNet_B0_Weights\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import random\n",
    "from torch.nn import functional as F\n",
    "import gc  # For explicit garbage collection\n",
    "from sklearn.utils import resample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-23T20:29:47.769164Z",
     "iopub.status.busy": "2025-04-23T20:29:47.768776Z",
     "iopub.status.idle": "2025-04-23T20:29:47.870318Z",
     "shell.execute_reply": "2025-04-23T20:29:47.869744Z",
     "shell.execute_reply.started": "2025-04-23T20:29:47.769139Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Set seeds for reproducibility\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# Define Constants\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "IMG_SIZE = 224\n",
    "BATCH_SIZE = 8  # Reduced from 16 to save memory\n",
    "EPOCHS = 100\n",
    "PATIENCE = 10\n",
    "LEARNING_RATE = 3e-4\n",
    "NUM_FRAMES = 16  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-23T20:29:47.871190Z",
     "iopub.status.busy": "2025-04-23T20:29:47.870995Z",
     "iopub.status.idle": "2025-04-23T20:29:47.875700Z",
     "shell.execute_reply": "2025-04-23T20:29:47.875113Z",
     "shell.execute_reply.started": "2025-04-23T20:29:47.871175Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Dataset paths\n",
    "VIOLENT_PATHS = [\n",
    "    \"/kaggle/input/real-life-violence-situations-dataset/Real Life Violence Dataset/Violence\",\n",
    "    \"/kaggle/input/project-data/Complete Dataset/train/Fight\",\n",
    "    \"/kaggle/input/smartcity-cctv-violence-detection-dataset-scvd/SCVD/SCVD_converted_sec_split/Test/Violence\",\n",
    "    \"/kaggle/input/smartcity-cctv-violence-detection-dataset-scvd/SCVD/SCVD_converted_sec_split/Train/Violence\",\n",
    "    \"/kaggle/input/project-data/Complete Dataset/val/Fight\"\n",
    "]\n",
    "\n",
    "NON_VIOLENT_PATHS = [\n",
    "    \"/kaggle/input/real-life-violence-situations-dataset/Real Life Violence Dataset/NonViolence\",\n",
    "    \"/kaggle/input/project-data/Complete Dataset/train/NonFight\",\n",
    "    \"/kaggle/input/project-data/Complete Dataset/val/NonFight\",\n",
    "    \"/kaggle/input/smartcity-cctv-violence-detection-dataset-scvd/SCVD/SCVD_converted_sec_split/Test/Normal\",\n",
    "    \"/kaggle/input/smartcity-cctv-violence-detection-dataset-scvd/SCVD/SCVD_converted_sec_split/Train/Normal\"\n",
    "]\n",
    "TEST_SIZE = 0.2\n",
    "RANDOM_STATE = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-23T20:29:47.877700Z",
     "iopub.status.busy": "2025-04-23T20:29:47.877501Z",
     "iopub.status.idle": "2025-04-23T20:29:47.907207Z",
     "shell.execute_reply": "2025-04-23T20:29:47.906478Z",
     "shell.execute_reply.started": "2025-04-23T20:29:47.877685Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Memory monitoring function\n",
    "def print_gpu_memory():\n",
    "    if torch.cuda.is_available():\n",
    "        total_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "        reserved = torch.cuda.memory_reserved(0) / 1024**3\n",
    "        allocated = torch.cuda.memory_allocated(0) / 1024**3\n",
    "        free = total_memory - reserved\n",
    "\n",
    "        print(f\"GPU Memory: Total={total_memory:.2f}GB | Reserved={reserved:.2f}GB | Allocated={allocated:.2f}GB | Free={free:.2f}GB\")\n",
    "\n",
    "def clear_gpu_memory():\n",
    "    \"\"\"Clear GPU cache and run garbage collector\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "from sklearn.utils import resample\n",
    "from sklearn.model_selection import train_test_split\n",
    "import glob\n",
    "import os\n",
    "\n",
    "def prepare_dataset():\n",
    "    \"\"\"\n",
    "    Prepare the dataset by gathering all video paths and creating labels.\n",
    "    Returns:\n",
    "        - Training and test splits (video paths and labels)\n",
    "        - Separate hold-out test set (20 videos)\n",
    "    \"\"\"\n",
    "    # List of common video extensions\n",
    "    VIDEO_FORMATS = ['mp4', 'avi', 'mov', 'mkv', 'flv', 'wmv']\n",
    "    \n",
    "    # Gather violent videos and print counts per path\n",
    "    violent_videos = []\n",
    "    print(\"Violent videos per path:\")\n",
    "    for path in VIOLENT_PATHS:\n",
    "        videos_in_path = []\n",
    "        for ext in VIDEO_FORMATS:\n",
    "            videos_in_path.extend(glob.glob(os.path.join(path, f\"*.{ext}\")))\n",
    "        print(f\"- {path}: {len(videos_in_path)} videos\")\n",
    "        violent_videos.extend(videos_in_path)\n",
    "    \n",
    "    # Gather non-violent videos and print counts per path\n",
    "    non_violent_videos = []\n",
    "    print(\"\\nNon-violent videos per path:\")\n",
    "    for path in NON_VIOLENT_PATHS:\n",
    "        videos_in_path = []\n",
    "        for ext in VIDEO_FORMATS:\n",
    "            videos_in_path.extend(glob.glob(os.path.join(path, f\"*.{ext}\")))\n",
    "        print(f\"- {path}: {len(videos_in_path)} videos\")\n",
    "        non_violent_videos.extend(videos_in_path)\n",
    "    \n",
    "    # Reserve 20 videos (10 violent + 10 non-violent) for separate testing\n",
    "    holdout_violent = resample(violent_videos, replace=False, n_samples=10, random_state=42)\n",
    "    holdout_non_violent = resample(non_violent_videos, replace=False, n_samples=10, random_state=42)\n",
    "    holdout_paths = holdout_violent + holdout_non_violent\n",
    "    holdout_labels = [1] * 10 + [0] * 10\n",
    "\n",
    "    # Remove holdout videos from the main dataset\n",
    "    violent_videos = [v for v in violent_videos if v not in holdout_violent]\n",
    "    non_violent_videos = [v for v in non_violent_videos if v not in holdout_non_violent]\n",
    "\n",
    "    # Balance the remaining dataset\n",
    "    def balance_dataset(violent, non_violent):\n",
    "        min_size = min(len(violent), len(non_violent))\n",
    "        violent_balanced = resample(violent, replace=False, n_samples=min_size, random_state=42)\n",
    "        non_violent_balanced = resample(non_violent, replace=False, n_samples=min_size, random_state=42)\n",
    "        return violent_balanced, non_violent_balanced\n",
    "\n",
    "    violent_videos, non_violent_videos = balance_dataset(violent_videos, non_violent_videos)\n",
    "\n",
    "    # Create paths and labels for the main dataset\n",
    "    all_video_paths = violent_videos + non_violent_videos\n",
    "    all_labels = [1] * len(violent_videos) + [0] * len(non_violent_videos)\n",
    "\n",
    "    # Print dataset statistics\n",
    "    print(f\"\\nMain dataset after balancing:\")\n",
    "    print(f\"- Violent videos: {len(violent_videos)}\")\n",
    "    print(f\"- Non-violent videos: {len(non_violent_videos)}\")\n",
    "    print(f\"- Total: {len(all_video_paths)} videos\")\n",
    "    print(f\"\\nHold-out test set: {len(holdout_paths)} videos (10 violent + 10 non-violent)\")\n",
    "\n",
    "    # Train-test split (stratified)\n",
    "    video_paths_train, video_paths_test, labels_train, labels_test = train_test_split(\n",
    "        all_video_paths, all_labels, test_size=TEST_SIZE, random_state=RANDOM_STATE, stratify=all_labels\n",
    "    )\n",
    "\n",
    "    print(f\"\\nTraining set: {len(video_paths_train)} videos\")\n",
    "    print(f\"Test set (main): {len(video_paths_test)} videos\")\n",
    "\n",
    "    return (\n",
    "        video_paths_train, video_paths_test, labels_train, labels_test,\n",
    "        holdout_paths, holdout_labels\n",
    "    )\n",
    "\n",
    "# Simplified data augmentation to reduce computation\n",
    "class VideoAugmentation:\n",
    "    def __init__(self, p=0.5):\n",
    "        self.p = p\n",
    "\n",
    "    def random_flip(self, frames):\n",
    "        if random.random() < self.p:\n",
    "            return torch.flip(frames, [3])  # Horizontal flip\n",
    "        return frames\n",
    "\n",
    "    def random_crop_resize(self, frames):\n",
    "        if random.random() < self.p:\n",
    "            h, w = frames.shape[2], frames.shape[3]\n",
    "            new_h, new_w = int(h * 0.8), int(w * 0.8)\n",
    "\n",
    "            # Random crop coordinates\n",
    "            top = random.randint(0, h - new_h)\n",
    "            left = random.randint(0, w - new_w)\n",
    "\n",
    "            # Apply crop and resize to all frames at once using slicing\n",
    "            cropped = frames[:, :, top:top+new_h, left:left+new_w]\n",
    "            resized = F.interpolate(cropped, size=(h, w), mode='bilinear', align_corners=False)\n",
    "            return resized\n",
    "        return frames\n",
    "\n",
    "    def __call__(self, frames):\n",
    "        frames = self.random_flip(frames)\n",
    "        frames = self.random_crop_resize(frames)\n",
    "        return frames\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-23T20:29:47.908292Z",
     "iopub.status.busy": "2025-04-23T20:29:47.908113Z",
     "iopub.status.idle": "2025-04-23T20:29:47.928586Z",
     "shell.execute_reply": "2025-04-23T20:29:47.927897Z",
     "shell.execute_reply.started": "2025-04-23T20:29:47.908277Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "class ViolenceDataset(Dataset):\n",
    "    def __init__(self, video_paths, labels, transform=None, is_train=False):\n",
    "        self.video_paths = video_paths\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "        self.is_train = is_train\n",
    "        self.augmentation = VideoAugmentation() if is_train else None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.video_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        video_path = self.video_paths[idx]\n",
    "\n",
    "        # Open video\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        frames = []\n",
    "        frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "        # Uniform sampling: choose frames at regular intervals\n",
    "        if frame_count <= NUM_FRAMES:\n",
    "            # If video has fewer frames than needed, duplicate frames\n",
    "            indices = list(range(frame_count)) + [frame_count-1] * (NUM_FRAMES - frame_count)\n",
    "        else:\n",
    "            # Otherwise, sample uniformly\n",
    "            indices = np.linspace(0, frame_count - 1, NUM_FRAMES, dtype=int)\n",
    "\n",
    "        # Process each frame\n",
    "        success = True\n",
    "        frame_idx = 0\n",
    "\n",
    "        while success and len(frames) < NUM_FRAMES:\n",
    "            success, frame = cap.read()\n",
    "            if not success:\n",
    "                break\n",
    "\n",
    "            if frame_idx in indices:\n",
    "                # Preprocess the frame\n",
    "                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)  # Convert to RGB\n",
    "                frame = cv2.resize(frame, (IMG_SIZE, IMG_SIZE))\n",
    "                frame = torch.tensor(frame).permute(2, 0, 1).float() / 255.0\n",
    "                frames.append(frame)\n",
    "\n",
    "            frame_idx += 1\n",
    "\n",
    "        cap.release()\n",
    "\n",
    "        # Handle case of zero frames (corrupted video)\n",
    "        if len(frames) == 0:\n",
    "            # Create a blank frame\n",
    "            blank_frame = torch.zeros(3, IMG_SIZE, IMG_SIZE)\n",
    "            frames = [blank_frame] * NUM_FRAMES\n",
    "        # Handle case of fewer frames than required\n",
    "        elif len(frames) < NUM_FRAMES:\n",
    "            # Duplicate the last frame to reach NUM_FRAMES\n",
    "            last_frame = frames[-1]\n",
    "            frames.extend([last_frame] * (NUM_FRAMES - len(frames)))\n",
    "\n",
    "        # Stack frames and apply augmentation if in training mode\n",
    "        frames = torch.stack(frames)\n",
    "        if self.is_train and self.augmentation:\n",
    "            frames = self.augmentation(frames)\n",
    "\n",
    "        label = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return frames, label\n",
    "\n",
    "# Custom collate function for batching\n",
    "def custom_collate(batch):\n",
    "    frames = torch.stack([item[0] for item in batch])  # Shape: [B, T, C, H, W]\n",
    "    labels = torch.stack([item[1] for item in batch])\n",
    "    return frames, labels\n",
    "\n",
    "# Memory-efficient feature extractor using EfficientNet\n",
    "from torchvision.models import convnext_tiny, ConvNeXt_Tiny_Weights\n",
    "\n",
    "class FeatureExtractor(nn.Module):\n",
    "    def __init__(self, pretrained=True):\n",
    "        super(FeatureExtractor, self).__init__()\n",
    "        # Use ConvNeXt instead of EfficientNet\n",
    "        weights = ConvNeXt_Tiny_Weights.IMAGENET1K_V1 if pretrained else None\n",
    "        base_model = convnext_tiny(weights=weights)\n",
    "        # Remove the classification layer\n",
    "        self.model = nn.Sequential(*list(base_model.children())[:-1])\n",
    "        self.feature_dim = 768  # ConvNeXt tiny output dimension\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, c, h, w = x.shape\n",
    "\n",
    "        # Process frames in chunks to save memory\n",
    "        features = []\n",
    "        chunk_size = 4  # Process 4 frames at a time\n",
    "\n",
    "        for i in range(0, seq_len, chunk_size):\n",
    "            end_idx = min(i + chunk_size, seq_len)\n",
    "            chunk = x[:, i:end_idx].reshape(-1, c, h, w)  # Reshape chunk\n",
    "\n",
    "            # Extract features\n",
    "            with torch.cuda.amp.autocast(enabled=True):  # Use mixed precision\n",
    "                chunk_features = self.model(chunk)\n",
    "\n",
    "            # Reshape back\n",
    "            chunk_features = chunk_features.reshape(batch_size, end_idx - i, -1)\n",
    "            features.append(chunk_features)\n",
    "\n",
    "            # Clear unnecessary tensors to free memory\n",
    "            del chunk, chunk_features\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "        # Concatenate chunks\n",
    "        x = torch.cat(features, dim=1)\n",
    "        return x\n",
    "\n",
    "# Improved temporal modeling with reduced complexity\n",
    "class TemporalAttention(nn.Module):\n",
    "    def __init__(self, feature_dim, num_heads=4, dropout=0.1):  # Reduced heads\n",
    "        super(TemporalAttention, self).__init__()\n",
    "        self.self_attn = nn.MultiheadAttention(feature_dim, num_heads, dropout=dropout, batch_first=True)\n",
    "        self.norm1 = nn.LayerNorm(feature_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply self-attention with residual connection\n",
    "        attn_output, _ = self.self_attn(x, x, x)\n",
    "        x = x + self.dropout(attn_output)\n",
    "        x = self.norm1(x)\n",
    "        return x\n",
    "\n",
    "# Simplified sequence model\n",
    "class TemporalSequenceModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=384, num_layers=1, dropout=0.5):  # Reduced complexity\n",
    "        super(TemporalSequenceModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True, dropout=0)\n",
    "        self.attention = TemporalAttention(hidden_dim)\n",
    "        self.fc = nn.Linear(hidden_dim, 2)  # 2 classes\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.norm = nn.LayerNorm(hidden_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Process with LSTM\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "\n",
    "        # Apply attention\n",
    "        attn_out = self.attention(lstm_out)\n",
    "\n",
    "        # Global temporal pooling\n",
    "        pooled = torch.mean(attn_out, dim=1)\n",
    "\n",
    "        # Apply normalization and dropout\n",
    "        pooled = self.norm(pooled)\n",
    "        pooled = self.dropout(pooled)\n",
    "\n",
    "        # Final classification\n",
    "        x = self.fc(pooled)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-23T20:29:47.929665Z",
     "iopub.status.busy": "2025-04-23T20:29:47.929356Z",
     "iopub.status.idle": "2025-04-23T20:29:47.956829Z",
     "shell.execute_reply": "2025-04-23T20:29:47.956157Z",
     "shell.execute_reply.started": "2025-04-23T20:29:47.929645Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "class ViolenceDetectionModel(nn.Module):\n",
    "    def __init__(self, feature_dim=768):  # Updated feature dim for ConvNeXt tiny\n",
    "        super(ViolenceDetectionModel, self).__init__()\n",
    "        self.feature_extractor = FeatureExtractor(pretrained=True)\n",
    "        self.feature_dim = feature_dim\n",
    "        self.sequence_model = TemporalSequenceModel(input_dim=feature_dim)\n",
    "        \n",
    "       \n",
    "\n",
    "        # Freeze feature extractor layers for initial training\n",
    "        for param in self.feature_extractor.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.feature_extractor(x)\n",
    "        prediction = self.sequence_model(features)\n",
    "        return prediction\n",
    "\n",
    "    # Progressive unfreezing to reduce memory impact\n",
    "    def unfreeze_last_n_layers(self, n=3):\n",
    "        \"\"\"Unfreeze only the last n layers of the feature extractor\"\"\"\n",
    "        total_layers = len(list(self.feature_extractor.model.children()))\n",
    "        print(f\"Unfreezing last {n} of {total_layers} feature extractor layers\")\n",
    "\n",
    "        for i, child in enumerate(self.feature_extractor.model.children()):\n",
    "            if i >= total_layers - n:\n",
    "                for param in child.parameters():\n",
    "                    param.requires_grad = True\n",
    "            else:\n",
    "                for param in child.parameters():\n",
    "                    param.requires_grad = False\n",
    "\n",
    "def create_data_loaders(video_paths_train, labels_train, video_paths_test, labels_test, batch_size=BATCH_SIZE):\n",
    "    train_dataset = ViolenceDataset(video_paths_train, labels_train, is_train=True)\n",
    "    test_dataset = ViolenceDataset(video_paths_test, labels_test, is_train=False)\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        collate_fn=custom_collate,\n",
    "        num_workers=2,  # Reduced workers\n",
    "        pin_memory=True,\n",
    "        drop_last=True\n",
    "    )\n",
    "\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        collate_fn=custom_collate,\n",
    "        num_workers=2,  # Reduced workers\n",
    "        pin_memory=True\n",
    "    )\n",
    "\n",
    "    return train_loader, test_loader\n",
    "\n",
    "def train(model, train_loader, test_loader, epochs=EPOCHS):\n",
    "    model.to(device)\n",
    "\n",
    "    # Initialize mixed precision training\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=torch.cuda.is_available())\n",
    "\n",
    "    # Class weights (if needed)\n",
    "    class_weights = torch.tensor([1.0, 1.0], device=device)\n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "    \n",
    "    # Progressive unfreezing schedule\n",
    "    unfreeze_schedule = {\n",
    "        10: 1,  # At epoch 10, unfreeze last layer\n",
    "        15: 2,  # At epoch 15, unfreeze last 2 layers\n",
    "        20: 3   # At epoch 20, unfreeze last 3 layers\n",
    "    }\n",
    "\n",
    "    # Optimizer with differential learning rates\n",
    "    optimizer = optim.AdamW([\n",
    "        {'params': model.sequence_model.parameters(), 'lr': LEARNING_RATE},\n",
    "        {'params': model.feature_extractor.parameters(), 'lr': LEARNING_RATE/10}\n",
    "    ], weight_decay=1e-4)\n",
    "\n",
    "    # Learning rate scheduler\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='max', factor=0.5, patience=5, verbose=True\n",
    "    )\n",
    "\n",
    "    best_val_f1 = 0.0\n",
    "    patience_counter = 0\n",
    "\n",
    "    # Checkpoint handling\n",
    "    start_epoch = 0\n",
    "    if os.path.exists('last_checkpoint.pth'):\n",
    "        checkpoint = torch.load('last_checkpoint.pth')\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        start_epoch = checkpoint['epoch'] + 1\n",
    "        best_val_f1 = checkpoint.get('f1', 0.0)\n",
    "        print(f\"\\nResuming training from epoch {start_epoch}\")\n",
    "        print(f\"Previous best F1: {best_val_f1:.4f}\")\n",
    "\n",
    "    print(f\"Training on {device}\")\n",
    "    print_gpu_memory()\n",
    "\n",
    "    # Training history\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'train_acc': [],\n",
    "        'val_loss': [],\n",
    "        'val_acc': [],\n",
    "        'val_f1': []\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        for epoch in range(start_epoch, epochs):\n",
    "            # Progressive unfreezing\n",
    "            if epoch in unfreeze_schedule:\n",
    "                model.unfreeze_last_n_layers(unfreeze_schedule[epoch])\n",
    "                print_gpu_memory()\n",
    "\n",
    "            # Training phase\n",
    "            model.train()\n",
    "            total_loss = 0\n",
    "            correct = 0\n",
    "            total = 0\n",
    "\n",
    "            print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "\n",
    "            for i, batch in enumerate(train_loader):\n",
    "                try:\n",
    "                    inputs, labels = batch\n",
    "                    inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "                    # Mixed precision training\n",
    "                    with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n",
    "                        outputs = model(inputs)\n",
    "                        loss = criterion(outputs, labels)\n",
    "\n",
    "                    optimizer.zero_grad()\n",
    "                    scaler.scale(loss).backward()\n",
    "                    scaler.unscale_(optimizer)\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                    scaler.step(optimizer)\n",
    "                    scaler.update()\n",
    "\n",
    "                    total_loss += loss.item()\n",
    "                    _, predicted = torch.max(outputs.data, 1)\n",
    "                    correct += (predicted == labels).sum().item()\n",
    "                    total += labels.size(0)\n",
    "\n",
    "                    # Print progress and monitor memory\n",
    "                    if (i+1) % 10 == 0:\n",
    "                        print(f\"  Batch {i+1}/{len(train_loader)}, Loss: {loss.item():.4f}\")\n",
    "                        if (i+1) % 50 == 0:\n",
    "                            print_gpu_memory()\n",
    "                            clear_gpu_memory()\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Error in batch {i}: {str(e)}\")\n",
    "                    # Save checkpoint before continuing\n",
    "                    torch.save({\n",
    "                        'epoch': epoch,\n",
    "                        'model_state_dict': model.state_dict(),\n",
    "                        'optimizer_state_dict': optimizer.state_dict(),\n",
    "                        'loss': total_loss/(i+1),\n",
    "                        'f1': best_val_f1\n",
    "                    }, 'last_checkpoint.pth')\n",
    "                    continue\n",
    "\n",
    "            # Save checkpoint at end of each epoch\n",
    "            avg_loss = total_loss / len(train_loader)\n",
    "            accuracy = correct / total\n",
    "            history['train_loss'].append(avg_loss)\n",
    "            history['train_acc'].append(accuracy)\n",
    "\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': avg_loss,\n",
    "                'f1': best_val_f1\n",
    "            }, 'last_checkpoint.pth')\n",
    "\n",
    "            # Validation phase\n",
    "            print(\"Starting validation...\")\n",
    "            val_metrics = evaluate(model, test_loader, criterion)\n",
    "            val_loss = val_metrics['loss']\n",
    "            val_accuracy = val_metrics['accuracy']\n",
    "            val_f1 = val_metrics['f1']\n",
    "\n",
    "            # Update history\n",
    "            history['val_loss'].append(val_loss)\n",
    "            history['val_acc'].append(val_accuracy)\n",
    "            history['val_f1'].append(val_f1)\n",
    "\n",
    "            # Update learning rate and check early stopping\n",
    "            scheduler.step(val_f1)\n",
    "            if val_f1 > best_val_f1:\n",
    "                best_val_f1 = val_f1\n",
    "                patience_counter = 0\n",
    "                torch.save({\n",
    "                    'epoch': epoch,\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'loss': val_loss,\n",
    "                    'accuracy': val_accuracy,\n",
    "                    'f1': val_f1\n",
    "                }, 'best_violence_detector.pth')\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                if patience_counter >= PATIENCE:\n",
    "                    print(f\"Early stopping triggered after {epoch+1} epochs\")\n",
    "                    break\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nTraining interrupted by user\")\n",
    "        # Save the current state before exiting\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': avg_loss if 'avg_loss' in locals() else None,\n",
    "            'f1': best_val_f1\n",
    "        }, 'interrupted_checkpoint.pth')\n",
    "        print(\"Saved interrupted training state to 'interrupted_checkpoint.pth'\")\n",
    "        return history\n",
    "\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-23T20:29:47.957904Z",
     "iopub.status.busy": "2025-04-23T20:29:47.957685Z",
     "iopub.status.idle": "2025-04-23T20:29:47.979745Z",
     "shell.execute_reply": "2025-04-23T20:29:47.979114Z",
     "shell.execute_reply.started": "2025-04-23T20:29:47.957873Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def evaluate(model, data_loader, criterion=None):\n",
    "    \"\"\"\n",
    "    Evaluate the model on the provided data loader.\n",
    "    Returns a dictionary with various performance metrics.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    if criterion is None:\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    all_probs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            inputs, labels = batch\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            # Use mixed precision for inference\n",
    "            with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Get predictions and probabilities\n",
    "            probs = F.softmax(outputs, dim=1)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_probs.extend(probs[:, 1].cpu().numpy())\n",
    "\n",
    "            # Free memory\n",
    "            del inputs, outputs, labels\n",
    "\n",
    "    # Convert to numpy arrays for metrics calculation\n",
    "    all_preds = np.array(all_preds)\n",
    "    all_labels = np.array(all_labels)\n",
    "    all_probs = np.array(all_probs)\n",
    "\n",
    "    # Calculate metrics\n",
    "    accuracy = (all_preds == all_labels).mean()\n",
    "\n",
    "    # Get classification report\n",
    "    report = classification_report(all_labels, all_preds,\n",
    "                                  target_names=['Non-Violent', 'Violent'],\n",
    "                                  output_dict=True)\n",
    "\n",
    "    # Extract F1 score for the positive class (violence)\n",
    "    f1 = report['Violent']['f1-score']\n",
    "\n",
    "    # Calculate confusion matrix\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "    # Calculate ROC and AUC\n",
    "    fpr, tpr, _ = roc_curve(all_labels, all_probs)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    return {\n",
    "        'loss': total_loss / len(data_loader),\n",
    "        'accuracy': accuracy,\n",
    "        'f1': f1,\n",
    "        'report': report,\n",
    "        'confusion_matrix': cm,\n",
    "        'predictions': all_preds,\n",
    "        'labels': all_labels,\n",
    "        'probabilities': all_probs,\n",
    "        'roc': {'fpr': fpr, 'tpr': tpr, 'auc': roc_auc}\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-23T20:29:47.980952Z",
     "iopub.status.busy": "2025-04-23T20:29:47.980680Z",
     "iopub.status.idle": "2025-04-23T20:29:48.005445Z",
     "shell.execute_reply": "2025-04-23T20:29:48.004958Z",
     "shell.execute_reply.started": "2025-04-23T20:29:47.980911Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def plot_training_history(history):\n",
    "    \"\"\"\n",
    "    Plot training and validation metrics.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(12, 10))\n",
    "\n",
    "    # Plot loss\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.plot(history['train_loss'], label='Train Loss')\n",
    "    plt.plot(history['val_loss'], label='Validation Loss')\n",
    "    plt.title('Loss Over Epochs')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot accuracy\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.plot(history['train_acc'], label='Train Accuracy')\n",
    "    plt.plot(history['val_acc'], label='Validation Accuracy')\n",
    "    plt.title('Accuracy Over Epochs')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot F1 score\n",
    "    plt.subplot(2, 2, 3)\n",
    "    plt.plot(history['val_f1'], label='Validation F1 Score')\n",
    "    plt.title('F1 Score Over Epochs')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('F1 Score')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('training_history.png')\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-23T20:29:48.006447Z",
     "iopub.status.busy": "2025-04-23T20:29:48.006218Z",
     "iopub.status.idle": "2025-04-23T20:29:48.023022Z",
     "shell.execute_reply": "2025-04-23T20:29:48.022533Z",
     "shell.execute_reply.started": "2025-04-23T20:29:48.006422Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes=['Non-Violent', 'Violent']):\n",
    "    \"\"\"\n",
    "    Plot confusion matrix as a heatmap.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=classes, yticklabels=classes)\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('confusion_matrix.png')\n",
    "    plt.close()\n",
    "\n",
    "def plot_roc_curve(fpr, tpr, roc_auc):\n",
    "    \"\"\"\n",
    "    Plot ROC curve.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('roc_curve.png')\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-23T20:29:48.025268Z",
     "iopub.status.busy": "2025-04-23T20:29:48.024762Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing dataset...\n",
      "Violent videos per path:\n",
      "- /kaggle/input/real-life-violence-situations-dataset/Real Life Violence Dataset/Violence: 1000 videos\n",
      "- /kaggle/input/project-data/Complete Dataset/train/Fight: 1000 videos\n",
      "- /kaggle/input/smartcity-cctv-violence-detection-dataset-scvd/SCVD/SCVD_converted_sec_split/Test/Violence: 118 videos\n",
      "- /kaggle/input/smartcity-cctv-violence-detection-dataset-scvd/SCVD/SCVD_converted_sec_split/Train/Violence: 970 videos\n",
      "- /kaggle/input/project-data/Complete Dataset/val/Fight: 275 videos\n",
      "\n",
      "Non-violent videos per path:\n",
      "- /kaggle/input/real-life-violence-situations-dataset/Real Life Violence Dataset/NonViolence: 1000 videos\n",
      "- /kaggle/input/project-data/Complete Dataset/train/NonFight: 1000 videos\n",
      "- /kaggle/input/project-data/Complete Dataset/val/NonFight: 275 videos\n",
      "- /kaggle/input/smartcity-cctv-violence-detection-dataset-scvd/SCVD/SCVD_converted_sec_split/Test/Normal: 169 videos\n",
      "- /kaggle/input/smartcity-cctv-violence-detection-dataset-scvd/SCVD/SCVD_converted_sec_split/Train/Normal: 872 videos\n",
      "\n",
      "Main dataset after balancing:\n",
      "- Violent videos: 3306\n",
      "- Non-violent videos: 3306\n",
      "- Total: 6612 videos\n",
      "\n",
      "Hold-out test set: 20 videos (10 violent + 10 non-violent)\n",
      "\n",
      "Training set: 5289 videos\n",
      "Test set (main): 1323 videos\n",
      "\n",
      "Creating holdout videos archive...\n",
      "Saved holdout videos to holdout_videos_20250423_202948.zip\n",
      "\n",
      "Creating data loaders...\n",
      "Initializing model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/convnext_tiny-983f1562.pth\" to /root/.cache/torch/hub/checkpoints/convnext_tiny-983f1562.pth\n",
      "100%|██████████| 109M/109M [00:00<00:00, 164MB/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Summary:\n",
      "Total parameters: 30,184,802\n",
      "Trainable parameters: 2,366,210\n",
      "GPU Memory: Total=14.74GB | Reserved=0.00GB | Allocated=0.00GB | Free=14.74GB\n",
      "\n",
      "Starting training...\n",
      "Training on cuda\n",
      "GPU Memory: Total=14.74GB | Reserved=0.14GB | Allocated=0.11GB | Free=14.60GB\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31/4126126155.py:62: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler(enabled=torch.cuda.is_available())\n",
      "/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_31/4126126155.py:134: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n",
      "/tmp/ipykernel_31/169455789.py:98: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=True):  # Use mixed precision\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 10/661, Loss: 0.9940\n",
      "  Batch 20/661, Loss: 0.6970\n",
      "  Batch 30/661, Loss: 0.6456\n",
      "  Batch 40/661, Loss: 0.5927\n",
      "  Batch 50/661, Loss: 0.6321\n",
      "GPU Memory: Total=14.74GB | Reserved=0.23GB | Allocated=0.20GB | Free=14.51GB\n",
      "  Batch 60/661, Loss: 0.6241\n",
      "  Batch 70/661, Loss: 0.4130\n",
      "  Batch 80/661, Loss: 0.8190\n",
      "  Batch 90/661, Loss: 0.5242\n",
      "  Batch 100/661, Loss: 0.4569\n",
      "GPU Memory: Total=14.74GB | Reserved=0.23GB | Allocated=0.20GB | Free=14.51GB\n",
      "  Batch 110/661, Loss: 0.3098\n",
      "  Batch 120/661, Loss: 0.4896\n",
      "  Batch 130/661, Loss: 0.6788\n",
      "  Batch 140/661, Loss: 0.4292\n",
      "  Batch 150/661, Loss: 0.4810\n",
      "GPU Memory: Total=14.74GB | Reserved=0.23GB | Allocated=0.20GB | Free=14.51GB\n",
      "  Batch 160/661, Loss: 0.4013\n",
      "  Batch 170/661, Loss: 0.2721\n",
      "  Batch 180/661, Loss: 0.3741\n",
      "  Batch 190/661, Loss: 0.3125\n",
      "  Batch 200/661, Loss: 0.3501\n",
      "GPU Memory: Total=14.74GB | Reserved=0.23GB | Allocated=0.20GB | Free=14.51GB\n",
      "  Batch 210/661, Loss: 0.2484\n",
      "  Batch 220/661, Loss: 0.5272\n",
      "  Batch 230/661, Loss: 0.3967\n",
      "  Batch 240/661, Loss: 1.1380\n",
      "  Batch 250/661, Loss: 1.0707\n",
      "GPU Memory: Total=14.74GB | Reserved=0.23GB | Allocated=0.20GB | Free=14.51GB\n",
      "  Batch 260/661, Loss: 0.2761\n",
      "  Batch 270/661, Loss: 0.2812\n",
      "  Batch 280/661, Loss: 0.6780\n",
      "  Batch 290/661, Loss: 0.7564\n",
      "  Batch 300/661, Loss: 0.6299\n",
      "GPU Memory: Total=14.74GB | Reserved=0.23GB | Allocated=0.20GB | Free=14.51GB\n",
      "  Batch 310/661, Loss: 0.6841\n",
      "  Batch 320/661, Loss: 0.1463\n",
      "  Batch 330/661, Loss: 1.1029\n",
      "  Batch 340/661, Loss: 0.4619\n",
      "  Batch 350/661, Loss: 0.0507\n",
      "GPU Memory: Total=14.74GB | Reserved=0.23GB | Allocated=0.20GB | Free=14.51GB\n",
      "  Batch 360/661, Loss: 0.3442\n",
      "  Batch 370/661, Loss: 0.8624\n",
      "  Batch 380/661, Loss: 1.3124\n",
      "  Batch 390/661, Loss: 0.1552\n",
      "  Batch 400/661, Loss: 0.4266\n",
      "GPU Memory: Total=14.74GB | Reserved=0.23GB | Allocated=0.20GB | Free=14.51GB\n",
      "  Batch 410/661, Loss: 0.7427\n",
      "  Batch 420/661, Loss: 0.0696\n",
      "  Batch 430/661, Loss: 0.6358\n",
      "  Batch 440/661, Loss: 0.4230\n",
      "  Batch 450/661, Loss: 0.7758\n",
      "GPU Memory: Total=14.74GB | Reserved=0.23GB | Allocated=0.20GB | Free=14.51GB\n",
      "  Batch 460/661, Loss: 0.4857\n",
      "  Batch 470/661, Loss: 0.2946\n",
      "  Batch 480/661, Loss: 0.5136\n",
      "  Batch 490/661, Loss: 0.8510\n",
      "  Batch 500/661, Loss: 0.1374\n",
      "GPU Memory: Total=14.74GB | Reserved=0.23GB | Allocated=0.20GB | Free=14.51GB\n",
      "  Batch 510/661, Loss: 0.0374\n",
      "  Batch 520/661, Loss: 0.0911\n",
      "  Batch 530/661, Loss: 1.6206\n",
      "  Batch 540/661, Loss: 0.1166\n",
      "  Batch 550/661, Loss: 0.0822\n",
      "GPU Memory: Total=14.74GB | Reserved=0.23GB | Allocated=0.20GB | Free=14.51GB\n",
      "  Batch 560/661, Loss: 0.3758\n",
      "  Batch 570/661, Loss: 0.5946\n",
      "  Batch 580/661, Loss: 0.4565\n",
      "  Batch 590/661, Loss: 0.6349\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[h264 @ 0x408105c0] mb_type 104 in P slice too large at 98 31\n",
      "[h264 @ 0x408105c0] error while decoding MB 98 31\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 600/661, Loss: 0.3208\n",
      "GPU Memory: Total=14.74GB | Reserved=0.23GB | Allocated=0.20GB | Free=14.51GB\n",
      "  Batch 610/661, Loss: 0.2521\n",
      "  Batch 620/661, Loss: 0.2900\n",
      "  Batch 630/661, Loss: 0.3890\n",
      "  Batch 640/661, Loss: 0.7743\n",
      "  Batch 650/661, Loss: 0.4019\n",
      "GPU Memory: Total=14.74GB | Reserved=0.23GB | Allocated=0.20GB | Free=14.51GB\n",
      "  Batch 660/661, Loss: 0.4018\n",
      "Starting validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31/1455025713.py:22: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31/4126126155.py:134: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n",
      "/tmp/ipykernel_31/169455789.py:98: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=True):  # Use mixed precision\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 10/661, Loss: 0.3909\n",
      "  Batch 20/661, Loss: 0.4872\n",
      "  Batch 30/661, Loss: 0.4345\n",
      "  Batch 40/661, Loss: 0.2666\n",
      "  Batch 50/661, Loss: 0.7270\n",
      "GPU Memory: Total=14.74GB | Reserved=0.23GB | Allocated=0.20GB | Free=14.51GB\n",
      "  Batch 60/661, Loss: 0.1355\n",
      "  Batch 70/661, Loss: 1.3810\n",
      "  Batch 80/661, Loss: 0.5747\n",
      "  Batch 90/661, Loss: 0.5253\n",
      "  Batch 100/661, Loss: 0.3506\n",
      "GPU Memory: Total=14.74GB | Reserved=0.23GB | Allocated=0.20GB | Free=14.51GB\n",
      "  Batch 110/661, Loss: 0.1398\n",
      "  Batch 120/661, Loss: 0.4477\n",
      "  Batch 130/661, Loss: 0.3977\n",
      "  Batch 140/661, Loss: 0.1447\n",
      "  Batch 150/661, Loss: 0.3361\n",
      "GPU Memory: Total=14.74GB | Reserved=0.23GB | Allocated=0.20GB | Free=14.51GB\n",
      "  Batch 160/661, Loss: 0.7434\n",
      "  Batch 170/661, Loss: 0.3684\n",
      "  Batch 180/661, Loss: 0.8329\n",
      "  Batch 190/661, Loss: 0.0415\n",
      "  Batch 200/661, Loss: 0.3129\n",
      "GPU Memory: Total=14.74GB | Reserved=0.23GB | Allocated=0.20GB | Free=14.51GB\n",
      "  Batch 210/661, Loss: 0.6646\n",
      "  Batch 220/661, Loss: 0.3541\n",
      "  Batch 230/661, Loss: 0.1348\n",
      "  Batch 240/661, Loss: 0.2084\n",
      "  Batch 250/661, Loss: 0.3032\n",
      "GPU Memory: Total=14.74GB | Reserved=0.23GB | Allocated=0.20GB | Free=14.51GB\n",
      "  Batch 260/661, Loss: 0.3090\n",
      "  Batch 270/661, Loss: 0.4898\n",
      "  Batch 280/661, Loss: 0.1758\n",
      "  Batch 290/661, Loss: 0.2848\n",
      "  Batch 300/661, Loss: 0.6679\n",
      "GPU Memory: Total=14.74GB | Reserved=0.23GB | Allocated=0.20GB | Free=14.51GB\n",
      "  Batch 310/661, Loss: 0.1663\n",
      "  Batch 320/661, Loss: 0.6096\n",
      "  Batch 330/661, Loss: 0.4399\n",
      "  Batch 340/661, Loss: 0.2993\n",
      "  Batch 350/661, Loss: 0.5873\n",
      "GPU Memory: Total=14.74GB | Reserved=0.23GB | Allocated=0.20GB | Free=14.51GB\n",
      "  Batch 360/661, Loss: 0.2161\n",
      "  Batch 370/661, Loss: 0.1295\n",
      "  Batch 380/661, Loss: 0.1180\n",
      "  Batch 390/661, Loss: 0.5859\n",
      "  Batch 400/661, Loss: 0.4016\n",
      "GPU Memory: Total=14.74GB | Reserved=0.23GB | Allocated=0.20GB | Free=14.51GB\n",
      "  Batch 410/661, Loss: 0.5160\n",
      "  Batch 420/661, Loss: 0.3489\n",
      "  Batch 430/661, Loss: 0.1861\n",
      "  Batch 440/661, Loss: 0.0667\n",
      "  Batch 450/661, Loss: 0.4526\n",
      "GPU Memory: Total=14.74GB | Reserved=0.23GB | Allocated=0.20GB | Free=14.51GB\n",
      "  Batch 460/661, Loss: 0.2888\n",
      "  Batch 470/661, Loss: 0.4325\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[h264 @ 0x5f656300] mb_type 104 in P slice too large at 98 31\n",
      "[h264 @ 0x5f656300] error while decoding MB 98 31\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 480/661, Loss: 0.2162\n",
      "  Batch 490/661, Loss: 0.1288\n",
      "  Batch 500/661, Loss: 0.2937\n",
      "GPU Memory: Total=14.74GB | Reserved=0.23GB | Allocated=0.20GB | Free=14.51GB\n",
      "  Batch 510/661, Loss: 0.6540\n",
      "  Batch 520/661, Loss: 0.1398\n",
      "  Batch 530/661, Loss: 0.1184\n",
      "  Batch 540/661, Loss: 0.5727\n",
      "  Batch 550/661, Loss: 0.5860\n",
      "GPU Memory: Total=14.74GB | Reserved=0.23GB | Allocated=0.20GB | Free=14.51GB\n",
      "  Batch 560/661, Loss: 0.1519\n",
      "  Batch 570/661, Loss: 0.4980\n",
      "  Batch 580/661, Loss: 0.2986\n",
      "  Batch 590/661, Loss: 0.4384\n",
      "  Batch 600/661, Loss: 0.3499\n",
      "GPU Memory: Total=14.74GB | Reserved=0.23GB | Allocated=0.20GB | Free=14.51GB\n",
      "  Batch 610/661, Loss: 0.4581\n",
      "  Batch 620/661, Loss: 0.2655\n",
      "  Batch 630/661, Loss: 0.3583\n",
      "  Batch 640/661, Loss: 0.2973\n",
      "  Batch 650/661, Loss: 0.2796\n",
      "GPU Memory: Total=14.74GB | Reserved=0.23GB | Allocated=0.20GB | Free=14.51GB\n",
      "  Batch 660/661, Loss: 0.0713\n",
      "Starting validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31/1455025713.py:22: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31/4126126155.py:134: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n",
      "/tmp/ipykernel_31/169455789.py:98: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=True):  # Use mixed precision\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 10/661, Loss: 0.9813\n",
      "  Batch 20/661, Loss: 0.5224\n",
      "  Batch 30/661, Loss: 0.6519\n",
      "  Batch 40/661, Loss: 0.0540\n",
      "  Batch 50/661, Loss: 0.0185\n",
      "GPU Memory: Total=14.74GB | Reserved=0.23GB | Allocated=0.20GB | Free=14.51GB\n",
      "  Batch 60/661, Loss: 0.0054\n",
      "  Batch 70/661, Loss: 1.0101\n",
      "  Batch 80/661, Loss: 0.1395\n",
      "  Batch 90/661, Loss: 0.8626\n",
      "  Batch 100/661, Loss: 0.1519\n",
      "GPU Memory: Total=14.74GB | Reserved=0.23GB | Allocated=0.20GB | Free=14.51GB\n",
      "  Batch 110/661, Loss: 0.3116\n",
      "  Batch 120/661, Loss: 0.3058\n",
      "  Batch 130/661, Loss: 0.1325\n",
      "  Batch 140/661, Loss: 0.1812\n",
      "  Batch 150/661, Loss: 0.6822\n",
      "GPU Memory: Total=14.74GB | Reserved=0.23GB | Allocated=0.20GB | Free=14.51GB\n",
      "  Batch 160/661, Loss: 0.4363\n",
      "  Batch 170/661, Loss: 0.1563\n",
      "  Batch 180/661, Loss: 0.0240\n",
      "  Batch 190/661, Loss: 1.3647\n",
      "  Batch 200/661, Loss: 0.0436\n",
      "GPU Memory: Total=14.74GB | Reserved=0.23GB | Allocated=0.20GB | Free=14.51GB\n",
      "  Batch 210/661, Loss: 0.2022\n",
      "  Batch 220/661, Loss: 0.2984\n",
      "  Batch 230/661, Loss: 0.2247\n",
      "  Batch 240/661, Loss: 0.7552\n",
      "  Batch 250/661, Loss: 0.2640\n",
      "GPU Memory: Total=14.74GB | Reserved=0.23GB | Allocated=0.20GB | Free=14.51GB\n",
      "  Batch 260/661, Loss: 0.2765\n",
      "  Batch 270/661, Loss: 0.5167\n",
      "  Batch 280/661, Loss: 0.2526\n",
      "  Batch 290/661, Loss: 0.3919\n",
      "  Batch 300/661, Loss: 0.5375\n",
      "GPU Memory: Total=14.74GB | Reserved=0.23GB | Allocated=0.20GB | Free=14.51GB\n",
      "  Batch 310/661, Loss: 0.5925\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[h264 @ 0x60141480] mb_type 104 in P slice too large at 98 31\n",
      "[h264 @ 0x60141480] error while decoding MB 98 31\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 320/661, Loss: 0.3609\n",
      "  Batch 330/661, Loss: 0.4424\n",
      "  Batch 340/661, Loss: 0.6526\n",
      "  Batch 350/661, Loss: 0.7184\n",
      "GPU Memory: Total=14.74GB | Reserved=0.23GB | Allocated=0.20GB | Free=14.51GB\n",
      "  Batch 360/661, Loss: 0.0830\n",
      "  Batch 370/661, Loss: 0.2051\n",
      "  Batch 380/661, Loss: 0.7139\n",
      "  Batch 390/661, Loss: 0.1287\n",
      "  Batch 400/661, Loss: 0.0427\n",
      "GPU Memory: Total=14.74GB | Reserved=0.23GB | Allocated=0.20GB | Free=14.51GB\n",
      "  Batch 410/661, Loss: 0.0065\n",
      "  Batch 420/661, Loss: 0.0093\n",
      "  Batch 430/661, Loss: 0.1515\n",
      "  Batch 440/661, Loss: 0.1470\n",
      "  Batch 450/661, Loss: 0.2146\n",
      "GPU Memory: Total=14.74GB | Reserved=0.23GB | Allocated=0.20GB | Free=14.51GB\n",
      "  Batch 460/661, Loss: 0.5862\n",
      "  Batch 470/661, Loss: 0.0588\n",
      "  Batch 480/661, Loss: 0.4005\n",
      "  Batch 490/661, Loss: 0.0499\n",
      "  Batch 500/661, Loss: 0.0735\n",
      "GPU Memory: Total=14.74GB | Reserved=0.23GB | Allocated=0.20GB | Free=14.51GB\n",
      "  Batch 510/661, Loss: 0.0857\n",
      "  Batch 520/661, Loss: 0.0268\n",
      "  Batch 530/661, Loss: 0.1982\n",
      "  Batch 540/661, Loss: 0.0853\n",
      "  Batch 550/661, Loss: 0.5066\n",
      "GPU Memory: Total=14.74GB | Reserved=0.23GB | Allocated=0.20GB | Free=14.51GB\n",
      "  Batch 560/661, Loss: 0.0430\n",
      "  Batch 570/661, Loss: 0.2491\n",
      "  Batch 580/661, Loss: 0.3314\n",
      "  Batch 590/661, Loss: 0.2956\n",
      "  Batch 600/661, Loss: 0.1114\n",
      "GPU Memory: Total=14.74GB | Reserved=0.23GB | Allocated=0.20GB | Free=14.51GB\n",
      "  Batch 610/661, Loss: 0.3770\n",
      "  Batch 620/661, Loss: 0.0367\n",
      "  Batch 630/661, Loss: 0.0526\n",
      "  Batch 640/661, Loss: 0.0550\n",
      "  Batch 650/661, Loss: 0.7203\n",
      "GPU Memory: Total=14.74GB | Reserved=0.23GB | Allocated=0.20GB | Free=14.51GB\n",
      "  Batch 660/661, Loss: 0.3680\n",
      "Starting validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31/1455025713.py:22: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31/4126126155.py:134: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n",
      "/tmp/ipykernel_31/169455789.py:98: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=True):  # Use mixed precision\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 10/661, Loss: 0.0786\n",
      "  Batch 20/661, Loss: 0.0438\n",
      "  Batch 30/661, Loss: 0.3670\n",
      "  Batch 40/661, Loss: 0.7081\n",
      "  Batch 50/661, Loss: 0.3940\n",
      "GPU Memory: Total=14.74GB | Reserved=0.23GB | Allocated=0.20GB | Free=14.51GB\n",
      "  Batch 60/661, Loss: 0.1794\n",
      "  Batch 70/661, Loss: 0.3922\n",
      "  Batch 80/661, Loss: 0.0345\n",
      "  Batch 90/661, Loss: 0.3242\n",
      "  Batch 100/661, Loss: 0.7099\n",
      "GPU Memory: Total=14.74GB | Reserved=0.23GB | Allocated=0.20GB | Free=14.51GB\n",
      "  Batch 110/661, Loss: 0.2523\n",
      "  Batch 120/661, Loss: 0.4474\n",
      "  Batch 130/661, Loss: 0.0752\n",
      "  Batch 140/661, Loss: 0.0662\n",
      "  Batch 150/661, Loss: 0.1953\n",
      "GPU Memory: Total=14.74GB | Reserved=0.23GB | Allocated=0.20GB | Free=14.51GB\n",
      "  Batch 160/661, Loss: 0.3515\n",
      "  Batch 170/661, Loss: 0.2083\n",
      "  Batch 180/661, Loss: 1.2753\n",
      "  Batch 190/661, Loss: 0.0151\n",
      "  Batch 200/661, Loss: 0.5445\n",
      "GPU Memory: Total=14.74GB | Reserved=0.23GB | Allocated=0.20GB | Free=14.51GB\n",
      "  Batch 210/661, Loss: 0.1345\n",
      "  Batch 220/661, Loss: 1.0465\n",
      "  Batch 230/661, Loss: 0.0285\n",
      "  Batch 240/661, Loss: 0.1824\n",
      "  Batch 250/661, Loss: 0.5774\n",
      "GPU Memory: Total=14.74GB | Reserved=0.23GB | Allocated=0.20GB | Free=14.51GB\n",
      "  Batch 260/661, Loss: 0.1110\n",
      "  Batch 270/661, Loss: 0.5500\n",
      "  Batch 280/661, Loss: 0.1404\n",
      "  Batch 290/661, Loss: 0.1975\n",
      "  Batch 300/661, Loss: 0.0566\n",
      "GPU Memory: Total=14.74GB | Reserved=0.23GB | Allocated=0.20GB | Free=14.51GB\n",
      "  Batch 310/661, Loss: 0.3818\n",
      "  Batch 320/661, Loss: 0.4943\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[h264 @ 0x5f4e9780] mb_type 104 in P slice too large at 98 31\n",
      "[h264 @ 0x5f4e9780] error while decoding MB 98 31\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 330/661, Loss: 0.0160\n",
      "  Batch 340/661, Loss: 0.0318\n",
      "  Batch 350/661, Loss: 0.0161\n",
      "GPU Memory: Total=14.74GB | Reserved=0.23GB | Allocated=0.20GB | Free=14.51GB\n",
      "  Batch 360/661, Loss: 0.0600\n",
      "  Batch 370/661, Loss: 1.0409\n",
      "  Batch 380/661, Loss: 0.2197\n",
      "  Batch 390/661, Loss: 0.0687\n",
      "  Batch 400/661, Loss: 0.0524\n",
      "GPU Memory: Total=14.74GB | Reserved=0.23GB | Allocated=0.20GB | Free=14.51GB\n",
      "  Batch 410/661, Loss: 0.6361\n",
      "  Batch 420/661, Loss: 0.0074\n",
      "  Batch 430/661, Loss: 0.0634\n",
      "  Batch 440/661, Loss: 0.1385\n",
      "  Batch 450/661, Loss: 0.7940\n",
      "GPU Memory: Total=14.74GB | Reserved=0.23GB | Allocated=0.20GB | Free=14.51GB\n",
      "  Batch 460/661, Loss: 0.3008\n",
      "  Batch 470/661, Loss: 0.4591\n",
      "  Batch 480/661, Loss: 0.0185\n",
      "  Batch 490/661, Loss: 0.2867\n",
      "  Batch 500/661, Loss: 0.1013\n",
      "GPU Memory: Total=14.74GB | Reserved=0.23GB | Allocated=0.20GB | Free=14.51GB\n",
      "  Batch 510/661, Loss: 0.3856\n",
      "  Batch 520/661, Loss: 0.4737\n",
      "  Batch 530/661, Loss: 0.5825\n",
      "  Batch 540/661, Loss: 0.1472\n",
      "  Batch 550/661, Loss: 0.9708\n",
      "GPU Memory: Total=14.74GB | Reserved=0.23GB | Allocated=0.20GB | Free=14.51GB\n",
      "  Batch 560/661, Loss: 0.0943\n",
      "  Batch 570/661, Loss: 0.1838\n",
      "  Batch 580/661, Loss: 0.3272\n",
      "  Batch 590/661, Loss: 0.0353\n",
      "  Batch 600/661, Loss: 0.1926\n",
      "GPU Memory: Total=14.74GB | Reserved=0.23GB | Allocated=0.20GB | Free=14.51GB\n",
      "  Batch 610/661, Loss: 0.0121\n",
      "  Batch 620/661, Loss: 0.1546\n",
      "  Batch 630/661, Loss: 0.1032\n",
      "  Batch 640/661, Loss: 0.0202\n",
      "  Batch 650/661, Loss: 0.1942\n",
      "GPU Memory: Total=14.74GB | Reserved=0.23GB | Allocated=0.20GB | Free=14.51GB\n",
      "  Batch 660/661, Loss: 0.4412\n",
      "Starting validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31/1455025713.py:22: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31/4126126155.py:134: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n",
      "/tmp/ipykernel_31/169455789.py:98: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=True):  # Use mixed precision\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 10/661, Loss: 0.3725\n",
      "  Batch 20/661, Loss: 0.0550\n",
      "  Batch 30/661, Loss: 0.6145\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[h264 @ 0x5f4c02c0] mb_type 104 in P slice too large at 98 31\n",
      "[h264 @ 0x5f4c02c0] error while decoding MB 98 31\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 40/661, Loss: 0.3050\n",
      "  Batch 50/661, Loss: 0.1456\n",
      "GPU Memory: Total=14.74GB | Reserved=0.23GB | Allocated=0.20GB | Free=14.51GB\n",
      "  Batch 60/661, Loss: 0.8896\n",
      "  Batch 70/661, Loss: 0.0057\n",
      "  Batch 80/661, Loss: 0.3884\n",
      "  Batch 90/661, Loss: 0.0943\n",
      "  Batch 100/661, Loss: 0.1560\n",
      "GPU Memory: Total=14.74GB | Reserved=0.23GB | Allocated=0.20GB | Free=14.51GB\n",
      "  Batch 110/661, Loss: 0.3820\n",
      "  Batch 120/661, Loss: 0.2659\n",
      "  Batch 130/661, Loss: 0.2858\n",
      "  Batch 140/661, Loss: 0.5283\n",
      "  Batch 150/661, Loss: 0.1521\n",
      "GPU Memory: Total=14.74GB | Reserved=0.23GB | Allocated=0.20GB | Free=14.51GB\n",
      "  Batch 160/661, Loss: 0.1828\n",
      "  Batch 170/661, Loss: 0.3057\n",
      "  Batch 180/661, Loss: 0.8577\n",
      "  Batch 190/661, Loss: 0.2809\n",
      "  Batch 200/661, Loss: 0.3954\n",
      "GPU Memory: Total=14.74GB | Reserved=0.23GB | Allocated=0.20GB | Free=14.51GB\n",
      "  Batch 210/661, Loss: 0.2261\n",
      "  Batch 220/661, Loss: 0.8228\n",
      "  Batch 230/661, Loss: 0.5055\n",
      "  Batch 240/661, Loss: 0.0278\n",
      "  Batch 250/661, Loss: 0.0142\n",
      "GPU Memory: Total=14.74GB | Reserved=0.23GB | Allocated=0.20GB | Free=14.51GB\n",
      "  Batch 260/661, Loss: 0.2541\n",
      "  Batch 270/661, Loss: 0.8575\n",
      "  Batch 280/661, Loss: 0.1213\n",
      "  Batch 290/661, Loss: 0.2803\n",
      "  Batch 300/661, Loss: 0.0600\n",
      "GPU Memory: Total=14.74GB | Reserved=0.23GB | Allocated=0.20GB | Free=14.51GB\n",
      "  Batch 310/661, Loss: 0.2420\n",
      "  Batch 320/661, Loss: 0.8094\n",
      "  Batch 330/661, Loss: 0.6465\n",
      "  Batch 340/661, Loss: 0.1854\n",
      "  Batch 350/661, Loss: 0.1633\n",
      "GPU Memory: Total=14.74GB | Reserved=0.23GB | Allocated=0.20GB | Free=14.51GB\n",
      "  Batch 360/661, Loss: 0.0065\n",
      "  Batch 370/661, Loss: 0.3086\n",
      "  Batch 380/661, Loss: 0.0325\n",
      "  Batch 390/661, Loss: 0.0088\n",
      "  Batch 400/661, Loss: 0.0078\n",
      "GPU Memory: Total=14.74GB | Reserved=0.23GB | Allocated=0.20GB | Free=14.51GB\n",
      "  Batch 410/661, Loss: 0.0094\n",
      "  Batch 420/661, Loss: 1.7221\n",
      "  Batch 430/661, Loss: 0.1710\n",
      "  Batch 440/661, Loss: 0.0252\n",
      "  Batch 450/661, Loss: 0.0119\n",
      "GPU Memory: Total=14.74GB | Reserved=0.23GB | Allocated=0.20GB | Free=14.51GB\n",
      "  Batch 460/661, Loss: 0.0231\n",
      "  Batch 470/661, Loss: 0.5542\n",
      "  Batch 480/661, Loss: 0.3932\n",
      "  Batch 490/661, Loss: 0.5364\n",
      "  Batch 500/661, Loss: 0.4822\n",
      "GPU Memory: Total=14.74GB | Reserved=0.23GB | Allocated=0.20GB | Free=14.51GB\n",
      "  Batch 510/661, Loss: 0.7655\n",
      "  Batch 520/661, Loss: 0.3118\n",
      "  Batch 530/661, Loss: 0.3155\n",
      "  Batch 540/661, Loss: 0.3177\n",
      "  Batch 550/661, Loss: 0.1408\n",
      "GPU Memory: Total=14.74GB | Reserved=0.23GB | Allocated=0.20GB | Free=14.51GB\n",
      "  Batch 560/661, Loss: 0.2908\n",
      "  Batch 570/661, Loss: 0.0815\n",
      "  Batch 580/661, Loss: 0.1156\n",
      "  Batch 590/661, Loss: 0.5757\n",
      "  Batch 600/661, Loss: 0.1322\n",
      "GPU Memory: Total=14.74GB | Reserved=0.23GB | Allocated=0.20GB | Free=14.51GB\n",
      "  Batch 610/661, Loss: 0.0501\n",
      "  Batch 620/661, Loss: 0.4100\n",
      "  Batch 630/661, Loss: 0.1464\n",
      "  Batch 640/661, Loss: 0.3524\n",
      "  Batch 650/661, Loss: 0.4204\n",
      "GPU Memory: Total=14.74GB | Reserved=0.23GB | Allocated=0.20GB | Free=14.51GB\n",
      "  Batch 660/661, Loss: 0.0742\n",
      "Starting validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31/1455025713.py:22: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31/4126126155.py:134: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n",
      "/tmp/ipykernel_31/169455789.py:98: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=True):  # Use mixed precision\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 10/661, Loss: 0.0167\n",
      "  Batch 20/661, Loss: 0.0044\n",
      "  Batch 30/661, Loss: 0.5684\n",
      "  Batch 40/661, Loss: 0.0938\n",
      "  Batch 50/661, Loss: 0.3509\n",
      "GPU Memory: Total=14.74GB | Reserved=0.23GB | Allocated=0.20GB | Free=14.51GB\n",
      "  Batch 60/661, Loss: 0.0256\n",
      "  Batch 70/661, Loss: 0.8873\n",
      "  Batch 80/661, Loss: 0.2975\n",
      "  Batch 90/661, Loss: 0.1849\n",
      "  Batch 100/661, Loss: 0.9258\n",
      "GPU Memory: Total=14.74GB | Reserved=0.23GB | Allocated=0.20GB | Free=14.51GB\n",
      "  Batch 110/661, Loss: 0.6212\n",
      "  Batch 120/661, Loss: 0.2196\n",
      "  Batch 130/661, Loss: 0.6214\n",
      "  Batch 140/661, Loss: 0.2589\n",
      "  Batch 150/661, Loss: 0.0030\n",
      "GPU Memory: Total=14.74GB | Reserved=0.23GB | Allocated=0.20GB | Free=14.51GB\n",
      "  Batch 160/661, Loss: 0.4274\n",
      "  Batch 170/661, Loss: 0.0397\n",
      "  Batch 180/661, Loss: 0.6995\n",
      "  Batch 190/661, Loss: 0.3444\n",
      "  Batch 200/661, Loss: 0.0434\n",
      "GPU Memory: Total=14.74GB | Reserved=0.23GB | Allocated=0.20GB | Free=14.51GB\n",
      "  Batch 210/661, Loss: 0.0243\n",
      "  Batch 220/661, Loss: 0.0112\n",
      "  Batch 230/661, Loss: 0.3421\n",
      "  Batch 240/661, Loss: 0.0354\n",
      "  Batch 250/661, Loss: 0.0294\n",
      "GPU Memory: Total=14.74GB | Reserved=0.23GB | Allocated=0.20GB | Free=14.51GB\n",
      "  Batch 260/661, Loss: 0.4610\n",
      "  Batch 270/661, Loss: 0.5307\n",
      "  Batch 280/661, Loss: 0.3266\n",
      "  Batch 290/661, Loss: 0.0149\n",
      "  Batch 300/661, Loss: 0.5561\n",
      "GPU Memory: Total=14.74GB | Reserved=0.23GB | Allocated=0.20GB | Free=14.51GB\n",
      "  Batch 310/661, Loss: 0.5647\n",
      "  Batch 320/661, Loss: 0.2463\n",
      "  Batch 330/661, Loss: 0.5132\n",
      "  Batch 340/661, Loss: 1.0908\n",
      "  Batch 350/661, Loss: 0.0332\n",
      "GPU Memory: Total=14.74GB | Reserved=0.23GB | Allocated=0.20GB | Free=14.51GB\n",
      "  Batch 360/661, Loss: 0.1955\n",
      "  Batch 370/661, Loss: 0.2658\n",
      "  Batch 380/661, Loss: 0.9662\n",
      "  Batch 390/661, Loss: 0.1188\n",
      "  Batch 400/661, Loss: 0.4607\n",
      "GPU Memory: Total=14.74GB | Reserved=0.23GB | Allocated=0.20GB | Free=14.51GB\n",
      "  Batch 410/661, Loss: 0.0078\n",
      "  Batch 420/661, Loss: 0.5756\n",
      "  Batch 430/661, Loss: 0.0898\n",
      "  Batch 440/661, Loss: 0.0950\n",
      "  Batch 450/661, Loss: 0.0209\n",
      "GPU Memory: Total=14.74GB | Reserved=0.23GB | Allocated=0.20GB | Free=14.51GB\n",
      "  Batch 460/661, Loss: 0.0110\n",
      "  Batch 470/661, Loss: 1.0920\n",
      "  Batch 480/661, Loss: 0.2710\n",
      "  Batch 490/661, Loss: 0.0047\n",
      "  Batch 500/661, Loss: 0.4426\n",
      "GPU Memory: Total=14.74GB | Reserved=0.23GB | Allocated=0.20GB | Free=14.51GB\n",
      "  Batch 510/661, Loss: 0.1346\n",
      "  Batch 520/661, Loss: 1.0516\n",
      "  Batch 530/661, Loss: 0.5271\n",
      "  Batch 540/661, Loss: 0.7664\n",
      "  Batch 550/661, Loss: 0.0982\n",
      "GPU Memory: Total=14.74GB | Reserved=0.23GB | Allocated=0.20GB | Free=14.51GB\n",
      "  Batch 560/661, Loss: 1.0874\n",
      "  Batch 570/661, Loss: 0.4983\n",
      "  Batch 580/661, Loss: 0.4903\n",
      "  Batch 590/661, Loss: 0.8831\n",
      "  Batch 600/661, Loss: 0.0509\n",
      "GPU Memory: Total=14.74GB | Reserved=0.23GB | Allocated=0.20GB | Free=14.51GB\n",
      "  Batch 610/661, Loss: 0.6499\n",
      "  Batch 620/661, Loss: 0.1063\n",
      "  Batch 630/661, Loss: 0.4152\n",
      "  Batch 640/661, Loss: 0.2432\n",
      "  Batch 650/661, Loss: 0.0474\n",
      "GPU Memory: Total=14.74GB | Reserved=0.23GB | Allocated=0.20GB | Free=14.51GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[h264 @ 0x5f936d40] mb_type 104 in P slice too large at 98 31\n",
      "[h264 @ 0x5f936d40] error while decoding MB 98 31\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 660/661, Loss: 0.8931\n",
      "Starting validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31/1455025713.py:22: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31/4126126155.py:134: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n",
      "/tmp/ipykernel_31/169455789.py:98: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=True):  # Use mixed precision\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 10/661, Loss: 0.0066\n",
      "  Batch 20/661, Loss: 0.3604\n",
      "  Batch 30/661, Loss: 0.0637\n",
      "  Batch 40/661, Loss: 0.0168\n",
      "  Batch 50/661, Loss: 0.0098\n",
      "GPU Memory: Total=14.74GB | Reserved=0.23GB | Allocated=0.20GB | Free=14.51GB\n",
      "  Batch 60/661, Loss: 0.2157\n",
      "  Batch 70/661, Loss: 0.1860\n",
      "  Batch 80/661, Loss: 0.4740\n",
      "  Batch 90/661, Loss: 0.5766\n",
      "  Batch 100/661, Loss: 0.0066\n",
      "GPU Memory: Total=14.74GB | Reserved=0.23GB | Allocated=0.20GB | Free=14.51GB\n",
      "  Batch 110/661, Loss: 0.0315\n",
      "  Batch 120/661, Loss: 0.0183\n",
      "  Batch 130/661, Loss: 0.0990\n",
      "  Batch 140/661, Loss: 0.0012\n",
      "  Batch 150/661, Loss: 0.0115\n",
      "GPU Memory: Total=14.74GB | Reserved=0.23GB | Allocated=0.20GB | Free=14.51GB\n",
      "  Batch 160/661, Loss: 1.1434\n",
      "  Batch 170/661, Loss: 0.0025\n",
      "  Batch 180/661, Loss: 0.1168\n",
      "  Batch 190/661, Loss: 0.2896\n",
      "  Batch 200/661, Loss: 0.0698\n",
      "GPU Memory: Total=14.74GB | Reserved=0.23GB | Allocated=0.20GB | Free=14.51GB\n",
      "  Batch 210/661, Loss: 0.0954\n",
      "  Batch 220/661, Loss: 0.0055\n",
      "  Batch 230/661, Loss: 0.1242\n",
      "  Batch 240/661, Loss: 0.0069\n",
      "  Batch 250/661, Loss: 0.4260\n",
      "GPU Memory: Total=14.74GB | Reserved=0.23GB | Allocated=0.20GB | Free=14.51GB\n",
      "  Batch 260/661, Loss: 0.0083\n",
      "  Batch 270/661, Loss: 0.4166\n",
      "  Batch 280/661, Loss: 0.5831\n",
      "  Batch 290/661, Loss: 0.2349\n",
      "  Batch 300/661, Loss: 0.0302\n",
      "GPU Memory: Total=14.74GB | Reserved=0.23GB | Allocated=0.20GB | Free=14.51GB\n",
      "  Batch 310/661, Loss: 0.5107\n",
      "  Batch 320/661, Loss: 0.1682\n",
      "  Batch 330/661, Loss: 0.0403\n",
      "  Batch 340/661, Loss: 0.3287\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[h264 @ 0x5f67f580] mb_type 104 in P slice too large at 98 31\n",
      "[h264 @ 0x5f67f580] error while decoding MB 98 31\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 350/661, Loss: 0.3575\n",
      "GPU Memory: Total=14.74GB | Reserved=0.23GB | Allocated=0.20GB | Free=14.51GB\n",
      "  Batch 360/661, Loss: 0.3012\n",
      "  Batch 370/661, Loss: 0.8238\n",
      "  Batch 380/661, Loss: 0.0100\n",
      "  Batch 390/661, Loss: 0.8545\n",
      "  Batch 400/661, Loss: 0.0318\n",
      "GPU Memory: Total=14.74GB | Reserved=0.23GB | Allocated=0.20GB | Free=14.51GB\n",
      "  Batch 410/661, Loss: 0.5387\n",
      "  Batch 420/661, Loss: 0.6873\n",
      "  Batch 430/661, Loss: 0.0172\n",
      "  Batch 440/661, Loss: 0.0055\n",
      "  Batch 450/661, Loss: 0.4761\n",
      "GPU Memory: Total=14.74GB | Reserved=0.23GB | Allocated=0.20GB | Free=14.51GB\n",
      "  Batch 460/661, Loss: 0.0122\n",
      "  Batch 470/661, Loss: 0.7888\n",
      "  Batch 480/661, Loss: 0.6031\n",
      "  Batch 490/661, Loss: 0.6670\n",
      "  Batch 500/661, Loss: 0.0099\n",
      "GPU Memory: Total=14.74GB | Reserved=0.23GB | Allocated=0.20GB | Free=14.51GB\n",
      "  Batch 510/661, Loss: 0.0071\n",
      "  Batch 520/661, Loss: 0.2805\n",
      "  Batch 530/661, Loss: 0.4857\n",
      "  Batch 540/661, Loss: 0.2100\n",
      "  Batch 550/661, Loss: 0.5363\n",
      "GPU Memory: Total=14.74GB | Reserved=0.23GB | Allocated=0.20GB | Free=14.51GB\n",
      "  Batch 560/661, Loss: 0.6423\n",
      "  Batch 570/661, Loss: 0.0075\n",
      "  Batch 580/661, Loss: 0.0164\n",
      "  Batch 590/661, Loss: 0.4185\n",
      "  Batch 600/661, Loss: 0.6097\n",
      "GPU Memory: Total=14.74GB | Reserved=0.23GB | Allocated=0.20GB | Free=14.51GB\n",
      "  Batch 610/661, Loss: 0.1727\n",
      "  Batch 620/661, Loss: 0.2420\n",
      "  Batch 630/661, Loss: 0.0162\n",
      "  Batch 640/661, Loss: 0.0071\n",
      "  Batch 650/661, Loss: 0.7217\n",
      "GPU Memory: Total=14.74GB | Reserved=0.23GB | Allocated=0.20GB | Free=14.51GB\n",
      "  Batch 660/661, Loss: 0.6772\n",
      "Starting validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31/1455025713.py:22: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31/4126126155.py:134: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n",
      "/tmp/ipykernel_31/169455789.py:98: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=True):  # Use mixed precision\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 10/661, Loss: 0.4370\n",
      "  Batch 20/661, Loss: 0.0570\n",
      "  Batch 30/661, Loss: 0.0133\n",
      "  Batch 40/661, Loss: 0.1892\n",
      "  Batch 50/661, Loss: 0.3758\n",
      "GPU Memory: Total=14.74GB | Reserved=0.23GB | Allocated=0.20GB | Free=14.51GB\n",
      "  Batch 60/661, Loss: 0.0293\n",
      "  Batch 70/661, Loss: 0.5666\n",
      "  Batch 80/661, Loss: 0.3352\n",
      "  Batch 90/661, Loss: 0.2134\n",
      "  Batch 100/661, Loss: 0.4511\n",
      "GPU Memory: Total=14.74GB | Reserved=0.23GB | Allocated=0.20GB | Free=14.51GB\n",
      "  Batch 110/661, Loss: 0.1177\n",
      "  Batch 120/661, Loss: 0.3764\n",
      "  Batch 130/661, Loss: 0.6089\n",
      "  Batch 140/661, Loss: 0.5865\n",
      "  Batch 150/661, Loss: 0.0171\n",
      "GPU Memory: Total=14.74GB | Reserved=0.23GB | Allocated=0.20GB | Free=14.51GB\n",
      "  Batch 160/661, Loss: 0.1571\n",
      "  Batch 170/661, Loss: 1.1325\n",
      "  Batch 180/661, Loss: 0.1792\n",
      "  Batch 190/661, Loss: 1.0588\n",
      "  Batch 200/661, Loss: 0.1260\n",
      "GPU Memory: Total=14.74GB | Reserved=0.23GB | Allocated=0.20GB | Free=14.51GB\n",
      "  Batch 210/661, Loss: 0.0891\n",
      "  Batch 220/661, Loss: 0.8191\n",
      "  Batch 230/661, Loss: 0.0151\n",
      "  Batch 240/661, Loss: 0.0674\n",
      "  Batch 250/661, Loss: 0.3515\n",
      "GPU Memory: Total=14.74GB | Reserved=0.23GB | Allocated=0.20GB | Free=14.51GB\n",
      "  Batch 260/661, Loss: 0.4192\n",
      "  Batch 270/661, Loss: 0.0034\n",
      "  Batch 280/661, Loss: 0.0077\n",
      "  Batch 290/661, Loss: 0.4292\n",
      "  Batch 300/661, Loss: 0.1192\n",
      "GPU Memory: Total=14.74GB | Reserved=0.23GB | Allocated=0.20GB | Free=14.51GB\n",
      "  Batch 310/661, Loss: 0.1106\n",
      "  Batch 320/661, Loss: 0.2914\n",
      "  Batch 330/661, Loss: 0.0093\n",
      "  Batch 340/661, Loss: 0.3037\n",
      "  Batch 350/661, Loss: 0.8513\n",
      "GPU Memory: Total=14.74GB | Reserved=0.23GB | Allocated=0.20GB | Free=14.51GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[h264 @ 0x5f4e7000] mb_type 104 in P slice too large at 98 31\n",
      "[h264 @ 0x5f4e7000] error while decoding MB 98 31\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 360/661, Loss: 0.4634\n",
      "  Batch 370/661, Loss: 0.0045\n",
      "  Batch 380/661, Loss: 0.5487\n",
      "  Batch 390/661, Loss: 0.1166\n",
      "  Batch 400/661, Loss: 0.0202\n",
      "GPU Memory: Total=14.74GB | Reserved=0.23GB | Allocated=0.20GB | Free=14.51GB\n",
      "  Batch 410/661, Loss: 0.2970\n",
      "  Batch 420/661, Loss: 0.0403\n",
      "  Batch 430/661, Loss: 0.0767\n",
      "  Batch 440/661, Loss: 0.1611\n",
      "  Batch 450/661, Loss: 0.0243\n",
      "GPU Memory: Total=14.74GB | Reserved=0.23GB | Allocated=0.20GB | Free=14.51GB\n",
      "  Batch 460/661, Loss: 0.2613\n",
      "  Batch 470/661, Loss: 0.5291\n",
      "  Batch 480/661, Loss: 0.0323\n",
      "  Batch 490/661, Loss: 0.1596\n",
      "  Batch 500/661, Loss: 0.1168\n",
      "GPU Memory: Total=14.74GB | Reserved=0.23GB | Allocated=0.20GB | Free=14.51GB\n",
      "  Batch 510/661, Loss: 0.0300\n",
      "  Batch 520/661, Loss: 0.0777\n",
      "  Batch 530/661, Loss: 0.0396\n",
      "  Batch 540/661, Loss: 0.0267\n",
      "  Batch 550/661, Loss: 0.3784\n",
      "GPU Memory: Total=14.74GB | Reserved=0.23GB | Allocated=0.20GB | Free=14.51GB\n",
      "  Batch 560/661, Loss: 0.3881\n",
      "  Batch 570/661, Loss: 0.0096\n",
      "  Batch 580/661, Loss: 0.4264\n",
      "  Batch 590/661, Loss: 0.1379\n",
      "  Batch 600/661, Loss: 0.0126\n",
      "GPU Memory: Total=14.74GB | Reserved=0.23GB | Allocated=0.20GB | Free=14.51GB\n",
      "  Batch 610/661, Loss: 0.2922\n",
      "  Batch 620/661, Loss: 0.3523\n",
      "  Batch 630/661, Loss: 1.2762\n",
      "  Batch 640/661, Loss: 0.2018\n",
      "  Batch 650/661, Loss: 0.7397\n",
      "GPU Memory: Total=14.74GB | Reserved=0.23GB | Allocated=0.20GB | Free=14.51GB\n",
      "  Batch 660/661, Loss: 0.0259\n",
      "Starting validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31/1455025713.py:22: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31/4126126155.py:134: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n",
      "/tmp/ipykernel_31/169455789.py:98: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=True):  # Use mixed precision\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 10/661, Loss: 0.1068\n",
      "  Batch 20/661, Loss: 0.0153\n",
      "  Batch 30/661, Loss: 0.0537\n",
      "  Batch 40/661, Loss: 0.0127\n",
      "  Batch 50/661, Loss: 0.0070\n",
      "GPU Memory: Total=14.74GB | Reserved=0.23GB | Allocated=0.20GB | Free=14.51GB\n",
      "  Batch 60/661, Loss: 0.7441\n",
      "  Batch 70/661, Loss: 0.0052\n",
      "  Batch 80/661, Loss: 0.7489\n",
      "  Batch 90/661, Loss: 0.4394\n",
      "  Batch 100/661, Loss: 0.3881\n",
      "GPU Memory: Total=14.74GB | Reserved=0.23GB | Allocated=0.20GB | Free=14.51GB\n",
      "  Batch 110/661, Loss: 0.0895\n",
      "  Batch 120/661, Loss: 0.0148\n",
      "  Batch 130/661, Loss: 0.0035\n",
      "  Batch 140/661, Loss: 0.0093\n",
      "  Batch 150/661, Loss: 0.3788\n",
      "GPU Memory: Total=14.74GB | Reserved=0.23GB | Allocated=0.20GB | Free=14.51GB\n",
      "  Batch 160/661, Loss: 0.7818\n",
      "  Batch 170/661, Loss: 0.0049\n",
      "  Batch 180/661, Loss: 0.6801\n",
      "  Batch 190/661, Loss: 0.0545\n",
      "  Batch 200/661, Loss: 0.0375\n",
      "GPU Memory: Total=14.74GB | Reserved=0.23GB | Allocated=0.20GB | Free=14.51GB\n",
      "  Batch 210/661, Loss: 0.0086\n",
      "  Batch 220/661, Loss: 0.6317\n",
      "  Batch 230/661, Loss: 0.0175\n",
      "  Batch 240/661, Loss: 0.4344\n",
      "  Batch 250/661, Loss: 0.3897\n",
      "GPU Memory: Total=14.74GB | Reserved=0.23GB | Allocated=0.20GB | Free=14.51GB\n",
      "  Batch 260/661, Loss: 0.0247\n",
      "  Batch 270/661, Loss: 0.0175\n",
      "  Batch 280/661, Loss: 0.0089\n",
      "  Batch 290/661, Loss: 0.0324\n",
      "  Batch 300/661, Loss: 0.5222\n",
      "GPU Memory: Total=14.74GB | Reserved=0.23GB | Allocated=0.20GB | Free=14.51GB\n",
      "  Batch 310/661, Loss: 0.0701\n",
      "  Batch 320/661, Loss: 0.6469\n",
      "  Batch 330/661, Loss: 0.0111\n",
      "  Batch 340/661, Loss: 0.1419\n",
      "  Batch 350/661, Loss: 0.2795\n",
      "GPU Memory: Total=14.74GB | Reserved=0.23GB | Allocated=0.20GB | Free=14.51GB\n",
      "  Batch 360/661, Loss: 0.8522\n",
      "  Batch 370/661, Loss: 0.0024\n",
      "  Batch 380/661, Loss: 0.0043\n",
      "  Batch 390/661, Loss: 0.1272\n",
      "  Batch 400/661, Loss: 1.5979\n",
      "GPU Memory: Total=14.74GB | Reserved=0.23GB | Allocated=0.20GB | Free=14.51GB\n",
      "  Batch 410/661, Loss: 0.0165\n",
      "  Batch 420/661, Loss: 0.0898\n",
      "  Batch 430/661, Loss: 0.4609\n",
      "  Batch 440/661, Loss: 0.0397\n",
      "  Batch 450/661, Loss: 0.0183\n",
      "GPU Memory: Total=14.74GB | Reserved=0.23GB | Allocated=0.20GB | Free=14.51GB\n",
      "  Batch 460/661, Loss: 0.7250\n",
      "  Batch 470/661, Loss: 0.0031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[h264 @ 0x5f4e55c0] mb_type 104 in P slice too large at 98 31\n",
      "[h264 @ 0x5f4e55c0] error while decoding MB 98 31\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 480/661, Loss: 0.0727\n",
      "  Batch 490/661, Loss: 0.1579\n",
      "  Batch 500/661, Loss: 1.0911\n",
      "GPU Memory: Total=14.74GB | Reserved=0.23GB | Allocated=0.20GB | Free=14.51GB\n",
      "  Batch 510/661, Loss: 0.0064\n",
      "  Batch 520/661, Loss: 0.0225\n",
      "  Batch 530/661, Loss: 0.0248\n",
      "  Batch 540/661, Loss: 0.0035\n",
      "  Batch 550/661, Loss: 1.0006\n",
      "GPU Memory: Total=14.74GB | Reserved=0.23GB | Allocated=0.20GB | Free=14.51GB\n",
      "  Batch 560/661, Loss: 0.2069\n",
      "  Batch 570/661, Loss: 0.3679\n",
      "  Batch 580/661, Loss: 0.0147\n",
      "  Batch 590/661, Loss: 0.0111\n",
      "  Batch 600/661, Loss: 0.0464\n",
      "GPU Memory: Total=14.74GB | Reserved=0.23GB | Allocated=0.20GB | Free=14.51GB\n",
      "  Batch 610/661, Loss: 0.0124\n",
      "  Batch 620/661, Loss: 0.0278\n",
      "  Batch 630/661, Loss: 0.0498\n",
      "  Batch 640/661, Loss: 0.4872\n",
      "  Batch 650/661, Loss: 0.4968\n",
      "GPU Memory: Total=14.74GB | Reserved=0.23GB | Allocated=0.20GB | Free=14.51GB\n",
      "  Batch 660/661, Loss: 0.0652\n",
      "Starting validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31/1455025713.py:22: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31/4126126155.py:134: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n",
      "/tmp/ipykernel_31/169455789.py:98: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=True):  # Use mixed precision\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 10/661, Loss: 0.9561\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[h264 @ 0x5f67ee40] mb_type 104 in P slice too large at 98 31\n",
      "[h264 @ 0x5f67ee40] error while decoding MB 98 31\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 20/661, Loss: 0.0176\n",
      "  Batch 30/661, Loss: 0.5961\n",
      "  Batch 40/661, Loss: 0.0992\n",
      "  Batch 50/661, Loss: 0.0077\n",
      "GPU Memory: Total=14.74GB | Reserved=0.23GB | Allocated=0.20GB | Free=14.51GB\n",
      "  Batch 60/661, Loss: 0.4613\n",
      "  Batch 70/661, Loss: 0.0147\n",
      "  Batch 80/661, Loss: 0.0034\n",
      "  Batch 90/661, Loss: 0.5463\n",
      "  Batch 100/661, Loss: 1.1473\n",
      "GPU Memory: Total=14.74GB | Reserved=0.23GB | Allocated=0.20GB | Free=14.51GB\n",
      "  Batch 110/661, Loss: 0.0107\n",
      "  Batch 120/661, Loss: 0.0825\n",
      "  Batch 130/661, Loss: 0.1094\n",
      "  Batch 140/661, Loss: 1.0061\n",
      "  Batch 150/661, Loss: 0.1791\n",
      "GPU Memory: Total=14.74GB | Reserved=0.23GB | Allocated=0.20GB | Free=14.51GB\n",
      "  Batch 160/661, Loss: 0.0072\n",
      "  Batch 170/661, Loss: 0.1135\n",
      "  Batch 180/661, Loss: 0.0932\n",
      "  Batch 190/661, Loss: 0.2877\n",
      "  Batch 200/661, Loss: 0.7852\n",
      "GPU Memory: Total=14.74GB | Reserved=0.23GB | Allocated=0.20GB | Free=14.51GB\n",
      "  Batch 210/661, Loss: 0.0069\n",
      "  Batch 220/661, Loss: 0.0010\n",
      "  Batch 230/661, Loss: 0.1110\n",
      "  Batch 240/661, Loss: 1.2333\n",
      "  Batch 250/661, Loss: 0.0038\n",
      "GPU Memory: Total=14.74GB | Reserved=0.23GB | Allocated=0.20GB | Free=14.51GB\n",
      "  Batch 260/661, Loss: 0.0153\n",
      "  Batch 270/661, Loss: 0.1834\n",
      "  Batch 280/661, Loss: 0.2871\n",
      "  Batch 290/661, Loss: 0.3884\n",
      "  Batch 300/661, Loss: 0.0441\n",
      "GPU Memory: Total=14.74GB | Reserved=0.23GB | Allocated=0.20GB | Free=14.51GB\n",
      "  Batch 310/661, Loss: 0.9203\n",
      "  Batch 320/661, Loss: 0.6013\n",
      "  Batch 330/661, Loss: 0.0113\n",
      "  Batch 340/661, Loss: 0.0953\n",
      "  Batch 350/661, Loss: 0.0630\n",
      "GPU Memory: Total=14.74GB | Reserved=0.23GB | Allocated=0.20GB | Free=14.51GB\n",
      "  Batch 360/661, Loss: 0.5188\n",
      "  Batch 370/661, Loss: 0.1694\n",
      "  Batch 380/661, Loss: 0.1624\n",
      "  Batch 390/661, Loss: 0.2280\n",
      "  Batch 400/661, Loss: 0.3735\n",
      "GPU Memory: Total=14.74GB | Reserved=0.23GB | Allocated=0.20GB | Free=14.51GB\n",
      "  Batch 410/661, Loss: 0.0059\n",
      "  Batch 420/661, Loss: 0.1253\n",
      "  Batch 430/661, Loss: 0.1323\n",
      "  Batch 440/661, Loss: 0.1812\n",
      "  Batch 450/661, Loss: 0.2115\n",
      "GPU Memory: Total=14.74GB | Reserved=0.23GB | Allocated=0.20GB | Free=14.51GB\n",
      "  Batch 460/661, Loss: 0.0647\n",
      "  Batch 470/661, Loss: 0.0102\n",
      "  Batch 480/661, Loss: 0.7338\n",
      "  Batch 490/661, Loss: 0.0059\n",
      "  Batch 500/661, Loss: 0.0451\n",
      "GPU Memory: Total=14.74GB | Reserved=0.23GB | Allocated=0.20GB | Free=14.51GB\n",
      "  Batch 510/661, Loss: 0.5082\n",
      "  Batch 520/661, Loss: 0.0452\n",
      "  Batch 530/661, Loss: 0.0261\n",
      "  Batch 540/661, Loss: 0.0081\n",
      "  Batch 550/661, Loss: 0.0251\n",
      "GPU Memory: Total=14.74GB | Reserved=0.23GB | Allocated=0.20GB | Free=14.51GB\n",
      "  Batch 560/661, Loss: 0.0053\n",
      "  Batch 570/661, Loss: 1.0321\n",
      "  Batch 580/661, Loss: 0.0241\n",
      "  Batch 590/661, Loss: 0.1124\n",
      "  Batch 600/661, Loss: 0.0032\n",
      "GPU Memory: Total=14.74GB | Reserved=0.23GB | Allocated=0.20GB | Free=14.51GB\n",
      "  Batch 610/661, Loss: 0.0151\n",
      "  Batch 620/661, Loss: 0.7475\n",
      "  Batch 630/661, Loss: 0.2318\n",
      "  Batch 640/661, Loss: 0.2038\n",
      "  Batch 650/661, Loss: 0.0185\n",
      "GPU Memory: Total=14.74GB | Reserved=0.23GB | Allocated=0.20GB | Free=14.51GB\n",
      "  Batch 660/661, Loss: 0.0381\n",
      "Starting validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31/1455025713.py:22: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unfreezing last 1 of 2 feature extractor layers\n",
      "GPU Memory: Total=14.74GB | Reserved=0.26GB | Allocated=0.20GB | Free=14.48GB\n",
      "Epoch 11/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31/4126126155.py:134: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n",
      "/tmp/ipykernel_31/169455789.py:98: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=True):  # Use mixed precision\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 10/661, Loss: 0.0908\n",
      "  Batch 20/661, Loss: 0.8571\n",
      "  Batch 30/661, Loss: 0.0064\n",
      "  Batch 40/661, Loss: 0.0264\n",
      "  Batch 50/661, Loss: 0.2813\n",
      "GPU Memory: Total=14.74GB | Reserved=0.23GB | Allocated=0.20GB | Free=14.51GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[h264 @ 0x5f903b00] mb_type 104 in P slice too large at 98 31\n",
      "[h264 @ 0x5f903b00] error while decoding MB 98 31\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 60/661, Loss: 1.2696\n",
      "  Batch 70/661, Loss: 0.0518\n",
      "  Batch 80/661, Loss: 0.2523\n",
      "  Batch 90/661, Loss: 0.1033\n",
      "  Batch 100/661, Loss: 0.0046\n",
      "GPU Memory: Total=14.74GB | Reserved=0.23GB | Allocated=0.20GB | Free=14.51GB\n",
      "  Batch 110/661, Loss: 0.0050\n",
      "  Batch 120/661, Loss: 0.1118\n",
      "  Batch 130/661, Loss: 0.4461\n",
      "  Batch 140/661, Loss: 0.1987\n",
      "  Batch 150/661, Loss: 0.1033\n",
      "GPU Memory: Total=14.74GB | Reserved=0.23GB | Allocated=0.20GB | Free=14.51GB\n",
      "  Batch 160/661, Loss: 0.1836\n",
      "  Batch 170/661, Loss: 0.0432\n",
      "  Batch 180/661, Loss: 0.0978\n",
      "  Batch 190/661, Loss: 0.7517\n",
      "  Batch 200/661, Loss: 0.5765\n",
      "GPU Memory: Total=14.74GB | Reserved=0.23GB | Allocated=0.20GB | Free=14.51GB\n",
      "  Batch 210/661, Loss: 0.0032\n",
      "  Batch 220/661, Loss: 0.0050\n",
      "  Batch 230/661, Loss: 0.7122\n",
      "  Batch 240/661, Loss: 0.2020\n",
      "  Batch 250/661, Loss: 0.0092\n",
      "GPU Memory: Total=14.74GB | Reserved=0.23GB | Allocated=0.20GB | Free=14.51GB\n",
      "  Batch 260/661, Loss: 0.5901\n",
      "  Batch 270/661, Loss: 0.2681\n",
      "  Batch 280/661, Loss: 0.4139\n",
      "  Batch 290/661, Loss: 0.0122\n",
      "  Batch 300/661, Loss: 0.8985\n",
      "GPU Memory: Total=14.74GB | Reserved=0.23GB | Allocated=0.20GB | Free=14.51GB\n",
      "  Batch 310/661, Loss: 0.3690\n",
      "  Batch 320/661, Loss: 0.3111\n",
      "  Batch 330/661, Loss: 0.2172\n",
      "  Batch 340/661, Loss: 0.0048\n",
      "  Batch 350/661, Loss: 0.1446\n",
      "GPU Memory: Total=14.74GB | Reserved=0.23GB | Allocated=0.20GB | Free=14.51GB\n",
      "  Batch 360/661, Loss: 0.1204\n",
      "  Batch 370/661, Loss: 0.0021\n",
      "  Batch 380/661, Loss: 0.0771\n",
      "  Batch 390/661, Loss: 0.0086\n",
      "  Batch 400/661, Loss: 0.0245\n",
      "GPU Memory: Total=14.74GB | Reserved=0.23GB | Allocated=0.20GB | Free=14.51GB\n",
      "  Batch 410/661, Loss: 0.2092\n",
      "  Batch 420/661, Loss: 0.0533\n",
      "  Batch 430/661, Loss: 0.6538\n",
      "  Batch 440/661, Loss: 0.4304\n",
      "  Batch 450/661, Loss: 0.1411\n",
      "GPU Memory: Total=14.74GB | Reserved=0.23GB | Allocated=0.20GB | Free=14.51GB\n",
      "  Batch 460/661, Loss: 0.0089\n",
      "  Batch 470/661, Loss: 0.0020\n",
      "  Batch 480/661, Loss: 0.0053\n",
      "  Batch 490/661, Loss: 0.0065\n",
      "  Batch 500/661, Loss: 0.0228\n",
      "GPU Memory: Total=14.74GB | Reserved=0.23GB | Allocated=0.20GB | Free=14.51GB\n",
      "  Batch 510/661, Loss: 0.0024\n",
      "  Batch 520/661, Loss: 0.0011\n",
      "  Batch 530/661, Loss: 0.0011\n",
      "  Batch 540/661, Loss: 0.7850\n",
      "  Batch 550/661, Loss: 0.3695\n",
      "GPU Memory: Total=14.74GB | Reserved=0.23GB | Allocated=0.20GB | Free=14.51GB\n",
      "  Batch 560/661, Loss: 0.5829\n",
      "  Batch 570/661, Loss: 0.0040\n",
      "  Batch 580/661, Loss: 0.0357\n",
      "  Batch 590/661, Loss: 1.2407\n",
      "  Batch 600/661, Loss: 0.2194\n",
      "GPU Memory: Total=14.74GB | Reserved=0.23GB | Allocated=0.20GB | Free=14.51GB\n",
      "  Batch 610/661, Loss: 0.0125\n",
      "  Batch 620/661, Loss: 0.3121\n",
      "  Batch 630/661, Loss: 0.0045\n",
      "  Batch 640/661, Loss: 0.0082\n",
      "  Batch 650/661, Loss: 0.3070\n",
      "GPU Memory: Total=14.74GB | Reserved=0.23GB | Allocated=0.20GB | Free=14.51GB\n",
      "  Batch 660/661, Loss: 0.1632\n",
      "Starting validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31/1455025713.py:22: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31/4126126155.py:134: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n",
      "/tmp/ipykernel_31/169455789.py:98: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=True):  # Use mixed precision\n",
      "[h264 @ 0x5ffe2480] mb_type 104 in P slice too large at 98 31\n",
      "[h264 @ 0x5ffe2480] error while decoding MB 98 31\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 10/661, Loss: 0.0058\n",
      "  Batch 20/661, Loss: 0.1998\n",
      "  Batch 30/661, Loss: 0.0422\n",
      "  Batch 40/661, Loss: 0.5878\n",
      "  Batch 50/661, Loss: 0.0021\n",
      "GPU Memory: Total=14.74GB | Reserved=0.23GB | Allocated=0.20GB | Free=14.51GB\n",
      "  Batch 60/661, Loss: 1.0181\n",
      "  Batch 70/661, Loss: 0.1186\n",
      "  Batch 80/661, Loss: 0.1480\n",
      "  Batch 90/661, Loss: 0.0325\n",
      "  Batch 100/661, Loss: 0.0062\n",
      "GPU Memory: Total=14.74GB | Reserved=0.23GB | Allocated=0.20GB | Free=14.51GB\n",
      "  Batch 110/661, Loss: 0.4924\n",
      "  Batch 120/661, Loss: 0.4431\n",
      "  Batch 130/661, Loss: 0.0013\n",
      "  Batch 140/661, Loss: 0.2158\n",
      "  Batch 150/661, Loss: 1.1565\n",
      "GPU Memory: Total=14.74GB | Reserved=0.23GB | Allocated=0.20GB | Free=14.51GB\n",
      "  Batch 160/661, Loss: 0.3321\n",
      "  Batch 170/661, Loss: 0.1800\n",
      "  Batch 180/661, Loss: 0.6370\n",
      "  Batch 190/661, Loss: 0.0389\n",
      "  Batch 200/661, Loss: 0.6282\n",
      "GPU Memory: Total=14.74GB | Reserved=0.23GB | Allocated=0.20GB | Free=14.51GB\n",
      "  Batch 210/661, Loss: 0.0038\n",
      "  Batch 220/661, Loss: 1.4072\n",
      "  Batch 230/661, Loss: 0.7415\n",
      "  Batch 240/661, Loss: 0.1878\n",
      "  Batch 250/661, Loss: 0.0096\n",
      "GPU Memory: Total=14.74GB | Reserved=0.23GB | Allocated=0.20GB | Free=14.51GB\n",
      "  Batch 260/661, Loss: 0.3461\n",
      "  Batch 270/661, Loss: 0.4716\n",
      "  Batch 280/661, Loss: 0.1779\n",
      "  Batch 290/661, Loss: 1.2482\n",
      "  Batch 300/661, Loss: 0.7029\n",
      "GPU Memory: Total=14.74GB | Reserved=0.23GB | Allocated=0.20GB | Free=14.51GB\n",
      "  Batch 310/661, Loss: 0.2169\n",
      "  Batch 320/661, Loss: 0.6202\n",
      "  Batch 330/661, Loss: 0.8446\n",
      "  Batch 340/661, Loss: 0.0126\n",
      "  Batch 350/661, Loss: 0.0266\n",
      "GPU Memory: Total=14.74GB | Reserved=0.23GB | Allocated=0.20GB | Free=14.51GB\n",
      "  Batch 360/661, Loss: 0.0009\n",
      "  Batch 370/661, Loss: 0.0010\n",
      "  Batch 380/661, Loss: 0.8756\n",
      "  Batch 390/661, Loss: 0.0017\n",
      "  Batch 400/661, Loss: 0.4279\n",
      "GPU Memory: Total=14.74GB | Reserved=0.23GB | Allocated=0.20GB | Free=14.51GB\n",
      "  Batch 410/661, Loss: 0.0345\n",
      "  Batch 420/661, Loss: 0.0046\n",
      "  Batch 430/661, Loss: 0.1692\n",
      "  Batch 440/661, Loss: 0.0005\n",
      "  Batch 450/661, Loss: 0.0027\n",
      "GPU Memory: Total=14.74GB | Reserved=0.23GB | Allocated=0.20GB | Free=14.51GB\n",
      "  Batch 460/661, Loss: 0.4027\n",
      "  Batch 470/661, Loss: 0.2363\n",
      "  Batch 480/661, Loss: 0.1850\n",
      "  Batch 490/661, Loss: 0.0340\n",
      "  Batch 500/661, Loss: 0.6788\n",
      "GPU Memory: Total=14.74GB | Reserved=0.23GB | Allocated=0.20GB | Free=14.51GB\n",
      "  Batch 510/661, Loss: 0.1982\n",
      "  Batch 520/661, Loss: 0.0038\n",
      "  Batch 530/661, Loss: 0.5778\n",
      "  Batch 540/661, Loss: 0.0372\n",
      "  Batch 550/661, Loss: 0.0067\n",
      "GPU Memory: Total=14.74GB | Reserved=0.23GB | Allocated=0.20GB | Free=14.51GB\n",
      "  Batch 560/661, Loss: 1.1164\n",
      "  Batch 570/661, Loss: 0.8524\n",
      "  Batch 580/661, Loss: 0.0162\n",
      "  Batch 590/661, Loss: 0.0009\n",
      "  Batch 600/661, Loss: 0.2531\n",
      "GPU Memory: Total=14.74GB | Reserved=0.23GB | Allocated=0.20GB | Free=14.51GB\n",
      "  Batch 610/661, Loss: 0.0352\n",
      "  Batch 620/661, Loss: 0.0009\n",
      "  Batch 630/661, Loss: 0.6290\n",
      "  Batch 640/661, Loss: 0.0327\n",
      "  Batch 650/661, Loss: 0.0645\n",
      "GPU Memory: Total=14.74GB | Reserved=0.23GB | Allocated=0.20GB | Free=14.51GB\n",
      "  Batch 660/661, Loss: 0.5949\n",
      "Starting validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31/1455025713.py:22: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31/4126126155.py:134: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n",
      "/tmp/ipykernel_31/169455789.py:98: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=True):  # Use mixed precision\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 10/661, Loss: 0.0262\n",
      "  Batch 20/661, Loss: 0.1167\n",
      "  Batch 30/661, Loss: 0.0275\n",
      "  Batch 40/661, Loss: 0.4728\n",
      "  Batch 50/661, Loss: 0.0027\n",
      "GPU Memory: Total=14.74GB | Reserved=0.23GB | Allocated=0.20GB | Free=14.51GB\n",
      "  Batch 60/661, Loss: 0.8319\n",
      "  Batch 70/661, Loss: 0.0221\n",
      "  Batch 80/661, Loss: 0.0231\n",
      "  Batch 90/661, Loss: 0.0244\n",
      "  Batch 100/661, Loss: 0.0183\n",
      "GPU Memory: Total=14.74GB | Reserved=0.23GB | Allocated=0.20GB | Free=14.51GB\n",
      "  Batch 110/661, Loss: 0.8031\n",
      "  Batch 120/661, Loss: 0.0033\n",
      "  Batch 130/661, Loss: 0.4062\n",
      "  Batch 140/661, Loss: 0.0277\n",
      "  Batch 150/661, Loss: 0.3583\n",
      "GPU Memory: Total=14.74GB | Reserved=0.23GB | Allocated=0.20GB | Free=14.51GB\n",
      "  Batch 160/661, Loss: 0.6721\n",
      "  Batch 170/661, Loss: 0.0052\n",
      "  Batch 180/661, Loss: 0.0600\n",
      "  Batch 190/661, Loss: 0.0546\n",
      "  Batch 200/661, Loss: 0.0784\n",
      "GPU Memory: Total=14.74GB | Reserved=0.23GB | Allocated=0.20GB | Free=14.51GB\n",
      "  Batch 210/661, Loss: 0.3696\n",
      "  Batch 220/661, Loss: 0.1423\n",
      "  Batch 230/661, Loss: 0.1122\n",
      "  Batch 240/661, Loss: 0.0389\n",
      "  Batch 250/661, Loss: 0.4157\n",
      "GPU Memory: Total=14.74GB | Reserved=0.23GB | Allocated=0.20GB | Free=14.51GB\n",
      "  Batch 260/661, Loss: 0.5257\n",
      "  Batch 270/661, Loss: 0.8613\n",
      "  Batch 280/661, Loss: 0.2141\n",
      "  Batch 290/661, Loss: 0.0366\n",
      "  Batch 300/661, Loss: 0.0459\n",
      "GPU Memory: Total=14.74GB | Reserved=0.23GB | Allocated=0.20GB | Free=14.51GB\n",
      "  Batch 310/661, Loss: 0.1406\n",
      "  Batch 320/661, Loss: 0.7571\n",
      "  Batch 330/661, Loss: 0.0009\n",
      "  Batch 340/661, Loss: 0.8494\n",
      "  Batch 350/661, Loss: 0.3382\n",
      "GPU Memory: Total=14.74GB | Reserved=0.23GB | Allocated=0.20GB | Free=14.51GB\n",
      "  Batch 360/661, Loss: 0.0073\n",
      "  Batch 370/661, Loss: 0.2728\n",
      "  Batch 380/661, Loss: 0.0059\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[h264 @ 0x5f4b74c0] mb_type 104 in P slice too large at 98 31\n",
      "[h264 @ 0x5f4b74c0] error while decoding MB 98 31\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 390/661, Loss: 0.0252\n",
      "  Batch 400/661, Loss: 1.0778\n",
      "GPU Memory: Total=14.74GB | Reserved=0.23GB | Allocated=0.20GB | Free=14.51GB\n",
      "  Batch 410/661, Loss: 0.0035\n",
      "  Batch 420/661, Loss: 0.2076\n",
      "  Batch 430/661, Loss: 0.0022\n",
      "  Batch 440/661, Loss: 0.0041\n",
      "  Batch 450/661, Loss: 0.0031\n",
      "GPU Memory: Total=14.74GB | Reserved=0.23GB | Allocated=0.20GB | Free=14.51GB\n",
      "  Batch 460/661, Loss: 0.0100\n",
      "  Batch 470/661, Loss: 0.0014\n",
      "  Batch 480/661, Loss: 0.0031\n",
      "  Batch 490/661, Loss: 0.2790\n",
      "  Batch 500/661, Loss: 0.4888\n",
      "GPU Memory: Total=14.74GB | Reserved=0.23GB | Allocated=0.20GB | Free=14.51GB\n",
      "  Batch 510/661, Loss: 0.0359\n",
      "  Batch 520/661, Loss: 0.1188\n",
      "  Batch 530/661, Loss: 0.0512\n",
      "  Batch 540/661, Loss: 0.2664\n",
      "  Batch 550/661, Loss: 0.0168\n",
      "GPU Memory: Total=14.74GB | Reserved=0.23GB | Allocated=0.20GB | Free=14.51GB\n",
      "  Batch 560/661, Loss: 0.0640\n",
      "  Batch 570/661, Loss: 0.6964\n",
      "  Batch 580/661, Loss: 0.0376\n",
      "  Batch 590/661, Loss: 0.1338\n",
      "  Batch 600/661, Loss: 0.0617\n",
      "GPU Memory: Total=14.74GB | Reserved=0.23GB | Allocated=0.20GB | Free=14.51GB\n",
      "  Batch 610/661, Loss: 1.5278\n",
      "  Batch 620/661, Loss: 1.2339\n",
      "  Batch 630/661, Loss: 0.0069\n",
      "  Batch 640/661, Loss: 1.2064\n",
      "  Batch 650/661, Loss: 0.0117\n",
      "GPU Memory: Total=14.74GB | Reserved=0.23GB | Allocated=0.20GB | Free=14.51GB\n",
      "  Batch 660/661, Loss: 0.7164\n",
      "Starting validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31/1455025713.py:22: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31/4126126155.py:134: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n",
      "/tmp/ipykernel_31/169455789.py:98: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=True):  # Use mixed precision\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 10/661, Loss: 0.4945\n",
      "  Batch 20/661, Loss: 0.0679\n",
      "  Batch 30/661, Loss: 0.0337\n",
      "  Batch 40/661, Loss: 0.1936\n",
      "  Batch 50/661, Loss: 0.0044\n",
      "GPU Memory: Total=14.74GB | Reserved=0.23GB | Allocated=0.20GB | Free=14.51GB\n",
      "  Batch 60/661, Loss: 0.0029\n",
      "  Batch 70/661, Loss: 0.0987\n",
      "  Batch 80/661, Loss: 0.0040\n",
      "  Batch 90/661, Loss: 0.5613\n",
      "  Batch 100/661, Loss: 0.0530\n",
      "GPU Memory: Total=14.74GB | Reserved=0.23GB | Allocated=0.20GB | Free=14.51GB\n",
      "  Batch 110/661, Loss: 0.0004\n",
      "  Batch 120/661, Loss: 0.0016\n",
      "  Batch 130/661, Loss: 0.0551\n",
      "  Batch 140/661, Loss: 0.0032\n",
      "  Batch 150/661, Loss: 0.0490\n",
      "GPU Memory: Total=14.74GB | Reserved=0.23GB | Allocated=0.20GB | Free=14.51GB\n",
      "  Batch 160/661, Loss: 0.0232\n",
      "  Batch 170/661, Loss: 0.5184\n",
      "  Batch 180/661, Loss: 0.8106\n",
      "  Batch 190/661, Loss: 0.0414\n",
      "  Batch 200/661, Loss: 0.0096\n",
      "GPU Memory: Total=14.74GB | Reserved=0.23GB | Allocated=0.20GB | Free=14.51GB\n",
      "  Batch 210/661, Loss: 0.0098\n",
      "  Batch 220/661, Loss: 0.0009\n",
      "  Batch 230/661, Loss: 0.0170\n",
      "  Batch 240/661, Loss: 0.7676\n",
      "  Batch 250/661, Loss: 0.1076\n",
      "GPU Memory: Total=14.74GB | Reserved=0.23GB | Allocated=0.20GB | Free=14.51GB\n",
      "  Batch 260/661, Loss: 0.0108\n",
      "  Batch 270/661, Loss: 0.0368\n",
      "  Batch 280/661, Loss: 0.0033\n",
      "  Batch 290/661, Loss: 0.0362\n",
      "  Batch 300/661, Loss: 0.0133\n",
      "GPU Memory: Total=14.74GB | Reserved=0.23GB | Allocated=0.20GB | Free=14.51GB\n",
      "  Batch 310/661, Loss: 0.5926\n",
      "  Batch 320/661, Loss: 0.0006\n",
      "  Batch 330/661, Loss: 0.0008\n",
      "  Batch 340/661, Loss: 0.0006\n",
      "  Batch 350/661, Loss: 0.0019\n",
      "GPU Memory: Total=14.74GB | Reserved=0.23GB | Allocated=0.20GB | Free=14.51GB\n",
      "  Batch 360/661, Loss: 0.0268\n",
      "  Batch 370/661, Loss: 0.0025\n",
      "  Batch 380/661, Loss: 0.0013\n",
      "  Batch 390/661, Loss: 0.0021\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[h264 @ 0x60c40ec0] mb_type 104 in P slice too large at 98 31\n",
      "[h264 @ 0x60c40ec0] error while decoding MB 98 31\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 400/661, Loss: 0.0330\n",
      "GPU Memory: Total=14.74GB | Reserved=0.23GB | Allocated=0.20GB | Free=14.51GB\n",
      "  Batch 410/661, Loss: 0.0010\n",
      "  Batch 420/661, Loss: 0.0113\n",
      "  Batch 430/661, Loss: 0.1354\n",
      "  Batch 440/661, Loss: 0.1161\n",
      "  Batch 450/661, Loss: 0.2153\n",
      "GPU Memory: Total=14.74GB | Reserved=0.23GB | Allocated=0.20GB | Free=14.51GB\n",
      "  Batch 460/661, Loss: 0.0008\n",
      "  Batch 470/661, Loss: 0.0039\n",
      "  Batch 480/661, Loss: 0.5920\n",
      "  Batch 490/661, Loss: 1.0395\n",
      "  Batch 500/661, Loss: 0.0011\n",
      "GPU Memory: Total=14.74GB | Reserved=0.23GB | Allocated=0.20GB | Free=14.51GB\n",
      "  Batch 510/661, Loss: 0.0088\n",
      "  Batch 520/661, Loss: 0.5132\n",
      "  Batch 530/661, Loss: 0.4397\n",
      "  Batch 540/661, Loss: 0.3060\n",
      "  Batch 550/661, Loss: 0.0019\n",
      "GPU Memory: Total=14.74GB | Reserved=0.23GB | Allocated=0.20GB | Free=14.51GB\n",
      "  Batch 560/661, Loss: 0.0039\n",
      "  Batch 570/661, Loss: 0.2930\n",
      "  Batch 580/661, Loss: 0.3120\n",
      "  Batch 590/661, Loss: 0.0053\n",
      "  Batch 600/661, Loss: 0.0100\n",
      "GPU Memory: Total=14.74GB | Reserved=0.23GB | Allocated=0.20GB | Free=14.51GB\n",
      "  Batch 610/661, Loss: 0.0534\n",
      "  Batch 620/661, Loss: 0.0248\n",
      "  Batch 630/661, Loss: 0.2504\n",
      "  Batch 640/661, Loss: 0.3437\n",
      "  Batch 650/661, Loss: 0.8938\n",
      "GPU Memory: Total=14.74GB | Reserved=0.23GB | Allocated=0.20GB | Free=14.51GB\n",
      "  Batch 660/661, Loss: 0.0176\n",
      "Starting validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31/1455025713.py:22: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31/4126126155.py:134: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n",
      "/tmp/ipykernel_31/169455789.py:98: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=True):  # Use mixed precision\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 10/661, Loss: 0.0010\n",
      "  Batch 20/661, Loss: 0.0929\n",
      "  Batch 30/661, Loss: 0.0141\n",
      "  Batch 40/661, Loss: 0.6562\n",
      "  Batch 50/661, Loss: 0.0047\n",
      "GPU Memory: Total=14.74GB | Reserved=0.23GB | Allocated=0.20GB | Free=14.51GB\n",
      "  Batch 60/661, Loss: 0.0185\n",
      "  Batch 70/661, Loss: 0.5969\n",
      "  Batch 80/661, Loss: 0.0264\n",
      "  Batch 90/661, Loss: 0.0049\n",
      "  Batch 100/661, Loss: 0.0016\n",
      "GPU Memory: Total=14.74GB | Reserved=0.23GB | Allocated=0.20GB | Free=14.51GB\n",
      "  Batch 110/661, Loss: 0.0951\n",
      "  Batch 120/661, Loss: 0.2982\n",
      "  Batch 130/661, Loss: 0.1044\n",
      "  Batch 140/661, Loss: 0.0012\n",
      "  Batch 150/661, Loss: 0.0005\n",
      "GPU Memory: Total=14.74GB | Reserved=0.23GB | Allocated=0.20GB | Free=14.51GB\n",
      "  Batch 160/661, Loss: 0.3287\n",
      "  Batch 170/661, Loss: 1.7621\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[h264 @ 0x5f4c32c0] mb_type 104 in P slice too large at 98 31\n",
      "[h264 @ 0x5f4c32c0] error while decoding MB 98 31\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 180/661, Loss: 0.1187\n",
      "  Batch 190/661, Loss: 0.0610\n",
      "  Batch 200/661, Loss: 0.0067\n",
      "GPU Memory: Total=14.74GB | Reserved=0.23GB | Allocated=0.20GB | Free=14.51GB\n",
      "  Batch 210/661, Loss: 0.8623\n",
      "  Batch 220/661, Loss: 0.0023\n",
      "  Batch 230/661, Loss: 0.0026\n",
      "  Batch 240/661, Loss: 0.0050\n",
      "  Batch 250/661, Loss: 0.0058\n",
      "GPU Memory: Total=14.74GB | Reserved=0.23GB | Allocated=0.20GB | Free=14.51GB\n",
      "  Batch 260/661, Loss: 0.6551\n",
      "  Batch 270/661, Loss: 0.0793\n",
      "  Batch 280/661, Loss: 0.0007\n",
      "  Batch 290/661, Loss: 0.0034\n",
      "  Batch 300/661, Loss: 0.1353\n",
      "GPU Memory: Total=14.74GB | Reserved=0.23GB | Allocated=0.20GB | Free=14.51GB\n",
      "  Batch 310/661, Loss: 0.1347\n",
      "  Batch 320/661, Loss: 1.6640\n",
      "  Batch 330/661, Loss: 1.0295\n",
      "  Batch 340/661, Loss: 0.0020\n",
      "  Batch 350/661, Loss: 0.4215\n",
      "GPU Memory: Total=14.74GB | Reserved=0.23GB | Allocated=0.20GB | Free=14.51GB\n",
      "  Batch 360/661, Loss: 0.0956\n",
      "  Batch 370/661, Loss: 0.0797\n",
      "  Batch 380/661, Loss: 0.0036\n",
      "  Batch 390/661, Loss: 0.0271\n",
      "  Batch 400/661, Loss: 0.6868\n",
      "GPU Memory: Total=14.74GB | Reserved=0.23GB | Allocated=0.20GB | Free=14.51GB\n",
      "  Batch 410/661, Loss: 0.3474\n",
      "  Batch 420/661, Loss: 0.2673\n",
      "  Batch 430/661, Loss: 0.2306\n",
      "  Batch 440/661, Loss: 0.3222\n",
      "  Batch 450/661, Loss: 0.0017\n",
      "GPU Memory: Total=14.74GB | Reserved=0.23GB | Allocated=0.20GB | Free=14.51GB\n",
      "  Batch 460/661, Loss: 0.1197\n",
      "  Batch 470/661, Loss: 1.2094\n",
      "  Batch 480/661, Loss: 0.2937\n",
      "  Batch 490/661, Loss: 0.6873\n",
      "  Batch 500/661, Loss: 0.0217\n",
      "GPU Memory: Total=14.74GB | Reserved=0.23GB | Allocated=0.20GB | Free=14.51GB\n",
      "  Batch 510/661, Loss: 0.2015\n",
      "  Batch 520/661, Loss: 0.0305\n",
      "  Batch 530/661, Loss: 0.0283\n",
      "  Batch 540/661, Loss: 0.0031\n",
      "  Batch 550/661, Loss: 0.0030\n",
      "GPU Memory: Total=14.74GB | Reserved=0.23GB | Allocated=0.20GB | Free=14.51GB\n",
      "  Batch 560/661, Loss: 0.0032\n",
      "  Batch 570/661, Loss: 0.0214\n",
      "  Batch 580/661, Loss: 0.0136\n",
      "  Batch 590/661, Loss: 0.6211\n",
      "  Batch 600/661, Loss: 0.3774\n",
      "GPU Memory: Total=14.74GB | Reserved=0.23GB | Allocated=0.20GB | Free=14.51GB\n",
      "  Batch 610/661, Loss: 0.0118\n",
      "  Batch 620/661, Loss: 0.0111\n",
      "  Batch 630/661, Loss: 0.0052\n",
      "  Batch 640/661, Loss: 0.0645\n",
      "  Batch 650/661, Loss: 0.0018\n",
      "GPU Memory: Total=14.74GB | Reserved=0.23GB | Allocated=0.20GB | Free=14.51GB\n",
      "  Batch 660/661, Loss: 0.0023\n",
      "Starting validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31/1455025713.py:22: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unfreezing last 2 of 2 feature extractor layers\n",
      "GPU Memory: Total=14.74GB | Reserved=0.26GB | Allocated=0.20GB | Free=14.48GB\n",
      "Epoch 16/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31/4126126155.py:134: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n",
      "/tmp/ipykernel_31/169455789.py:98: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=True):  # Use mixed precision\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 10/661, Loss: 0.0028\n",
      "  Batch 20/661, Loss: 0.0008\n",
      "  Batch 30/661, Loss: 0.1170\n",
      "  Batch 40/661, Loss: 0.0726\n",
      "  Batch 50/661, Loss: 0.0012\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 60/661, Loss: 0.0012\n",
      "  Batch 70/661, Loss: 0.0159\n",
      "  Batch 80/661, Loss: 0.0034\n",
      "  Batch 90/661, Loss: 1.1858\n",
      "  Batch 100/661, Loss: 0.0270\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 110/661, Loss: 0.0186\n",
      "  Batch 120/661, Loss: 0.0016\n",
      "  Batch 130/661, Loss: 0.0034\n",
      "  Batch 140/661, Loss: 0.3255\n",
      "  Batch 150/661, Loss: 0.0079\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 160/661, Loss: 0.0342\n",
      "  Batch 170/661, Loss: 0.5455\n",
      "  Batch 180/661, Loss: 1.8215\n",
      "  Batch 190/661, Loss: 0.4079\n",
      "  Batch 200/661, Loss: 0.5471\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 210/661, Loss: 0.0310\n",
      "  Batch 220/661, Loss: 0.9960\n",
      "  Batch 230/661, Loss: 1.2060\n",
      "  Batch 240/661, Loss: 0.0054\n",
      "  Batch 250/661, Loss: 0.4429\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 260/661, Loss: 0.0021\n",
      "  Batch 270/661, Loss: 1.4865\n",
      "  Batch 280/661, Loss: 0.6521\n",
      "  Batch 290/661, Loss: 0.0110\n",
      "  Batch 300/661, Loss: 0.0028\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 310/661, Loss: 0.0264\n",
      "  Batch 320/661, Loss: 0.7559\n",
      "  Batch 330/661, Loss: 0.0068\n",
      "  Batch 340/661, Loss: 0.0303\n",
      "  Batch 350/661, Loss: 0.0034\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 360/661, Loss: 0.0067\n",
      "  Batch 370/661, Loss: 0.0043\n",
      "  Batch 380/661, Loss: 0.0019\n",
      "  Batch 390/661, Loss: 0.6939\n",
      "  Batch 400/661, Loss: 0.1159\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 410/661, Loss: 1.1928\n",
      "  Batch 420/661, Loss: 0.3968\n",
      "  Batch 430/661, Loss: 0.2587\n",
      "  Batch 440/661, Loss: 0.1429\n",
      "  Batch 450/661, Loss: 0.0264\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 460/661, Loss: 0.0851\n",
      "  Batch 470/661, Loss: 0.0056\n",
      "  Batch 480/661, Loss: 1.5812\n",
      "  Batch 490/661, Loss: 0.5410\n",
      "  Batch 500/661, Loss: 0.0042\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 510/661, Loss: 0.6776\n",
      "  Batch 520/661, Loss: 0.0081\n",
      "  Batch 530/661, Loss: 0.9170\n",
      "  Batch 540/661, Loss: 0.2805\n",
      "  Batch 550/661, Loss: 0.0884\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 560/661, Loss: 0.0052\n",
      "  Batch 570/661, Loss: 0.0046\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[h264 @ 0x5f978600] mb_type 104 in P slice too large at 98 31\n",
      "[h264 @ 0x5f978600] error while decoding MB 98 31\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 580/661, Loss: 0.0051\n",
      "  Batch 590/661, Loss: 0.0112\n",
      "  Batch 600/661, Loss: 0.1309\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 610/661, Loss: 0.3218\n",
      "  Batch 620/661, Loss: 0.6381\n",
      "  Batch 630/661, Loss: 0.0231\n",
      "  Batch 640/661, Loss: 0.0707\n",
      "  Batch 650/661, Loss: 0.0075\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 660/661, Loss: 0.5222\n",
      "Starting validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31/1455025713.py:22: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31/4126126155.py:134: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n",
      "/tmp/ipykernel_31/169455789.py:98: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=True):  # Use mixed precision\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 10/661, Loss: 0.0030\n",
      "  Batch 20/661, Loss: 0.0021\n",
      "  Batch 30/661, Loss: 0.6883\n",
      "  Batch 40/661, Loss: 0.0062\n",
      "  Batch 50/661, Loss: 0.0174\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[h264 @ 0x5f8ffd40] mb_type 104 in P slice too large at 98 31\n",
      "[h264 @ 0x5f8ffd40] error while decoding MB 98 31\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 60/661, Loss: 0.0041\n",
      "  Batch 70/661, Loss: 0.0026\n",
      "  Batch 80/661, Loss: 0.0312\n",
      "  Batch 90/661, Loss: 0.0669\n",
      "  Batch 100/661, Loss: 0.0086\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 110/661, Loss: 0.7312\n",
      "  Batch 120/661, Loss: 0.0017\n",
      "  Batch 130/661, Loss: 0.0292\n",
      "  Batch 140/661, Loss: 0.0018\n",
      "  Batch 150/661, Loss: 0.5115\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 160/661, Loss: 0.4846\n",
      "  Batch 170/661, Loss: 0.0081\n",
      "  Batch 180/661, Loss: 0.6185\n",
      "  Batch 190/661, Loss: 0.6031\n",
      "  Batch 200/661, Loss: 0.5785\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 210/661, Loss: 0.0027\n",
      "  Batch 220/661, Loss: 0.6330\n",
      "  Batch 230/661, Loss: 0.0068\n",
      "  Batch 240/661, Loss: 0.0052\n",
      "  Batch 250/661, Loss: 0.0025\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 260/661, Loss: 0.0025\n",
      "  Batch 270/661, Loss: 0.0249\n",
      "  Batch 280/661, Loss: 0.0032\n",
      "  Batch 290/661, Loss: 0.5168\n",
      "  Batch 300/661, Loss: 0.8928\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 310/661, Loss: 0.0012\n",
      "  Batch 320/661, Loss: 1.5238\n",
      "  Batch 330/661, Loss: 1.7021\n",
      "  Batch 340/661, Loss: 0.7559\n",
      "  Batch 350/661, Loss: 0.0020\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 360/661, Loss: 0.0009\n",
      "  Batch 370/661, Loss: 0.0042\n",
      "  Batch 380/661, Loss: 0.7510\n",
      "  Batch 390/661, Loss: 0.0014\n",
      "  Batch 400/661, Loss: 0.1337\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 410/661, Loss: 0.8503\n",
      "  Batch 420/661, Loss: 0.0014\n",
      "  Batch 430/661, Loss: 0.2288\n",
      "  Batch 440/661, Loss: 0.0016\n",
      "  Batch 450/661, Loss: 0.0017\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 460/661, Loss: 0.0007\n",
      "  Batch 470/661, Loss: 0.0012\n",
      "  Batch 480/661, Loss: 0.3635\n",
      "  Batch 490/661, Loss: 0.0023\n",
      "  Batch 500/661, Loss: 0.0023\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 510/661, Loss: 0.0034\n",
      "  Batch 520/661, Loss: 0.6100\n",
      "  Batch 530/661, Loss: 0.0009\n",
      "  Batch 540/661, Loss: 0.0008\n",
      "  Batch 550/661, Loss: 0.0008\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 560/661, Loss: 0.0007\n",
      "  Batch 570/661, Loss: 0.0006\n",
      "  Batch 580/661, Loss: 0.0034\n",
      "  Batch 590/661, Loss: 0.1257\n",
      "  Batch 600/661, Loss: 0.8464\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 610/661, Loss: 0.4843\n",
      "  Batch 620/661, Loss: 0.7415\n",
      "  Batch 630/661, Loss: 0.0028\n",
      "  Batch 640/661, Loss: 0.6195\n",
      "  Batch 650/661, Loss: 0.0007\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 660/661, Loss: 0.0495\n",
      "Starting validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31/1455025713.py:22: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31/4126126155.py:134: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n",
      "/tmp/ipykernel_31/169455789.py:98: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=True):  # Use mixed precision\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 10/661, Loss: 0.0006\n",
      "  Batch 20/661, Loss: 0.0017\n",
      "  Batch 30/661, Loss: 0.0019\n",
      "  Batch 40/661, Loss: 0.0028\n",
      "  Batch 50/661, Loss: 0.0014\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 60/661, Loss: 0.0032\n",
      "  Batch 70/661, Loss: 0.0005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[h264 @ 0x5f91b8c0] mb_type 104 in P slice too large at 98 31\n",
      "[h264 @ 0x5f91b8c0] error while decoding MB 98 31\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 80/661, Loss: 0.0022\n",
      "  Batch 90/661, Loss: 0.0012\n",
      "  Batch 100/661, Loss: 0.0031\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 110/661, Loss: 0.0009\n",
      "  Batch 120/661, Loss: 0.0044\n",
      "  Batch 130/661, Loss: 0.0037\n",
      "  Batch 140/661, Loss: 0.0018\n",
      "  Batch 150/661, Loss: 0.4554\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 160/661, Loss: 0.0009\n",
      "  Batch 170/661, Loss: 0.0005\n",
      "  Batch 180/661, Loss: 0.0013\n",
      "  Batch 190/661, Loss: 0.0010\n",
      "  Batch 200/661, Loss: 0.0012\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 210/661, Loss: 0.0582\n",
      "  Batch 220/661, Loss: 0.2645\n",
      "  Batch 230/661, Loss: 0.0024\n",
      "  Batch 240/661, Loss: 0.0007\n",
      "  Batch 250/661, Loss: 0.0298\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 260/661, Loss: 0.0191\n",
      "  Batch 270/661, Loss: 0.0005\n",
      "  Batch 280/661, Loss: 0.5272\n",
      "  Batch 290/661, Loss: 0.0014\n",
      "  Batch 300/661, Loss: 0.0021\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 310/661, Loss: 0.8773\n",
      "  Batch 320/661, Loss: 0.0006\n",
      "  Batch 330/661, Loss: 0.0005\n",
      "  Batch 340/661, Loss: 0.0003\n",
      "  Batch 350/661, Loss: 0.3522\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 360/661, Loss: 0.0006\n",
      "  Batch 370/661, Loss: 0.0009\n",
      "  Batch 380/661, Loss: 0.1774\n",
      "  Batch 390/661, Loss: 0.0022\n",
      "  Batch 400/661, Loss: 0.0016\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 410/661, Loss: 0.0027\n",
      "  Batch 420/661, Loss: 0.0012\n",
      "  Batch 430/661, Loss: 0.0010\n",
      "  Batch 440/661, Loss: 0.0014\n",
      "  Batch 450/661, Loss: 0.0013\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 460/661, Loss: 0.0003\n",
      "  Batch 470/661, Loss: 0.0005\n",
      "  Batch 480/661, Loss: 0.8320\n",
      "  Batch 490/661, Loss: 0.0003\n",
      "  Batch 500/661, Loss: 0.0002\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 510/661, Loss: 0.0020\n",
      "  Batch 520/661, Loss: 0.0004\n",
      "  Batch 530/661, Loss: 0.0002\n",
      "  Batch 540/661, Loss: 0.0002\n",
      "  Batch 550/661, Loss: 0.6452\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 560/661, Loss: 0.0003\n",
      "  Batch 570/661, Loss: 0.0004\n",
      "  Batch 580/661, Loss: 0.0008\n",
      "  Batch 590/661, Loss: 0.0031\n",
      "  Batch 600/661, Loss: 0.3484\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 610/661, Loss: 0.0012\n",
      "  Batch 620/661, Loss: 0.9329\n",
      "  Batch 630/661, Loss: 0.8611\n",
      "  Batch 640/661, Loss: 0.0006\n",
      "  Batch 650/661, Loss: 0.0023\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 660/661, Loss: 0.0009\n",
      "Starting validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31/1455025713.py:22: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31/4126126155.py:134: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n",
      "/tmp/ipykernel_31/169455789.py:98: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=True):  # Use mixed precision\n",
      "[h264 @ 0x5fbcbf00] mb_type 104 in P slice too large at 98 31\n",
      "[h264 @ 0x5fbcbf00] error while decoding MB 98 31\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 10/661, Loss: 0.6549\n",
      "  Batch 20/661, Loss: 0.0010\n",
      "  Batch 30/661, Loss: 0.0026\n",
      "  Batch 40/661, Loss: 0.0006\n",
      "  Batch 50/661, Loss: 0.8512\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 60/661, Loss: 0.0070\n",
      "  Batch 70/661, Loss: 0.4974\n",
      "  Batch 80/661, Loss: 0.0007\n",
      "  Batch 90/661, Loss: 0.0012\n",
      "  Batch 100/661, Loss: 0.0428\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 110/661, Loss: 0.0011\n",
      "  Batch 120/661, Loss: 0.0005\n",
      "  Batch 130/661, Loss: 0.0006\n",
      "  Batch 140/661, Loss: 0.0072\n",
      "  Batch 150/661, Loss: 0.0003\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 160/661, Loss: 0.0002\n",
      "  Batch 170/661, Loss: 0.0009\n",
      "  Batch 180/661, Loss: 0.8012\n",
      "  Batch 190/661, Loss: 0.0009\n",
      "  Batch 200/661, Loss: 0.0641\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 210/661, Loss: 0.7939\n",
      "  Batch 220/661, Loss: 0.0005\n",
      "  Batch 230/661, Loss: 0.0008\n",
      "  Batch 240/661, Loss: 0.0012\n",
      "  Batch 250/661, Loss: 0.0021\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 260/661, Loss: 0.0006\n",
      "  Batch 270/661, Loss: 0.4991\n",
      "  Batch 280/661, Loss: 0.0008\n",
      "  Batch 290/661, Loss: 0.0003\n",
      "  Batch 300/661, Loss: 0.0003\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 310/661, Loss: 0.0004\n",
      "  Batch 320/661, Loss: 0.5652\n",
      "  Batch 330/661, Loss: 0.0003\n",
      "  Batch 340/661, Loss: 0.0020\n",
      "  Batch 350/661, Loss: 0.1239\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 360/661, Loss: 0.0008\n",
      "  Batch 370/661, Loss: 0.0002\n",
      "  Batch 380/661, Loss: 0.0003\n",
      "  Batch 390/661, Loss: 0.3741\n",
      "  Batch 400/661, Loss: 0.0003\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 410/661, Loss: 0.0003\n",
      "  Batch 420/661, Loss: 0.7928\n",
      "  Batch 430/661, Loss: 0.1222\n",
      "  Batch 440/661, Loss: 0.0403\n",
      "  Batch 450/661, Loss: 0.0009\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 460/661, Loss: 0.0020\n",
      "  Batch 470/661, Loss: 0.9237\n",
      "  Batch 480/661, Loss: 0.0012\n",
      "  Batch 490/661, Loss: 0.0002\n",
      "  Batch 500/661, Loss: 0.0009\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 510/661, Loss: 0.0011\n",
      "  Batch 520/661, Loss: 0.0011\n",
      "  Batch 530/661, Loss: 0.0014\n",
      "  Batch 540/661, Loss: 0.7910\n",
      "  Batch 550/661, Loss: 0.0027\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 560/661, Loss: 0.6302\n",
      "  Batch 570/661, Loss: 0.0003\n",
      "  Batch 580/661, Loss: 0.0117\n",
      "  Batch 590/661, Loss: 0.0006\n",
      "  Batch 600/661, Loss: 0.0203\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 610/661, Loss: 0.0005\n",
      "  Batch 620/661, Loss: 0.6551\n",
      "  Batch 630/661, Loss: 0.0006\n",
      "  Batch 640/661, Loss: 0.0005\n",
      "  Batch 650/661, Loss: 0.0034\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 660/661, Loss: 0.0003\n",
      "Starting validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31/1455025713.py:22: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31/4126126155.py:134: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n",
      "/tmp/ipykernel_31/169455789.py:98: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=True):  # Use mixed precision\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 10/661, Loss: 0.0002\n",
      "  Batch 20/661, Loss: 0.0004\n",
      "  Batch 30/661, Loss: 0.0003\n",
      "  Batch 40/661, Loss: 0.0003\n",
      "  Batch 50/661, Loss: 0.0007\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 60/661, Loss: 0.0003\n",
      "  Batch 70/661, Loss: 0.0003\n",
      "  Batch 80/661, Loss: 0.0004\n",
      "  Batch 90/661, Loss: 0.7800\n",
      "  Batch 100/661, Loss: 0.0004\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 110/661, Loss: 0.0003\n",
      "  Batch 120/661, Loss: 0.0008\n",
      "  Batch 130/661, Loss: 0.0004\n",
      "  Batch 140/661, Loss: 0.0007\n",
      "  Batch 150/661, Loss: 0.0003\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 160/661, Loss: 0.0004\n",
      "  Batch 170/661, Loss: 0.0002\n",
      "  Batch 180/661, Loss: 0.6637\n",
      "  Batch 190/661, Loss: 0.0004\n",
      "  Batch 200/661, Loss: 1.6697\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 210/661, Loss: 0.0015\n",
      "  Batch 220/661, Loss: 0.0003\n",
      "  Batch 230/661, Loss: 0.0002\n",
      "  Batch 240/661, Loss: 0.0004\n",
      "  Batch 250/661, Loss: 0.0004\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 260/661, Loss: 0.0003\n",
      "  Batch 270/661, Loss: 0.0001\n",
      "  Batch 280/661, Loss: 0.0002\n",
      "  Batch 290/661, Loss: 0.0006\n",
      "  Batch 300/661, Loss: 0.0005\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 310/661, Loss: 0.0004\n",
      "  Batch 320/661, Loss: 0.0017\n",
      "  Batch 330/661, Loss: 0.0006\n",
      "  Batch 340/661, Loss: 0.0002\n",
      "  Batch 350/661, Loss: 0.0003\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 360/661, Loss: 0.0160\n",
      "  Batch 370/661, Loss: 0.7596\n",
      "  Batch 380/661, Loss: 0.0006\n",
      "  Batch 390/661, Loss: 0.5810\n",
      "  Batch 400/661, Loss: 0.0002\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 410/661, Loss: 0.0019\n",
      "  Batch 420/661, Loss: 0.0001\n",
      "  Batch 430/661, Loss: 0.0006\n",
      "  Batch 440/661, Loss: 0.0008\n",
      "  Batch 450/661, Loss: 0.0010\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 460/661, Loss: 0.4540\n",
      "  Batch 470/661, Loss: 0.0004\n",
      "  Batch 480/661, Loss: 0.0009\n",
      "  Batch 490/661, Loss: 0.3719\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[h264 @ 0x5faafe40] mb_type 104 in P slice too large at 98 31\n",
      "[h264 @ 0x5faafe40] error while decoding MB 98 31\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 500/661, Loss: 0.0016\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 510/661, Loss: 0.0004\n",
      "  Batch 520/661, Loss: 0.0007\n",
      "  Batch 530/661, Loss: 0.0019\n",
      "  Batch 540/661, Loss: 0.0004\n",
      "  Batch 550/661, Loss: 0.4093\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 560/661, Loss: 0.4839\n",
      "  Batch 570/661, Loss: 0.0006\n",
      "  Batch 580/661, Loss: 0.0002\n",
      "  Batch 590/661, Loss: 0.0028\n",
      "  Batch 600/661, Loss: 0.0002\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 610/661, Loss: 0.0008\n",
      "  Batch 620/661, Loss: 0.0258\n",
      "  Batch 630/661, Loss: 0.0003\n",
      "  Batch 640/661, Loss: 0.0390\n",
      "  Batch 650/661, Loss: 0.0001\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 660/661, Loss: 0.0001\n",
      "Starting validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31/1455025713.py:22: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unfreezing last 3 of 2 feature extractor layers\n",
      "GPU Memory: Total=14.74GB | Reserved=0.80GB | Allocated=0.52GB | Free=13.94GB\n",
      "Epoch 21/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31/4126126155.py:134: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n",
      "/tmp/ipykernel_31/169455789.py:98: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=True):  # Use mixed precision\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 10/661, Loss: 0.0000\n",
      "  Batch 20/661, Loss: 0.0002\n",
      "  Batch 30/661, Loss: 0.0001\n",
      "  Batch 40/661, Loss: 0.0001\n",
      "  Batch 50/661, Loss: 0.0001\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 60/661, Loss: 0.0001\n",
      "  Batch 70/661, Loss: 0.0001\n",
      "  Batch 80/661, Loss: 0.0001\n",
      "  Batch 90/661, Loss: 1.0658\n",
      "  Batch 100/661, Loss: 0.0002\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 110/661, Loss: 1.0014\n",
      "  Batch 120/661, Loss: 0.0005\n",
      "  Batch 130/661, Loss: 0.0013\n",
      "  Batch 140/661, Loss: 0.0001\n",
      "  Batch 150/661, Loss: 0.0002\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 160/661, Loss: 0.8401\n",
      "  Batch 170/661, Loss: 0.0017\n",
      "  Batch 180/661, Loss: 0.0002\n",
      "  Batch 190/661, Loss: 0.0002\n",
      "  Batch 200/661, Loss: 0.0002\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 210/661, Loss: 0.0003\n",
      "  Batch 220/661, Loss: 0.0051\n",
      "  Batch 230/661, Loss: 0.0002\n",
      "  Batch 240/661, Loss: 0.0003\n",
      "  Batch 250/661, Loss: 0.7216\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 260/661, Loss: 0.0005\n",
      "  Batch 270/661, Loss: 0.0037\n",
      "  Batch 280/661, Loss: 0.0001\n",
      "  Batch 290/661, Loss: 0.0005\n",
      "  Batch 300/661, Loss: 0.0026\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 310/661, Loss: 0.0002\n",
      "  Batch 320/661, Loss: 0.0004\n",
      "  Batch 330/661, Loss: 0.0003\n",
      "  Batch 340/661, Loss: 0.0024\n",
      "  Batch 350/661, Loss: 0.0001\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 360/661, Loss: 0.0117\n",
      "  Batch 370/661, Loss: 0.0019\n",
      "  Batch 380/661, Loss: 0.3882\n",
      "  Batch 390/661, Loss: 0.0006\n",
      "  Batch 400/661, Loss: 0.0001\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 410/661, Loss: 0.7374\n",
      "  Batch 420/661, Loss: 0.0008\n",
      "  Batch 430/661, Loss: 0.0002\n",
      "  Batch 440/661, Loss: 0.0001\n",
      "  Batch 450/661, Loss: 0.0002\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 460/661, Loss: 0.0002\n",
      "  Batch 470/661, Loss: 0.0007\n",
      "  Batch 480/661, Loss: 1.9646\n",
      "  Batch 490/661, Loss: 0.0004\n",
      "  Batch 500/661, Loss: 0.0004\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 510/661, Loss: 0.0004\n",
      "  Batch 520/661, Loss: 0.0012\n",
      "  Batch 530/661, Loss: 1.1173\n",
      "  Batch 540/661, Loss: 0.0036\n",
      "  Batch 550/661, Loss: 0.0001\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[h264 @ 0x5c4b7d80] mb_type 104 in P slice too large at 98 31\n",
      "[h264 @ 0x5c4b7d80] error while decoding MB 98 31\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 560/661, Loss: 0.0003\n",
      "  Batch 570/661, Loss: 0.0008\n",
      "  Batch 580/661, Loss: 0.9587\n",
      "  Batch 590/661, Loss: 0.0009\n",
      "  Batch 600/661, Loss: 0.0014\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 610/661, Loss: 0.7903\n",
      "  Batch 620/661, Loss: 0.0004\n",
      "  Batch 630/661, Loss: 0.0005\n",
      "  Batch 640/661, Loss: 0.0003\n",
      "  Batch 650/661, Loss: 0.0012\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 660/661, Loss: 0.0002\n",
      "Starting validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31/1455025713.py:22: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31/4126126155.py:134: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n",
      "/tmp/ipykernel_31/169455789.py:98: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=True):  # Use mixed precision\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 10/661, Loss: 0.0016\n",
      "  Batch 20/661, Loss: 0.0008\n",
      "  Batch 30/661, Loss: 0.0640\n",
      "  Batch 40/661, Loss: 0.0018\n",
      "  Batch 50/661, Loss: 0.0001\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 60/661, Loss: 0.0001\n",
      "  Batch 70/661, Loss: 0.0001\n",
      "  Batch 80/661, Loss: 0.0001\n",
      "  Batch 90/661, Loss: 0.0007\n",
      "  Batch 100/661, Loss: 0.7960\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 110/661, Loss: 0.0002\n",
      "  Batch 120/661, Loss: 0.0004\n",
      "  Batch 130/661, Loss: 0.0026\n",
      "  Batch 140/661, Loss: 0.6954\n",
      "  Batch 150/661, Loss: 0.0689\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 160/661, Loss: 0.0015\n",
      "  Batch 170/661, Loss: 0.0001\n",
      "  Batch 180/661, Loss: 0.0003\n",
      "  Batch 190/661, Loss: 0.0004\n",
      "  Batch 200/661, Loss: 0.0008\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 210/661, Loss: 0.0005\n",
      "  Batch 220/661, Loss: 0.0004\n",
      "  Batch 230/661, Loss: 0.0005\n",
      "  Batch 240/661, Loss: 0.0001\n",
      "  Batch 250/661, Loss: 0.0001\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 260/661, Loss: 0.0002\n",
      "  Batch 270/661, Loss: 0.8734\n",
      "  Batch 280/661, Loss: 0.0005\n",
      "  Batch 290/661, Loss: 0.0067\n",
      "  Batch 300/661, Loss: 0.0001\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 310/661, Loss: 0.0001\n",
      "  Batch 320/661, Loss: 0.0001\n",
      "  Batch 330/661, Loss: 0.0006\n",
      "  Batch 340/661, Loss: 0.0002\n",
      "  Batch 350/661, Loss: 0.0002\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 360/661, Loss: 0.0012\n",
      "  Batch 370/661, Loss: 0.0003\n",
      "  Batch 380/661, Loss: 0.0001\n",
      "  Batch 390/661, Loss: 0.0000\n",
      "  Batch 400/661, Loss: 0.0000\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 410/661, Loss: 0.0001\n",
      "  Batch 420/661, Loss: 0.0001\n",
      "  Batch 430/661, Loss: 0.0003\n",
      "  Batch 440/661, Loss: 0.0010\n",
      "  Batch 450/661, Loss: 1.0960\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 460/661, Loss: 0.0002\n",
      "  Batch 470/661, Loss: 0.0006\n",
      "  Batch 480/661, Loss: 0.0004\n",
      "  Batch 490/661, Loss: 0.0007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[h264 @ 0x5f93de40] mb_type 104 in P slice too large at 98 31\n",
      "[h264 @ 0x5f93de40] error while decoding MB 98 31\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 500/661, Loss: 0.0008\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 510/661, Loss: 0.0002\n",
      "  Batch 520/661, Loss: 0.0002\n",
      "  Batch 530/661, Loss: 0.0005\n",
      "  Batch 540/661, Loss: 0.0007\n",
      "  Batch 550/661, Loss: 0.0002\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 560/661, Loss: 0.0001\n",
      "  Batch 570/661, Loss: 0.0001\n",
      "  Batch 580/661, Loss: 0.0001\n",
      "  Batch 590/661, Loss: 0.4813\n",
      "  Batch 600/661, Loss: 0.0001\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 610/661, Loss: 0.0008\n",
      "  Batch 620/661, Loss: 0.4763\n",
      "  Batch 630/661, Loss: 0.0019\n",
      "  Batch 640/661, Loss: 0.0028\n",
      "  Batch 650/661, Loss: 0.6173\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 660/661, Loss: 0.0004\n",
      "Starting validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31/1455025713.py:22: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31/4126126155.py:134: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n",
      "/tmp/ipykernel_31/169455789.py:98: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=True):  # Use mixed precision\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 10/661, Loss: 0.0001\n",
      "  Batch 20/661, Loss: 0.8454\n",
      "  Batch 30/661, Loss: 0.0004\n",
      "  Batch 40/661, Loss: 0.0002\n",
      "  Batch 50/661, Loss: 0.5430\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 60/661, Loss: 0.0002\n",
      "  Batch 70/661, Loss: 0.0001\n",
      "  Batch 80/661, Loss: 0.0000\n",
      "  Batch 90/661, Loss: 0.0000\n",
      "  Batch 100/661, Loss: 0.0002\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 110/661, Loss: 0.0012\n",
      "  Batch 120/661, Loss: 0.0528\n",
      "  Batch 130/661, Loss: 0.0001\n",
      "  Batch 140/661, Loss: 0.0001\n",
      "  Batch 150/661, Loss: 0.0002\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 160/661, Loss: 0.0001\n",
      "  Batch 170/661, Loss: 0.0001\n",
      "  Batch 180/661, Loss: 0.0001\n",
      "  Batch 190/661, Loss: 0.0012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[h264 @ 0x5faba6c0] mb_type 104 in P slice too large at 98 31\n",
      "[h264 @ 0x5faba6c0] error while decoding MB 98 31\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 200/661, Loss: 0.0001\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 210/661, Loss: 0.0003\n",
      "  Batch 220/661, Loss: 0.0003\n",
      "  Batch 230/661, Loss: 0.0000\n",
      "  Batch 240/661, Loss: 0.0005\n",
      "  Batch 250/661, Loss: 0.0001\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 260/661, Loss: 0.0007\n",
      "  Batch 270/661, Loss: 0.1061\n",
      "  Batch 280/661, Loss: 0.0963\n",
      "  Batch 290/661, Loss: 0.0001\n",
      "  Batch 300/661, Loss: 0.0007\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 310/661, Loss: 0.0086\n",
      "  Batch 320/661, Loss: 0.0001\n",
      "  Batch 330/661, Loss: 0.0000\n",
      "  Batch 340/661, Loss: 0.0000\n",
      "  Batch 350/661, Loss: 0.0000\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 360/661, Loss: 2.3614\n",
      "  Batch 370/661, Loss: 0.0004\n",
      "  Batch 380/661, Loss: 0.0006\n",
      "  Batch 390/661, Loss: 0.0001\n",
      "  Batch 400/661, Loss: 0.0001\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 410/661, Loss: 0.0007\n",
      "  Batch 420/661, Loss: 0.0002\n",
      "  Batch 430/661, Loss: 0.0002\n",
      "  Batch 440/661, Loss: 0.0002\n",
      "  Batch 450/661, Loss: 0.0030\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 460/661, Loss: 0.0001\n",
      "  Batch 470/661, Loss: 0.0001\n",
      "  Batch 480/661, Loss: 0.0003\n",
      "  Batch 490/661, Loss: 0.0001\n",
      "  Batch 500/661, Loss: 0.0001\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 510/661, Loss: 0.0009\n",
      "  Batch 520/661, Loss: 1.1641\n",
      "  Batch 530/661, Loss: 0.0000\n",
      "  Batch 540/661, Loss: 0.0001\n",
      "  Batch 550/661, Loss: 0.0007\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 560/661, Loss: 0.0001\n",
      "  Batch 570/661, Loss: 0.8004\n",
      "  Batch 580/661, Loss: 0.0001\n",
      "  Batch 590/661, Loss: 0.0001\n",
      "  Batch 600/661, Loss: 0.0004\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 610/661, Loss: 0.0013\n",
      "  Batch 620/661, Loss: 0.0002\n",
      "  Batch 630/661, Loss: 0.0004\n",
      "  Batch 640/661, Loss: 0.0026\n",
      "  Batch 650/661, Loss: 0.0001\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 660/661, Loss: 0.0002\n",
      "Starting validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31/1455025713.py:22: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31/4126126155.py:134: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n",
      "/tmp/ipykernel_31/169455789.py:98: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=True):  # Use mixed precision\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 10/661, Loss: 0.0000\n",
      "  Batch 20/661, Loss: 0.0000\n",
      "  Batch 30/661, Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[h264 @ 0x5f906900] mb_type 104 in P slice too large at 98 31\n",
      "[h264 @ 0x5f906900] error while decoding MB 98 31\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 40/661, Loss: 0.0000\n",
      "  Batch 50/661, Loss: 0.0001\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 60/661, Loss: 0.0057\n",
      "  Batch 70/661, Loss: 0.0003\n",
      "  Batch 80/661, Loss: 0.0005\n",
      "  Batch 90/661, Loss: 0.0002\n",
      "  Batch 100/661, Loss: 0.0002\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 110/661, Loss: 0.0002\n",
      "  Batch 120/661, Loss: 0.0001\n",
      "  Batch 130/661, Loss: 0.0007\n",
      "  Batch 140/661, Loss: 0.0001\n",
      "  Batch 150/661, Loss: 0.0001\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 160/661, Loss: 0.0003\n",
      "  Batch 170/661, Loss: 0.0002\n",
      "  Batch 180/661, Loss: 0.0001\n",
      "  Batch 190/661, Loss: 0.0002\n",
      "  Batch 200/661, Loss: 0.0000\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 210/661, Loss: 0.0002\n",
      "  Batch 220/661, Loss: 0.0001\n",
      "  Batch 230/661, Loss: 0.0005\n",
      "  Batch 240/661, Loss: 0.0002\n",
      "  Batch 250/661, Loss: 0.0002\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 260/661, Loss: 0.0001\n",
      "  Batch 270/661, Loss: 0.0000\n",
      "  Batch 280/661, Loss: 0.0001\n",
      "  Batch 290/661, Loss: 0.0001\n",
      "  Batch 300/661, Loss: 0.0000\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 310/661, Loss: 0.0087\n",
      "  Batch 320/661, Loss: 0.0000\n",
      "  Batch 330/661, Loss: 0.0000\n",
      "  Batch 340/661, Loss: 0.0007\n",
      "  Batch 350/661, Loss: 0.0001\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 360/661, Loss: 0.0001\n",
      "  Batch 370/661, Loss: 0.1435\n",
      "  Batch 380/661, Loss: 0.0001\n",
      "  Batch 390/661, Loss: 0.0000\n",
      "  Batch 400/661, Loss: 0.0000\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 410/661, Loss: 0.0000\n",
      "  Batch 420/661, Loss: 0.0001\n",
      "  Batch 430/661, Loss: 0.0001\n",
      "  Batch 440/661, Loss: 0.0005\n",
      "  Batch 450/661, Loss: 0.0001\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 460/661, Loss: 0.5435\n",
      "  Batch 470/661, Loss: 0.0008\n",
      "  Batch 480/661, Loss: 0.0000\n",
      "  Batch 490/661, Loss: 0.0001\n",
      "  Batch 500/661, Loss: 0.0001\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 510/661, Loss: 0.0000\n",
      "  Batch 520/661, Loss: 0.0003\n",
      "  Batch 530/661, Loss: 0.0001\n",
      "  Batch 540/661, Loss: 0.0015\n",
      "  Batch 550/661, Loss: 0.0001\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 560/661, Loss: 0.0000\n",
      "  Batch 570/661, Loss: 0.0000\n",
      "  Batch 580/661, Loss: 0.0002\n",
      "  Batch 590/661, Loss: 0.0002\n",
      "  Batch 600/661, Loss: 0.0513\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 610/661, Loss: 0.0001\n",
      "  Batch 620/661, Loss: 0.0000\n",
      "  Batch 630/661, Loss: 0.0000\n",
      "  Batch 640/661, Loss: 0.0000\n",
      "  Batch 650/661, Loss: 0.0000\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 660/661, Loss: 0.0001\n",
      "Starting validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31/1455025713.py:22: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31/4126126155.py:134: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n",
      "/tmp/ipykernel_31/169455789.py:98: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=True):  # Use mixed precision\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 10/661, Loss: 0.0000\n",
      "  Batch 20/661, Loss: 0.0001\n",
      "  Batch 30/661, Loss: 0.0016\n",
      "  Batch 40/661, Loss: 0.0000\n",
      "  Batch 50/661, Loss: 0.0003\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 60/661, Loss: 0.0000\n",
      "  Batch 70/661, Loss: 0.0000\n",
      "  Batch 80/661, Loss: 0.0000\n",
      "  Batch 90/661, Loss: 0.0000\n",
      "  Batch 100/661, Loss: 0.0104\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 110/661, Loss: 0.0003\n",
      "  Batch 120/661, Loss: 0.0004\n",
      "  Batch 130/661, Loss: 0.0026\n",
      "  Batch 140/661, Loss: 0.8177\n",
      "  Batch 150/661, Loss: 0.0002\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 160/661, Loss: 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[h264 @ 0x5f490f00] mb_type 104 in P slice too large at 98 31\n",
      "[h264 @ 0x5f490f00] error while decoding MB 98 31\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 170/661, Loss: 0.0001\n",
      "  Batch 180/661, Loss: 1.2216\n",
      "  Batch 190/661, Loss: 0.0003\n",
      "  Batch 200/661, Loss: 0.0012\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 210/661, Loss: 0.0003\n",
      "  Batch 220/661, Loss: 0.0005\n",
      "  Batch 230/661, Loss: 0.0001\n",
      "  Batch 240/661, Loss: 0.0014\n",
      "  Batch 250/661, Loss: 0.0072\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 260/661, Loss: 0.0003\n",
      "  Batch 270/661, Loss: 0.0008\n",
      "  Batch 280/661, Loss: 0.0007\n",
      "  Batch 290/661, Loss: 0.0002\n",
      "  Batch 300/661, Loss: 0.0008\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 310/661, Loss: 0.0005\n",
      "  Batch 320/661, Loss: 0.0003\n",
      "  Batch 330/661, Loss: 0.0002\n",
      "  Batch 340/661, Loss: 0.0002\n",
      "  Batch 350/661, Loss: 0.0423\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 360/661, Loss: 0.0021\n",
      "  Batch 370/661, Loss: 0.0001\n",
      "  Batch 380/661, Loss: 0.0001\n",
      "  Batch 390/661, Loss: 0.0001\n",
      "  Batch 400/661, Loss: 0.0000\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 410/661, Loss: 0.0001\n",
      "  Batch 420/661, Loss: 0.0001\n",
      "  Batch 430/661, Loss: 0.0008\n",
      "  Batch 440/661, Loss: 0.0001\n",
      "  Batch 450/661, Loss: 0.0000\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 460/661, Loss: 0.0001\n",
      "  Batch 470/661, Loss: 0.0001\n",
      "  Batch 480/661, Loss: 0.0001\n",
      "  Batch 490/661, Loss: 0.0001\n",
      "  Batch 500/661, Loss: 0.0000\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 510/661, Loss: 0.7164\n",
      "  Batch 520/661, Loss: 0.0002\n",
      "  Batch 530/661, Loss: 0.0001\n",
      "  Batch 540/661, Loss: 0.0005\n",
      "  Batch 550/661, Loss: 0.8976\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 560/661, Loss: 0.0003\n",
      "  Batch 570/661, Loss: 0.0028\n",
      "  Batch 580/661, Loss: 0.0001\n",
      "  Batch 590/661, Loss: 0.0001\n",
      "  Batch 600/661, Loss: 0.0004\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 610/661, Loss: 0.0003\n",
      "  Batch 620/661, Loss: 0.7173\n",
      "  Batch 630/661, Loss: 0.0003\n",
      "  Batch 640/661, Loss: 0.0001\n",
      "  Batch 650/661, Loss: 0.0005\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 660/661, Loss: 0.0100\n",
      "Starting validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31/1455025713.py:22: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31/4126126155.py:134: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n",
      "/tmp/ipykernel_31/169455789.py:98: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=True):  # Use mixed precision\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 10/661, Loss: 0.0000\n",
      "  Batch 20/661, Loss: 0.0000\n",
      "  Batch 30/661, Loss: 0.0001\n",
      "  Batch 40/661, Loss: 0.0001\n",
      "  Batch 50/661, Loss: 0.0000\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 60/661, Loss: 0.0000\n",
      "  Batch 70/661, Loss: 0.0001\n",
      "  Batch 80/661, Loss: 0.0009\n",
      "  Batch 90/661, Loss: 0.0005\n",
      "  Batch 100/661, Loss: 0.0003\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 110/661, Loss: 0.0003\n",
      "  Batch 120/661, Loss: 0.0001\n",
      "  Batch 130/661, Loss: 0.0003\n",
      "  Batch 140/661, Loss: 0.0017\n",
      "  Batch 150/661, Loss: 0.0004\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 160/661, Loss: 0.0004\n",
      "  Batch 170/661, Loss: 0.0001\n",
      "  Batch 180/661, Loss: 0.0011\n",
      "  Batch 190/661, Loss: 0.2149\n",
      "  Batch 200/661, Loss: 0.0001\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 210/661, Loss: 0.0002\n",
      "  Batch 220/661, Loss: 0.0001\n",
      "  Batch 230/661, Loss: 0.0001\n",
      "  Batch 240/661, Loss: 0.0001\n",
      "  Batch 250/661, Loss: 0.0000\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 260/661, Loss: 0.0015\n",
      "  Batch 270/661, Loss: 0.0001\n",
      "  Batch 280/661, Loss: 1.0909\n",
      "  Batch 290/661, Loss: 0.0000\n",
      "  Batch 300/661, Loss: 0.0001\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 310/661, Loss: 0.0000\n",
      "  Batch 320/661, Loss: 0.0000\n",
      "  Batch 330/661, Loss: 0.3504\n",
      "  Batch 340/661, Loss: 0.0005\n",
      "  Batch 350/661, Loss: 0.0005\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 360/661, Loss: 0.0014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[h264 @ 0x5fb24bc0] mb_type 104 in P slice too large at 98 31\n",
      "[h264 @ 0x5fb24bc0] error while decoding MB 98 31\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 370/661, Loss: 0.0012\n",
      "  Batch 380/661, Loss: 0.0507\n",
      "  Batch 390/661, Loss: 0.0003\n",
      "  Batch 400/661, Loss: 0.0008\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 410/661, Loss: 0.0001\n",
      "  Batch 420/661, Loss: 0.0479\n",
      "  Batch 430/661, Loss: 0.0002\n",
      "  Batch 440/661, Loss: 0.0002\n",
      "  Batch 450/661, Loss: 0.0010\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 460/661, Loss: 0.0005\n",
      "  Batch 470/661, Loss: 0.0007\n",
      "  Batch 480/661, Loss: 0.0003\n",
      "  Batch 490/661, Loss: 0.0002\n",
      "  Batch 500/661, Loss: 0.0003\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 510/661, Loss: 0.0002\n",
      "  Batch 520/661, Loss: 0.0004\n",
      "  Batch 530/661, Loss: 0.0006\n",
      "  Batch 540/661, Loss: 0.0007\n",
      "  Batch 550/661, Loss: 0.0002\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 560/661, Loss: 0.0002\n",
      "  Batch 570/661, Loss: 0.0002\n",
      "  Batch 580/661, Loss: 0.0011\n",
      "  Batch 590/661, Loss: 0.0001\n",
      "  Batch 600/661, Loss: 0.0001\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 610/661, Loss: 0.0001\n",
      "  Batch 620/661, Loss: 0.0001\n",
      "  Batch 630/661, Loss: 0.0003\n",
      "  Batch 640/661, Loss: 0.0007\n",
      "  Batch 650/661, Loss: 0.0087\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 660/661, Loss: 0.8963\n",
      "Starting validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31/1455025713.py:22: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31/4126126155.py:134: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n",
      "/tmp/ipykernel_31/169455789.py:98: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=True):  # Use mixed precision\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 10/661, Loss: 0.0006\n",
      "  Batch 20/661, Loss: 0.0004\n",
      "  Batch 30/661, Loss: 0.0010\n",
      "  Batch 40/661, Loss: 0.0003\n",
      "  Batch 50/661, Loss: 0.0002\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 60/661, Loss: 0.0001\n",
      "  Batch 70/661, Loss: 0.0001\n",
      "  Batch 80/661, Loss: 0.0001\n",
      "  Batch 90/661, Loss: 0.7379\n",
      "  Batch 100/661, Loss: 0.0001\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 110/661, Loss: 0.0001\n",
      "  Batch 120/661, Loss: 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[h264 @ 0x5fa09680] mb_type 104 in P slice too large at 98 31\n",
      "[h264 @ 0x5fa09680] error while decoding MB 98 31\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 130/661, Loss: 0.0002\n",
      "  Batch 140/661, Loss: 0.0002\n",
      "  Batch 150/661, Loss: 0.0001\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 160/661, Loss: 0.0003\n",
      "  Batch 170/661, Loss: 0.0012\n",
      "  Batch 180/661, Loss: 0.0003\n",
      "  Batch 190/661, Loss: 0.0001\n",
      "  Batch 200/661, Loss: 0.0001\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 210/661, Loss: 1.0763\n",
      "  Batch 220/661, Loss: 0.0001\n",
      "  Batch 230/661, Loss: 0.0017\n",
      "  Batch 240/661, Loss: 0.8047\n",
      "  Batch 250/661, Loss: 0.0006\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 260/661, Loss: 0.0003\n",
      "  Batch 270/661, Loss: 0.0002\n",
      "  Batch 280/661, Loss: 0.0009\n",
      "  Batch 290/661, Loss: 0.0010\n",
      "  Batch 300/661, Loss: 0.0012\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 310/661, Loss: 0.0004\n",
      "  Batch 320/661, Loss: 0.0002\n",
      "  Batch 330/661, Loss: 0.0001\n",
      "  Batch 340/661, Loss: 0.0001\n",
      "  Batch 350/661, Loss: 0.0001\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 360/661, Loss: 0.0003\n",
      "  Batch 370/661, Loss: 0.0003\n",
      "  Batch 380/661, Loss: 0.2970\n",
      "  Batch 390/661, Loss: 0.0001\n",
      "  Batch 400/661, Loss: 0.0000\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 410/661, Loss: 0.0001\n",
      "  Batch 420/661, Loss: 0.0009\n",
      "  Batch 430/661, Loss: 0.0000\n",
      "  Batch 440/661, Loss: 0.0000\n",
      "  Batch 450/661, Loss: 0.0001\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 460/661, Loss: 0.0001\n",
      "  Batch 470/661, Loss: 0.0001\n",
      "  Batch 480/661, Loss: 0.0002\n",
      "  Batch 490/661, Loss: 0.0001\n",
      "  Batch 500/661, Loss: 1.0675\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 510/661, Loss: 0.0007\n",
      "  Batch 520/661, Loss: 0.0002\n",
      "  Batch 530/661, Loss: 0.0002\n",
      "  Batch 540/661, Loss: 1.2346\n",
      "  Batch 550/661, Loss: 0.0006\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 560/661, Loss: 0.0039\n",
      "  Batch 570/661, Loss: 0.0069\n",
      "  Batch 580/661, Loss: 0.0009\n",
      "  Batch 590/661, Loss: 0.5834\n",
      "  Batch 600/661, Loss: 0.0006\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 610/661, Loss: 0.0018\n",
      "  Batch 620/661, Loss: 0.0005\n",
      "  Batch 630/661, Loss: 0.0004\n",
      "  Batch 640/661, Loss: 0.0004\n",
      "  Batch 650/661, Loss: 0.0009\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 660/661, Loss: 0.0011\n",
      "Starting validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31/1455025713.py:22: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31/4126126155.py:134: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n",
      "/tmp/ipykernel_31/169455789.py:98: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=True):  # Use mixed precision\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 10/661, Loss: 0.0003\n",
      "  Batch 20/661, Loss: 0.0003\n",
      "  Batch 30/661, Loss: 0.0001\n",
      "  Batch 40/661, Loss: 0.0001\n",
      "  Batch 50/661, Loss: 0.0004\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 60/661, Loss: 0.0004\n",
      "  Batch 70/661, Loss: 0.0002\n",
      "  Batch 80/661, Loss: 0.0003\n",
      "  Batch 90/661, Loss: 0.0004\n",
      "  Batch 100/661, Loss: 0.0004\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 110/661, Loss: 0.0002\n",
      "  Batch 120/661, Loss: 0.0002\n",
      "  Batch 130/661, Loss: 0.0003\n",
      "  Batch 140/661, Loss: 0.0001\n",
      "  Batch 150/661, Loss: 0.0004\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 160/661, Loss: 0.0003\n",
      "  Batch 170/661, Loss: 0.7086\n",
      "  Batch 180/661, Loss: 0.0002\n",
      "  Batch 190/661, Loss: 0.0017\n",
      "  Batch 200/661, Loss: 0.0001\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 210/661, Loss: 0.0001\n",
      "  Batch 220/661, Loss: 0.0098\n",
      "  Batch 230/661, Loss: 0.0002\n",
      "  Batch 240/661, Loss: 0.0002\n",
      "  Batch 250/661, Loss: 0.0007\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 260/661, Loss: 0.0004\n",
      "  Batch 270/661, Loss: 0.0002\n",
      "  Batch 280/661, Loss: 0.0000\n",
      "  Batch 290/661, Loss: 0.0001\n",
      "  Batch 300/661, Loss: 0.0000\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 310/661, Loss: 0.0000\n",
      "  Batch 320/661, Loss: 0.4922\n",
      "  Batch 330/661, Loss: 0.0001\n",
      "  Batch 340/661, Loss: 0.0001\n",
      "  Batch 350/661, Loss: 0.0002\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[h264 @ 0x5faceec0] mb_type 104 in P slice too large at 98 31\n",
      "[h264 @ 0x5faceec0] error while decoding MB 98 31\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 360/661, Loss: 0.0001\n",
      "  Batch 370/661, Loss: 0.0002\n",
      "  Batch 380/661, Loss: 0.0001\n",
      "  Batch 390/661, Loss: 0.0001\n",
      "  Batch 400/661, Loss: 0.0003\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 410/661, Loss: 0.0001\n",
      "  Batch 420/661, Loss: 0.0010\n",
      "  Batch 430/661, Loss: 0.0001\n",
      "  Batch 440/661, Loss: 0.0000\n",
      "  Batch 450/661, Loss: 1.5967\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 460/661, Loss: 0.0001\n",
      "  Batch 470/661, Loss: 0.0001\n",
      "  Batch 480/661, Loss: 0.0002\n",
      "  Batch 490/661, Loss: 0.0002\n",
      "  Batch 500/661, Loss: 0.0003\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 510/661, Loss: 0.0003\n",
      "  Batch 520/661, Loss: 0.0018\n",
      "  Batch 530/661, Loss: 0.0010\n",
      "  Batch 540/661, Loss: 0.0011\n",
      "  Batch 550/661, Loss: 0.0010\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 560/661, Loss: 0.0864\n",
      "  Batch 570/661, Loss: 0.0013\n",
      "  Batch 580/661, Loss: 0.0009\n",
      "  Batch 590/661, Loss: 0.0004\n",
      "  Batch 600/661, Loss: 0.0003\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 610/661, Loss: 0.0005\n",
      "  Batch 620/661, Loss: 0.0003\n",
      "  Batch 630/661, Loss: 0.1257\n",
      "  Batch 640/661, Loss: 0.0011\n",
      "  Batch 650/661, Loss: 0.0002\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 660/661, Loss: 0.0006\n",
      "Starting validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31/1455025713.py:22: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31/4126126155.py:134: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n",
      "/tmp/ipykernel_31/169455789.py:98: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=True):  # Use mixed precision\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 10/661, Loss: 0.0003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[h264 @ 0x5f8b1b00] mb_type 104 in P slice too large at 98 31\n",
      "[h264 @ 0x5f8b1b00] error while decoding MB 98 31\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 20/661, Loss: 0.0024\n",
      "  Batch 30/661, Loss: 0.0002\n",
      "  Batch 40/661, Loss: 0.0002\n",
      "  Batch 50/661, Loss: 0.0001\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 60/661, Loss: 0.0004\n",
      "  Batch 70/661, Loss: 0.0002\n",
      "  Batch 80/661, Loss: 0.0005\n",
      "  Batch 90/661, Loss: 0.0002\n",
      "  Batch 100/661, Loss: 0.0001\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 110/661, Loss: 0.0001\n",
      "  Batch 120/661, Loss: 0.0003\n",
      "  Batch 130/661, Loss: 0.0001\n",
      "  Batch 140/661, Loss: 0.0001\n",
      "  Batch 150/661, Loss: 0.0001\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 160/661, Loss: 0.0002\n",
      "  Batch 170/661, Loss: 0.0003\n",
      "  Batch 180/661, Loss: 0.0002\n",
      "  Batch 190/661, Loss: 0.0001\n",
      "  Batch 200/661, Loss: 0.0001\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 210/661, Loss: 0.0002\n",
      "  Batch 220/661, Loss: 0.0001\n",
      "  Batch 230/661, Loss: 0.0001\n",
      "  Batch 240/661, Loss: 0.0002\n",
      "  Batch 250/661, Loss: 0.0010\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 260/661, Loss: 0.0002\n",
      "  Batch 270/661, Loss: 0.0001\n",
      "  Batch 280/661, Loss: 0.0000\n",
      "  Batch 290/661, Loss: 0.7091\n",
      "  Batch 300/661, Loss: 0.0001\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 310/661, Loss: 0.0002\n",
      "  Batch 320/661, Loss: 0.8238\n",
      "  Batch 330/661, Loss: 0.0001\n",
      "  Batch 340/661, Loss: 0.0006\n",
      "  Batch 350/661, Loss: 0.0002\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 360/661, Loss: 0.0008\n",
      "  Batch 370/661, Loss: 0.0001\n",
      "  Batch 380/661, Loss: 0.0001\n",
      "  Batch 390/661, Loss: 0.0000\n",
      "  Batch 400/661, Loss: 0.0001\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 410/661, Loss: 0.0001\n",
      "  Batch 420/661, Loss: 0.0011\n",
      "  Batch 430/661, Loss: 0.0001\n",
      "  Batch 440/661, Loss: 0.0001\n",
      "  Batch 450/661, Loss: 0.0001\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 460/661, Loss: 0.0001\n",
      "  Batch 470/661, Loss: 0.0001\n",
      "  Batch 480/661, Loss: 0.0001\n",
      "  Batch 490/661, Loss: 0.0001\n",
      "  Batch 500/661, Loss: 0.0021\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 510/661, Loss: 0.0001\n",
      "  Batch 520/661, Loss: 1.2462\n",
      "  Batch 530/661, Loss: 0.0000\n",
      "  Batch 540/661, Loss: 0.0001\n",
      "  Batch 550/661, Loss: 0.0001\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 560/661, Loss: 0.0001\n",
      "  Batch 570/661, Loss: 0.0031\n",
      "  Batch 580/661, Loss: 0.0001\n",
      "  Batch 590/661, Loss: 0.0021\n",
      "  Batch 600/661, Loss: 0.0000\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 610/661, Loss: 0.0011\n",
      "  Batch 620/661, Loss: 0.0002\n",
      "  Batch 630/661, Loss: 0.0005\n",
      "  Batch 640/661, Loss: 0.0000\n",
      "  Batch 650/661, Loss: 0.0002\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 660/661, Loss: 0.0000\n",
      "Starting validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31/1455025713.py:22: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31/4126126155.py:134: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n",
      "/tmp/ipykernel_31/169455789.py:98: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=True):  # Use mixed precision\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 10/661, Loss: 0.0001\n",
      "  Batch 20/661, Loss: 0.9913\n",
      "  Batch 30/661, Loss: 0.0010\n",
      "  Batch 40/661, Loss: 0.0000\n",
      "  Batch 50/661, Loss: 0.0000\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 60/661, Loss: 0.0001\n",
      "  Batch 70/661, Loss: 0.0001\n",
      "  Batch 80/661, Loss: 0.0001\n",
      "  Batch 90/661, Loss: 0.0004\n",
      "  Batch 100/661, Loss: 0.0001\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 110/661, Loss: 0.0001\n",
      "  Batch 120/661, Loss: 0.0001\n",
      "  Batch 130/661, Loss: 0.0403\n",
      "  Batch 140/661, Loss: 0.0001\n",
      "  Batch 150/661, Loss: 0.0001\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 160/661, Loss: 0.0001\n",
      "  Batch 170/661, Loss: 0.0000\n",
      "  Batch 180/661, Loss: 0.0001\n",
      "  Batch 190/661, Loss: 0.0000\n",
      "  Batch 200/661, Loss: 0.0001\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 210/661, Loss: 0.0000\n",
      "  Batch 220/661, Loss: 0.0001\n",
      "  Batch 230/661, Loss: 0.0001\n",
      "  Batch 240/661, Loss: 0.0000\n",
      "  Batch 250/661, Loss: 0.0000\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 260/661, Loss: 0.0000\n",
      "  Batch 270/661, Loss: 0.0000\n",
      "  Batch 280/661, Loss: 0.0000\n",
      "  Batch 290/661, Loss: 0.0002\n",
      "  Batch 300/661, Loss: 0.0000\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 310/661, Loss: 0.0000\n",
      "  Batch 320/661, Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[h264 @ 0x5f8f15c0] mb_type 104 in P slice too large at 98 31\n",
      "[h264 @ 0x5f8f15c0] error while decoding MB 98 31\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 330/661, Loss: 0.0003\n",
      "  Batch 340/661, Loss: 0.0001\n",
      "  Batch 350/661, Loss: 0.2697\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 360/661, Loss: 0.0001\n",
      "  Batch 370/661, Loss: 0.0002\n",
      "  Batch 380/661, Loss: 0.0001\n",
      "  Batch 390/661, Loss: 0.0001\n",
      "  Batch 400/661, Loss: 0.9367\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 410/661, Loss: 0.0002\n",
      "  Batch 420/661, Loss: 0.0001\n",
      "  Batch 430/661, Loss: 0.0002\n",
      "  Batch 440/661, Loss: 0.0026\n",
      "  Batch 450/661, Loss: 0.0001\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 460/661, Loss: 0.0003\n",
      "  Batch 470/661, Loss: 0.0022\n",
      "  Batch 480/661, Loss: 0.0010\n",
      "  Batch 490/661, Loss: 0.0004\n",
      "  Batch 500/661, Loss: 0.0003\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 510/661, Loss: 0.0001\n",
      "  Batch 520/661, Loss: 0.0001\n",
      "  Batch 530/661, Loss: 0.0001\n",
      "  Batch 540/661, Loss: 0.0002\n",
      "  Batch 550/661, Loss: 0.0000\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 560/661, Loss: 0.0000\n",
      "  Batch 570/661, Loss: 0.0000\n",
      "  Batch 580/661, Loss: 0.0001\n",
      "  Batch 590/661, Loss: 0.0000\n",
      "  Batch 600/661, Loss: 0.0001\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 610/661, Loss: 0.0000\n",
      "  Batch 620/661, Loss: 0.7628\n",
      "  Batch 630/661, Loss: 0.0006\n",
      "  Batch 640/661, Loss: 0.0006\n",
      "  Batch 650/661, Loss: 0.0002\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 660/661, Loss: 0.0013\n",
      "Starting validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31/1455025713.py:22: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31/4126126155.py:134: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n",
      "/tmp/ipykernel_31/169455789.py:98: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=True):  # Use mixed precision\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 10/661, Loss: 0.0001\n",
      "  Batch 20/661, Loss: 0.0001\n",
      "  Batch 30/661, Loss: 0.0003\n",
      "  Batch 40/661, Loss: 0.0000\n",
      "  Batch 50/661, Loss: 0.0001\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 60/661, Loss: 0.0004\n",
      "  Batch 70/661, Loss: 0.0003\n",
      "  Batch 80/661, Loss: 0.0001\n",
      "  Batch 90/661, Loss: 0.0002\n",
      "  Batch 100/661, Loss: 0.0000\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 110/661, Loss: 0.0001\n",
      "  Batch 120/661, Loss: 0.0001\n",
      "  Batch 130/661, Loss: 0.3047\n",
      "  Batch 140/661, Loss: 0.0000\n",
      "  Batch 150/661, Loss: 0.0000\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 160/661, Loss: 0.0001\n",
      "  Batch 170/661, Loss: 0.0000\n",
      "  Batch 180/661, Loss: 0.0001\n",
      "  Batch 190/661, Loss: 0.0002\n",
      "  Batch 200/661, Loss: 0.0006\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 210/661, Loss: 0.0002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[h264 @ 0x5fa70300] mb_type 104 in P slice too large at 98 31\n",
      "[h264 @ 0x5fa70300] error while decoding MB 98 31\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 220/661, Loss: 0.0002\n",
      "  Batch 230/661, Loss: 0.0003\n",
      "  Batch 240/661, Loss: 0.0002\n",
      "  Batch 250/661, Loss: 0.0002\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 260/661, Loss: 0.0003\n",
      "  Batch 270/661, Loss: 0.0001\n",
      "  Batch 280/661, Loss: 0.0002\n",
      "  Batch 290/661, Loss: 0.0013\n",
      "  Batch 300/661, Loss: 0.0000\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 310/661, Loss: 0.0001\n",
      "  Batch 320/661, Loss: 0.0001\n",
      "  Batch 330/661, Loss: 0.0001\n",
      "  Batch 340/661, Loss: 0.0001\n",
      "  Batch 350/661, Loss: 0.0001\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 360/661, Loss: 0.0001\n",
      "  Batch 370/661, Loss: 0.0000\n",
      "  Batch 380/661, Loss: 0.0001\n",
      "  Batch 390/661, Loss: 0.0004\n",
      "  Batch 400/661, Loss: 0.0001\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 410/661, Loss: 0.0001\n",
      "  Batch 420/661, Loss: 0.0001\n",
      "  Batch 430/661, Loss: 0.4602\n",
      "  Batch 440/661, Loss: 0.0001\n",
      "  Batch 450/661, Loss: 1.3516\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 460/661, Loss: 0.0002\n",
      "  Batch 470/661, Loss: 0.0001\n",
      "  Batch 480/661, Loss: 0.0005\n",
      "  Batch 490/661, Loss: 0.0001\n",
      "  Batch 500/661, Loss: 0.0001\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 510/661, Loss: 0.0002\n",
      "  Batch 520/661, Loss: 0.0000\n",
      "  Batch 530/661, Loss: 0.0001\n",
      "  Batch 540/661, Loss: 0.0000\n",
      "  Batch 550/661, Loss: 0.0000\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 560/661, Loss: 0.0001\n",
      "  Batch 570/661, Loss: 0.0001\n",
      "  Batch 580/661, Loss: 0.0021\n",
      "  Batch 590/661, Loss: 0.0001\n",
      "  Batch 600/661, Loss: 0.0001\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 610/661, Loss: 0.0001\n",
      "  Batch 620/661, Loss: 0.0001\n",
      "  Batch 630/661, Loss: 0.0002\n",
      "  Batch 640/661, Loss: 0.0000\n",
      "  Batch 650/661, Loss: 0.0002\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 660/661, Loss: 0.0001\n",
      "Starting validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31/1455025713.py:22: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31/4126126155.py:134: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n",
      "/tmp/ipykernel_31/169455789.py:98: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=True):  # Use mixed precision\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 10/661, Loss: 0.0001\n",
      "  Batch 20/661, Loss: 0.0007\n",
      "  Batch 30/661, Loss: 0.0002\n",
      "  Batch 40/661, Loss: 0.0000\n",
      "  Batch 50/661, Loss: 0.0005\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 60/661, Loss: 0.0004\n",
      "  Batch 70/661, Loss: 0.0004\n",
      "  Batch 80/661, Loss: 0.0002\n",
      "  Batch 90/661, Loss: 0.0005\n",
      "  Batch 100/661, Loss: 0.9241\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 110/661, Loss: 0.0002\n",
      "  Batch 120/661, Loss: 0.0008\n",
      "  Batch 130/661, Loss: 0.0002\n",
      "  Batch 140/661, Loss: 0.0000\n",
      "  Batch 150/661, Loss: 0.0001\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 160/661, Loss: 0.0001\n",
      "  Batch 170/661, Loss: 0.0002\n",
      "  Batch 180/661, Loss: 0.0001\n",
      "  Batch 190/661, Loss: 0.2708\n",
      "  Batch 200/661, Loss: 0.0001\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 210/661, Loss: 0.2540\n",
      "  Batch 220/661, Loss: 0.0002\n",
      "  Batch 230/661, Loss: 0.0001\n",
      "  Batch 240/661, Loss: 0.0001\n",
      "  Batch 250/661, Loss: 0.0001\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 260/661, Loss: 0.0006\n",
      "  Batch 270/661, Loss: 0.0000\n",
      "  Batch 280/661, Loss: 0.0000\n",
      "  Batch 290/661, Loss: 0.0000\n",
      "  Batch 300/661, Loss: 0.0000\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 310/661, Loss: 0.0002\n",
      "  Batch 320/661, Loss: 0.0001\n",
      "  Batch 330/661, Loss: 0.0001\n",
      "  Batch 340/661, Loss: 0.0002\n",
      "  Batch 350/661, Loss: 2.1286\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 360/661, Loss: 0.0001\n",
      "  Batch 370/661, Loss: 0.0001\n",
      "  Batch 380/661, Loss: 0.0001\n",
      "  Batch 390/661, Loss: 0.0009\n",
      "  Batch 400/661, Loss: 0.0004\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 410/661, Loss: 0.0005\n",
      "  Batch 420/661, Loss: 0.0002\n",
      "  Batch 430/661, Loss: 0.0001\n",
      "  Batch 440/661, Loss: 0.0000\n",
      "  Batch 450/661, Loss: 0.0000\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 460/661, Loss: 0.0000\n",
      "  Batch 470/661, Loss: 0.0000\n",
      "  Batch 480/661, Loss: 0.0000\n",
      "  Batch 490/661, Loss: 0.0002\n",
      "  Batch 500/661, Loss: 0.0000\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 510/661, Loss: 0.0000\n",
      "  Batch 520/661, Loss: 0.0000\n",
      "  Batch 530/661, Loss: 0.0000\n",
      "  Batch 540/661, Loss: 0.0000\n",
      "  Batch 550/661, Loss: 0.0001\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 560/661, Loss: 0.0000\n",
      "  Batch 570/661, Loss: 0.0001\n",
      "  Batch 580/661, Loss: 0.0001\n",
      "  Batch 590/661, Loss: 0.0001\n",
      "  Batch 600/661, Loss: 0.0014\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 610/661, Loss: 0.0000\n",
      "  Batch 620/661, Loss: 0.0000\n",
      "  Batch 630/661, Loss: 0.0011\n",
      "  Batch 640/661, Loss: 0.5503\n",
      "  Batch 650/661, Loss: 0.0000\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[h264 @ 0x5fa8ad00] mb_type 104 in P slice too large at 98 31\n",
      "[h264 @ 0x5fa8ad00] error while decoding MB 98 31\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 660/661, Loss: 0.0000\n",
      "Starting validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31/1455025713.py:22: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31/4126126155.py:134: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n",
      "/tmp/ipykernel_31/169455789.py:98: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=True):  # Use mixed precision\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 10/661, Loss: 0.0001\n",
      "  Batch 20/661, Loss: 0.7647\n",
      "  Batch 30/661, Loss: 0.0002\n",
      "  Batch 40/661, Loss: 0.0000\n",
      "  Batch 50/661, Loss: 0.0000\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 60/661, Loss: 0.0000\n",
      "  Batch 70/661, Loss: 0.0000\n",
      "  Batch 80/661, Loss: 0.0000\n",
      "  Batch 90/661, Loss: 0.0000\n",
      "  Batch 100/661, Loss: 0.0001\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 110/661, Loss: 0.0001\n",
      "  Batch 120/661, Loss: 0.0000\n",
      "  Batch 130/661, Loss: 0.0000\n",
      "  Batch 140/661, Loss: 0.0003\n",
      "  Batch 150/661, Loss: 0.0000\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 160/661, Loss: 0.1315\n",
      "  Batch 170/661, Loss: 0.0000\n",
      "  Batch 180/661, Loss: 0.0000\n",
      "  Batch 190/661, Loss: 0.0000\n",
      "  Batch 200/661, Loss: 0.0000\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 210/661, Loss: 0.0000\n",
      "  Batch 220/661, Loss: 0.0000\n",
      "  Batch 230/661, Loss: 0.0000\n",
      "  Batch 240/661, Loss: 0.0001\n",
      "  Batch 250/661, Loss: 0.0000\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 260/661, Loss: 0.0003\n",
      "  Batch 270/661, Loss: 0.0000\n",
      "  Batch 280/661, Loss: 0.0000\n",
      "  Batch 290/661, Loss: 0.0002\n",
      "  Batch 300/661, Loss: 0.0000\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 310/661, Loss: 0.0000\n",
      "  Batch 320/661, Loss: 0.0000\n",
      "  Batch 330/661, Loss: 0.0000\n",
      "  Batch 340/661, Loss: 0.0000\n",
      "  Batch 350/661, Loss: 0.0000\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 360/661, Loss: 0.0000\n",
      "  Batch 370/661, Loss: 0.0000\n",
      "  Batch 380/661, Loss: 0.0008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[h264 @ 0x5f98eb40] mb_type 104 in P slice too large at 98 31\n",
      "[h264 @ 0x5f98eb40] error while decoding MB 98 31\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 390/661, Loss: 0.0411\n",
      "  Batch 400/661, Loss: 0.0006\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 410/661, Loss: 0.5145\n",
      "  Batch 420/661, Loss: 0.0002\n",
      "  Batch 430/661, Loss: 0.0001\n",
      "  Batch 440/661, Loss: 0.0001\n",
      "  Batch 450/661, Loss: 0.0000\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 460/661, Loss: 0.0000\n",
      "  Batch 470/661, Loss: 0.0340\n",
      "  Batch 480/661, Loss: 0.0000\n",
      "  Batch 490/661, Loss: 0.0000\n",
      "  Batch 500/661, Loss: 0.0004\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 510/661, Loss: 0.0001\n",
      "  Batch 520/661, Loss: 0.0001\n",
      "  Batch 530/661, Loss: 0.6734\n",
      "  Batch 540/661, Loss: 0.0001\n",
      "  Batch 550/661, Loss: 0.0003\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 560/661, Loss: 0.0002\n",
      "  Batch 570/661, Loss: 0.0008\n",
      "  Batch 580/661, Loss: 0.0001\n",
      "  Batch 590/661, Loss: 0.0000\n",
      "  Batch 600/661, Loss: 0.0000\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 610/661, Loss: 0.0000\n",
      "  Batch 620/661, Loss: 0.0000\n",
      "  Batch 630/661, Loss: 0.0000\n",
      "  Batch 640/661, Loss: 0.0000\n",
      "  Batch 650/661, Loss: 0.8624\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 660/661, Loss: 0.0000\n",
      "Starting validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31/1455025713.py:22: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31/4126126155.py:134: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n",
      "/tmp/ipykernel_31/169455789.py:98: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=True):  # Use mixed precision\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 10/661, Loss: 0.0001\n",
      "  Batch 20/661, Loss: 0.0001\n",
      "  Batch 30/661, Loss: 0.0001\n",
      "  Batch 40/661, Loss: 0.0001\n",
      "  Batch 50/661, Loss: 0.0013\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 60/661, Loss: 0.0000\n",
      "  Batch 70/661, Loss: 0.0001\n",
      "  Batch 80/661, Loss: 0.0001\n",
      "  Batch 90/661, Loss: 0.0000\n",
      "  Batch 100/661, Loss: 0.0001\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 110/661, Loss: 0.0000\n",
      "  Batch 120/661, Loss: 0.0000\n",
      "  Batch 130/661, Loss: 0.0000\n",
      "  Batch 140/661, Loss: 0.0001\n",
      "  Batch 150/661, Loss: 0.0000\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[h264 @ 0x5fbe4740] mb_type 104 in P slice too large at 98 31\n",
      "[h264 @ 0x5fbe4740] error while decoding MB 98 31\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 160/661, Loss: 0.0000\n",
      "  Batch 170/661, Loss: 0.0001\n",
      "  Batch 180/661, Loss: 0.0001\n",
      "  Batch 190/661, Loss: 0.0002\n",
      "  Batch 200/661, Loss: 0.0000\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 210/661, Loss: 0.0000\n",
      "  Batch 220/661, Loss: 0.0000\n",
      "  Batch 230/661, Loss: 0.0000\n",
      "  Batch 240/661, Loss: 0.0021\n",
      "  Batch 250/661, Loss: 0.0001\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 260/661, Loss: 0.0000\n",
      "  Batch 270/661, Loss: 0.0024\n",
      "  Batch 280/661, Loss: 0.0000\n",
      "  Batch 290/661, Loss: 0.0000\n",
      "  Batch 300/661, Loss: 0.0000\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 310/661, Loss: 0.0000\n",
      "  Batch 320/661, Loss: 0.0000\n",
      "  Batch 330/661, Loss: 0.0000\n",
      "  Batch 340/661, Loss: 0.0000\n",
      "  Batch 350/661, Loss: 0.0000\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 360/661, Loss: 0.0000\n",
      "  Batch 370/661, Loss: 1.0557\n",
      "  Batch 380/661, Loss: 0.0000\n",
      "  Batch 390/661, Loss: 0.0000\n",
      "  Batch 400/661, Loss: 0.0012\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 410/661, Loss: 0.0001\n",
      "  Batch 420/661, Loss: 0.0000\n",
      "  Batch 430/661, Loss: 0.4878\n",
      "  Batch 440/661, Loss: 0.0000\n",
      "  Batch 450/661, Loss: 0.0000\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 460/661, Loss: 0.0000\n",
      "  Batch 470/661, Loss: 0.0001\n",
      "  Batch 480/661, Loss: 0.0000\n",
      "  Batch 490/661, Loss: 0.0001\n",
      "  Batch 500/661, Loss: 0.0000\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 510/661, Loss: 0.7276\n",
      "  Batch 520/661, Loss: 0.0000\n",
      "  Batch 530/661, Loss: 0.0004\n",
      "  Batch 540/661, Loss: 0.0380\n",
      "  Batch 550/661, Loss: 0.0007\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 560/661, Loss: 0.0000\n",
      "  Batch 570/661, Loss: 0.0000\n",
      "  Batch 580/661, Loss: 0.0000\n",
      "  Batch 590/661, Loss: 0.0000\n",
      "  Batch 600/661, Loss: 0.0001\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 610/661, Loss: 0.0000\n",
      "  Batch 620/661, Loss: 0.0003\n",
      "  Batch 630/661, Loss: 0.0000\n",
      "  Batch 640/661, Loss: 0.0000\n",
      "  Batch 650/661, Loss: 0.0000\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 660/661, Loss: 0.0000\n",
      "Starting validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31/1455025713.py:22: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31/4126126155.py:134: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n",
      "/tmp/ipykernel_31/169455789.py:98: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=True):  # Use mixed precision\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 10/661, Loss: 0.0000\n",
      "  Batch 20/661, Loss: 0.0000\n",
      "  Batch 30/661, Loss: 0.0002\n",
      "  Batch 40/661, Loss: 0.0003\n",
      "  Batch 50/661, Loss: 0.9161\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 60/661, Loss: 0.0006\n",
      "  Batch 70/661, Loss: 0.0001\n",
      "  Batch 80/661, Loss: 0.0003\n",
      "  Batch 90/661, Loss: 0.0001\n",
      "  Batch 100/661, Loss: 0.0002\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 110/661, Loss: 0.0001\n",
      "  Batch 120/661, Loss: 0.0002\n",
      "  Batch 130/661, Loss: 0.0000\n",
      "  Batch 140/661, Loss: 0.0004\n",
      "  Batch 150/661, Loss: 0.0001\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 160/661, Loss: 0.0207\n",
      "  Batch 170/661, Loss: 0.0000\n",
      "  Batch 180/661, Loss: 0.0000\n",
      "  Batch 190/661, Loss: 0.0000\n",
      "  Batch 200/661, Loss: 0.0001\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 210/661, Loss: 0.0001\n",
      "  Batch 220/661, Loss: 0.0000\n",
      "  Batch 230/661, Loss: 0.0001\n",
      "  Batch 240/661, Loss: 0.0000\n",
      "  Batch 250/661, Loss: 0.0004\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 260/661, Loss: 0.0000\n",
      "  Batch 270/661, Loss: 0.0000\n",
      "  Batch 280/661, Loss: 0.0000\n",
      "  Batch 290/661, Loss: 0.0000\n",
      "  Batch 300/661, Loss: 0.0000\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 310/661, Loss: 0.0011\n",
      "  Batch 320/661, Loss: 0.0001\n",
      "  Batch 330/661, Loss: 0.0001\n",
      "  Batch 340/661, Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[h264 @ 0x5fa73200] mb_type 104 in P slice too large at 98 31\n",
      "[h264 @ 0x5fa73200] error while decoding MB 98 31\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 350/661, Loss: 0.0004\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 360/661, Loss: 0.0002\n",
      "  Batch 370/661, Loss: 0.0001\n",
      "  Batch 380/661, Loss: 0.0002\n",
      "  Batch 390/661, Loss: 0.8035\n",
      "  Batch 400/661, Loss: 0.0000\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 410/661, Loss: 0.8016\n",
      "  Batch 420/661, Loss: 0.0003\n",
      "  Batch 430/661, Loss: 0.0001\n",
      "  Batch 440/661, Loss: 0.0002\n",
      "  Batch 450/661, Loss: 0.0002\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 460/661, Loss: 0.0002\n",
      "  Batch 470/661, Loss: 0.0001\n",
      "  Batch 480/661, Loss: 0.0003\n",
      "  Batch 490/661, Loss: 0.0003\n",
      "  Batch 500/661, Loss: 0.0001\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 510/661, Loss: 0.0001\n",
      "  Batch 520/661, Loss: 0.0002\n",
      "  Batch 530/661, Loss: 0.0001\n",
      "  Batch 540/661, Loss: 0.0015\n",
      "  Batch 550/661, Loss: 0.0006\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 560/661, Loss: 0.0000\n",
      "  Batch 570/661, Loss: 0.0000\n",
      "  Batch 580/661, Loss: 0.0000\n",
      "  Batch 590/661, Loss: 0.0001\n",
      "  Batch 600/661, Loss: 0.0000\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 610/661, Loss: 0.0000\n",
      "  Batch 620/661, Loss: 0.0000\n",
      "  Batch 630/661, Loss: 0.0000\n",
      "  Batch 640/661, Loss: 0.0000\n",
      "  Batch 650/661, Loss: 0.8140\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 660/661, Loss: 0.4613\n",
      "Starting validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31/1455025713.py:22: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31/4126126155.py:134: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n",
      "/tmp/ipykernel_31/169455789.py:98: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=True):  # Use mixed precision\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 10/661, Loss: 0.0000\n",
      "  Batch 20/661, Loss: 0.0000\n",
      "  Batch 30/661, Loss: 0.0001\n",
      "  Batch 40/661, Loss: 0.0000\n",
      "  Batch 50/661, Loss: 0.0001\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 60/661, Loss: 0.0001\n",
      "  Batch 70/661, Loss: 0.0001\n",
      "  Batch 80/661, Loss: 0.0000\n",
      "  Batch 90/661, Loss: 0.0000\n",
      "  Batch 100/661, Loss: 1.0518\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 110/661, Loss: 0.0000\n",
      "  Batch 120/661, Loss: 0.0000\n",
      "  Batch 130/661, Loss: 0.0000\n",
      "  Batch 140/661, Loss: 0.0000\n",
      "  Batch 150/661, Loss: 0.0000\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 160/661, Loss: 0.0001\n",
      "  Batch 170/661, Loss: 0.0000\n",
      "  Batch 180/661, Loss: 0.0001\n",
      "  Batch 190/661, Loss: 0.0000\n",
      "  Batch 200/661, Loss: 0.0000\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 210/661, Loss: 0.0000\n",
      "  Batch 220/661, Loss: 0.6548\n",
      "  Batch 230/661, Loss: 0.0001\n",
      "  Batch 240/661, Loss: 0.0001\n",
      "  Batch 250/661, Loss: 0.0001\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 260/661, Loss: 0.0002\n",
      "  Batch 270/661, Loss: 0.0002\n",
      "  Batch 280/661, Loss: 0.0001\n",
      "  Batch 290/661, Loss: 0.1374\n",
      "  Batch 300/661, Loss: 0.0001\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 310/661, Loss: 0.0001\n",
      "  Batch 320/661, Loss: 0.0001\n",
      "  Batch 330/661, Loss: 0.0001\n",
      "  Batch 340/661, Loss: 0.0010\n",
      "  Batch 350/661, Loss: 0.0002\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 360/661, Loss: 0.0006\n",
      "  Batch 370/661, Loss: 0.0001\n",
      "  Batch 380/661, Loss: 0.0000\n",
      "  Batch 390/661, Loss: 0.0000\n",
      "  Batch 400/661, Loss: 0.0025\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 410/661, Loss: 0.0000\n",
      "  Batch 420/661, Loss: 0.0000\n",
      "  Batch 430/661, Loss: 0.0000\n",
      "  Batch 440/661, Loss: 0.0000\n",
      "  Batch 450/661, Loss: 0.0002\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 460/661, Loss: 0.0000\n",
      "  Batch 470/661, Loss: 0.0000\n",
      "  Batch 480/661, Loss: 0.0011\n",
      "  Batch 490/661, Loss: 0.0000\n",
      "  Batch 500/661, Loss: 0.0000\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 510/661, Loss: 0.0002\n",
      "  Batch 520/661, Loss: 0.0001\n",
      "  Batch 530/661, Loss: 0.0001\n",
      "  Batch 540/661, Loss: 0.0002\n",
      "  Batch 550/661, Loss: 0.0002\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 560/661, Loss: 0.0009\n",
      "  Batch 570/661, Loss: 0.0001\n",
      "  Batch 580/661, Loss: 0.0000\n",
      "  Batch 590/661, Loss: 0.0000\n",
      "  Batch 600/661, Loss: 0.0000\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 610/661, Loss: 0.0003\n",
      "  Batch 620/661, Loss: 0.3218\n",
      "  Batch 630/661, Loss: 0.0001\n",
      "  Batch 640/661, Loss: 0.0002\n",
      "  Batch 650/661, Loss: 0.0003\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[h264 @ 0x5f4c2200] mb_type 104 in P slice too large at 98 31\n",
      "[h264 @ 0x5f4c2200] error while decoding MB 98 31\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 660/661, Loss: 0.0002\n",
      "Starting validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31/1455025713.py:22: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31/4126126155.py:134: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n",
      "/tmp/ipykernel_31/169455789.py:98: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=True):  # Use mixed precision\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 10/661, Loss: 0.0001\n",
      "  Batch 20/661, Loss: 0.0001\n",
      "  Batch 30/661, Loss: 0.0001\n",
      "  Batch 40/661, Loss: 0.0001\n",
      "  Batch 50/661, Loss: 0.0000\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 60/661, Loss: 0.0000\n",
      "  Batch 70/661, Loss: 0.0003\n",
      "  Batch 80/661, Loss: 0.0000\n",
      "  Batch 90/661, Loss: 0.0000\n",
      "  Batch 100/661, Loss: 0.0000\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 110/661, Loss: 0.0002\n",
      "  Batch 120/661, Loss: 0.9136\n",
      "  Batch 130/661, Loss: 0.0001\n",
      "  Batch 140/661, Loss: 0.0000\n",
      "  Batch 150/661, Loss: 0.0000\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 160/661, Loss: 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[h264 @ 0x5f95e800] mb_type 104 in P slice too large at 98 31\n",
      "[h264 @ 0x5f95e800] error while decoding MB 98 31\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 170/661, Loss: 0.0009\n",
      "  Batch 180/661, Loss: 0.0001\n",
      "  Batch 190/661, Loss: 0.0003\n",
      "  Batch 200/661, Loss: 0.0000\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 210/661, Loss: 0.0000\n",
      "  Batch 220/661, Loss: 0.0001\n",
      "  Batch 230/661, Loss: 0.0001\n",
      "  Batch 240/661, Loss: 0.0001\n",
      "  Batch 250/661, Loss: 0.0001\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 260/661, Loss: 0.0005\n",
      "  Batch 270/661, Loss: 0.0001\n",
      "  Batch 280/661, Loss: 0.0001\n",
      "  Batch 290/661, Loss: 0.0001\n",
      "  Batch 300/661, Loss: 0.0001\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 310/661, Loss: 0.0000\n",
      "  Batch 320/661, Loss: 0.0000\n",
      "  Batch 330/661, Loss: 0.0001\n",
      "  Batch 340/661, Loss: 0.0001\n",
      "  Batch 350/661, Loss: 0.0001\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 360/661, Loss: 0.0008\n",
      "  Batch 370/661, Loss: 0.0001\n",
      "  Batch 380/661, Loss: 0.0000\n",
      "  Batch 390/661, Loss: 0.0001\n",
      "  Batch 400/661, Loss: 0.0001\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 410/661, Loss: 0.0001\n",
      "  Batch 420/661, Loss: 0.0000\n",
      "  Batch 430/661, Loss: 0.0000\n",
      "  Batch 440/661, Loss: 0.0001\n",
      "  Batch 450/661, Loss: 0.0000\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 460/661, Loss: 0.0001\n",
      "  Batch 470/661, Loss: 0.0001\n",
      "  Batch 480/661, Loss: 0.0000\n",
      "  Batch 490/661, Loss: 0.0000\n",
      "  Batch 500/661, Loss: 0.0000\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 510/661, Loss: 0.0000\n",
      "  Batch 520/661, Loss: 0.0000\n",
      "  Batch 530/661, Loss: 0.0000\n",
      "  Batch 540/661, Loss: 0.0000\n",
      "  Batch 550/661, Loss: 0.0001\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 560/661, Loss: 0.0000\n",
      "  Batch 570/661, Loss: 0.0000\n",
      "  Batch 580/661, Loss: 0.0000\n",
      "  Batch 590/661, Loss: 0.0000\n",
      "  Batch 600/661, Loss: 0.0020\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 610/661, Loss: 0.0014\n",
      "  Batch 620/661, Loss: 0.0001\n",
      "  Batch 630/661, Loss: 0.0000\n",
      "  Batch 640/661, Loss: 0.0001\n",
      "  Batch 650/661, Loss: 0.0000\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 660/661, Loss: 0.0001\n",
      "Starting validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31/1455025713.py:22: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31/4126126155.py:134: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n",
      "/tmp/ipykernel_31/169455789.py:98: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=True):  # Use mixed precision\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 10/661, Loss: 0.0000\n",
      "  Batch 20/661, Loss: 0.0000\n",
      "  Batch 30/661, Loss: 0.0560\n",
      "  Batch 40/661, Loss: 0.0001\n",
      "  Batch 50/661, Loss: 0.0000\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 60/661, Loss: 0.0000\n",
      "  Batch 70/661, Loss: 0.0000\n",
      "  Batch 80/661, Loss: 0.0000\n",
      "  Batch 90/661, Loss: 0.0000\n",
      "  Batch 100/661, Loss: 0.0000\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 110/661, Loss: 0.0000\n",
      "  Batch 120/661, Loss: 0.0000\n",
      "  Batch 130/661, Loss: 0.0000\n",
      "  Batch 140/661, Loss: 0.0002\n",
      "  Batch 150/661, Loss: 0.0000\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 160/661, Loss: 0.0000\n",
      "  Batch 170/661, Loss: 0.0000\n",
      "  Batch 180/661, Loss: 0.7144\n",
      "  Batch 190/661, Loss: 0.0000\n",
      "  Batch 200/661, Loss: 0.0000\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 210/661, Loss: 0.0000\n",
      "  Batch 220/661, Loss: 0.0000\n",
      "  Batch 230/661, Loss: 0.0000\n",
      "  Batch 240/661, Loss: 0.0031\n",
      "  Batch 250/661, Loss: 0.0000\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 260/661, Loss: 0.0000\n",
      "  Batch 270/661, Loss: 0.0000\n",
      "  Batch 280/661, Loss: 0.0000\n",
      "  Batch 290/661, Loss: 0.0000\n",
      "  Batch 300/661, Loss: 0.0000\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 310/661, Loss: 0.0000\n",
      "  Batch 320/661, Loss: 0.0000\n",
      "  Batch 330/661, Loss: 0.0000\n",
      "  Batch 340/661, Loss: 0.0000\n",
      "  Batch 350/661, Loss: 0.0000\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 360/661, Loss: 0.0000\n",
      "  Batch 370/661, Loss: 0.0000\n",
      "  Batch 380/661, Loss: 0.0000\n",
      "  Batch 390/661, Loss: 0.0000\n",
      "  Batch 400/661, Loss: 0.0000\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 410/661, Loss: 0.0000\n",
      "  Batch 420/661, Loss: 0.0000\n",
      "  Batch 430/661, Loss: 0.0000\n",
      "  Batch 440/661, Loss: 0.0000\n",
      "  Batch 450/661, Loss: 0.0000\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 460/661, Loss: 0.0001\n",
      "  Batch 470/661, Loss: 0.0000\n",
      "  Batch 480/661, Loss: 0.0000\n",
      "  Batch 490/661, Loss: 0.0000\n",
      "  Batch 500/661, Loss: 0.0000\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[h264 @ 0x5fb111c0] mb_type 104 in P slice too large at 98 31\n",
      "[h264 @ 0x5fb111c0] error while decoding MB 98 31\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 510/661, Loss: 0.0000\n",
      "  Batch 520/661, Loss: 0.0000\n",
      "  Batch 530/661, Loss: 0.0000\n",
      "  Batch 540/661, Loss: 0.0000\n",
      "  Batch 550/661, Loss: 0.0000\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 560/661, Loss: 0.0000\n",
      "  Batch 570/661, Loss: 0.0000\n",
      "  Batch 580/661, Loss: 0.0000\n",
      "  Batch 590/661, Loss: 0.0000\n",
      "  Batch 600/661, Loss: 0.0000\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 610/661, Loss: 0.0000\n",
      "  Batch 620/661, Loss: 0.0000\n",
      "  Batch 630/661, Loss: 0.0000\n",
      "  Batch 640/661, Loss: 0.0000\n",
      "  Batch 650/661, Loss: 0.0003\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 660/661, Loss: 0.0000\n",
      "Starting validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31/1455025713.py:22: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31/4126126155.py:134: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n",
      "/tmp/ipykernel_31/169455789.py:98: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=True):  # Use mixed precision\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 10/661, Loss: 0.0000\n",
      "  Batch 20/661, Loss: 0.0000\n",
      "  Batch 30/661, Loss: 0.0000\n",
      "  Batch 40/661, Loss: 0.0000\n",
      "  Batch 50/661, Loss: 0.0000\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 60/661, Loss: 0.0000\n",
      "  Batch 70/661, Loss: 0.0000\n",
      "  Batch 80/661, Loss: 0.0000\n",
      "  Batch 90/661, Loss: 0.0000\n",
      "  Batch 100/661, Loss: 0.0000\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 110/661, Loss: 0.0000\n",
      "  Batch 120/661, Loss: 0.0000\n",
      "  Batch 130/661, Loss: 0.9170\n",
      "  Batch 140/661, Loss: 0.0000\n",
      "  Batch 150/661, Loss: 0.0000\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 160/661, Loss: 0.0000\n",
      "  Batch 170/661, Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[h264 @ 0x5fbcd5c0] mb_type 104 in P slice too large at 98 31\n",
      "[h264 @ 0x5fbcd5c0] error while decoding MB 98 31\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 180/661, Loss: 0.0001\n",
      "  Batch 190/661, Loss: 0.0000\n",
      "  Batch 200/661, Loss: 0.0000\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 210/661, Loss: 0.0000\n",
      "  Batch 220/661, Loss: 0.0000\n",
      "  Batch 230/661, Loss: 0.0000\n",
      "  Batch 240/661, Loss: 0.0000\n",
      "  Batch 250/661, Loss: 0.0000\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 260/661, Loss: 0.0000\n",
      "  Batch 270/661, Loss: 0.0000\n",
      "  Batch 280/661, Loss: 0.0000\n",
      "  Batch 290/661, Loss: 0.0000\n",
      "  Batch 300/661, Loss: 0.0000\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 310/661, Loss: 0.0000\n",
      "  Batch 320/661, Loss: 0.0000\n",
      "  Batch 330/661, Loss: 0.7705\n",
      "  Batch 340/661, Loss: 0.0000\n",
      "  Batch 350/661, Loss: 0.0000\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 360/661, Loss: 0.0000\n",
      "  Batch 370/661, Loss: 0.0000\n",
      "  Batch 380/661, Loss: 0.0000\n",
      "  Batch 390/661, Loss: 0.0000\n",
      "  Batch 400/661, Loss: 0.0004\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 410/661, Loss: 0.0000\n",
      "  Batch 420/661, Loss: 0.0000\n",
      "  Batch 430/661, Loss: 0.0000\n",
      "  Batch 440/661, Loss: 0.0000\n",
      "  Batch 450/661, Loss: 0.0000\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 460/661, Loss: 0.0000\n",
      "  Batch 470/661, Loss: 0.0000\n",
      "  Batch 480/661, Loss: 0.0000\n",
      "  Batch 490/661, Loss: 0.0001\n",
      "  Batch 500/661, Loss: 0.0001\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 510/661, Loss: 0.0000\n",
      "  Batch 520/661, Loss: 0.0000\n",
      "  Batch 530/661, Loss: 0.0001\n",
      "  Batch 540/661, Loss: 0.0000\n",
      "  Batch 550/661, Loss: 0.0000\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 560/661, Loss: 0.0000\n",
      "  Batch 570/661, Loss: 0.0000\n",
      "  Batch 580/661, Loss: 0.0003\n",
      "  Batch 590/661, Loss: 0.0001\n",
      "  Batch 600/661, Loss: 0.0001\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n",
      "  Batch 610/661, Loss: 0.0003\n",
      "  Batch 620/661, Loss: 0.0001\n",
      "  Batch 630/661, Loss: 0.0000\n",
      "  Batch 640/661, Loss: 0.0000\n",
      "  Batch 650/661, Loss: 0.0000\n",
      "GPU Memory: Total=14.74GB | Reserved=5.54GB | Allocated=0.52GB | Free=9.20GB\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def create_results_zip():\n",
    "    \"\"\"\n",
    "    Create a ZIP file with all results and visualizations.\n",
    "    \"\"\"\n",
    "    import zipfile\n",
    "\n",
    "    # Files to include in the ZIP\n",
    "    result_files = [\n",
    "        'best_violence_detector.pth',\n",
    "        'training_history.png',\n",
    "        'confusion_matrix.png',\n",
    "        'roc_curve.png',\n",
    "        # Add any other files you want to include\n",
    "    ]\n",
    "\n",
    "    # Create the ZIP file\n",
    "    with zipfile.ZipFile('results.zip', 'w') as zipf:\n",
    "        for file in result_files:\n",
    "            if os.path.exists(file):\n",
    "                zipf.write(file)\n",
    "\n",
    "    print(\"Results ZIP file created successfully.\")\n",
    "\n",
    "import zipfile\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "def main():\n",
    "    print(\"Preparing dataset...\")\n",
    "    (\n",
    "        video_paths_train, video_paths_test, \n",
    "        labels_train, labels_test,\n",
    "        holdout_paths, holdout_labels\n",
    "    ) = prepare_dataset()\n",
    "\n",
    "    # Create ZIP file for holdout videos\n",
    "    print(\"\\nCreating holdout videos archive...\")\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    zip_filename = f\"holdout_videos_{timestamp}.zip\"\n",
    "    \n",
    "    with zipfile.ZipFile(zip_filename, 'w') as zipf:\n",
    "        for video_path in holdout_paths:\n",
    "            zipf.write(video_path, os.path.basename(video_path))\n",
    "    print(f\"Saved holdout videos to {zip_filename}\")\n",
    "\n",
    "    print(\"\\nCreating data loaders...\")\n",
    "    train_loader, test_loader = create_data_loaders(\n",
    "        video_paths_train, labels_train,\n",
    "        video_paths_test, labels_test\n",
    "    )\n",
    "\n",
    "    print(\"Initializing model...\")\n",
    "    model = ViolenceDetectionModel()\n",
    "\n",
    "    # Print model summary\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"\\nModel Summary:\")\n",
    "    print(f\"Total parameters: {total_params:,}\")\n",
    "    print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "\n",
    "    # Check initial GPU memory usage\n",
    "    print_gpu_memory()\n",
    "\n",
    "    print(\"\\nStarting training...\")\n",
    "    history = train(model, train_loader, test_loader)\n",
    "    print(\"Training complete!\")\n",
    "\n",
    "    # Load best model for evaluation\n",
    "    print(\"\\nEvaluating best model on test set...\")\n",
    "    best_model = ViolenceDetectionModel()\n",
    "    checkpoint = torch.load('best_violence_detector.pth')\n",
    "    best_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    best_model.to(device)\n",
    "\n",
    "    # Evaluate on main test set\n",
    "    test_metrics = evaluate(best_model, test_loader)\n",
    "    print(f\"\\nMain Test Set Performance:\")\n",
    "    print(f\"Accuracy: {test_metrics['accuracy']:.2%}\")\n",
    "    print(f\"F1 Score: {test_metrics['f1']:.4f}\")\n",
    "    print(f\"ROC AUC: {test_metrics['roc']['auc']:.4f}\")\n",
    "\n",
    "    # Evaluate on holdout set\n",
    "    print(\"\\nEvaluating on holdout set...\")\n",
    "    holdout_loader = create_holdout_loader(holdout_paths, holdout_labels)\n",
    "    holdout_metrics = evaluate(best_model, holdout_loader)\n",
    "    print(f\"\\nHoldout Set Performance:\")\n",
    "    print(f\"Accuracy: {holdout_metrics['accuracy']:.2%}\")\n",
    "    print(f\"F1 Score: {holdout_metrics['f1']:.4f}\")\n",
    "    print(f\"ROC AUC: {holdout_metrics['roc']['auc']:.4f}\")\n",
    "\n",
    "    # Save comprehensive report\n",
    "    print(\"\\nSaving results...\")\n",
    "    save_results(\n",
    "        history=history,\n",
    "        test_metrics=test_metrics,\n",
    "        holdout_metrics=holdout_metrics,\n",
    "        model=best_model,\n",
    "        zip_filename=zip_filename\n",
    "    )\n",
    "\n",
    "    # Plot results\n",
    "    plot_training_history(history)\n",
    "    plot_comparison(test_metrics, holdout_metrics)\n",
    "    plot_confusion_matrix(holdout_metrics['confusion_matrix'], title=\"Holdout Set Confusion Matrix\")\n",
    "\n",
    "    return best_model, history, (test_metrics, holdout_metrics)\n",
    "\n",
    "def create_holdout_loader(video_paths, labels):\n",
    "    \"\"\"Create a DataLoader for the holdout set\"\"\"\n",
    "    holdout_dataset = VideoDataset(video_paths, labels)\n",
    "    return DataLoader(\n",
    "        holdout_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=False,\n",
    "        num_workers=NUM_WORKERS\n",
    "    )\n",
    "\n",
    "def save_results(history, test_metrics, holdout_metrics, model, zip_filename):\n",
    "    \"\"\"Save all results to a structured directory\"\"\"\n",
    "    results_dir = \"training_results\"\n",
    "    os.makedirs(results_dir, exist_ok=True)\n",
    "    \n",
    "    # Save metrics\n",
    "    with open(f\"{results_dir}/performance_report.txt\", \"w\") as f:\n",
    "        f.write(\"=== MAIN TEST SET ===\\n\")\n",
    "        f.write(f\"Accuracy: {test_metrics['accuracy']:.2%}\\n\")\n",
    "        f.write(f\"F1: {test_metrics['f1']:.4f}\\n\")\n",
    "        f.write(f\"ROC AUC: {test_metrics['roc']['auc']:.4f}\\n\\n\")\n",
    "        f.write(\"=== HOLDOUT SET ===\\n\")\n",
    "        f.write(f\"Accuracy: {holdout_metrics['accuracy']:.2%}\\n\")\n",
    "        f.write(f\"F1: {holdout_metrics['f1']:.4f}\\n\")\n",
    "        f.write(f\"ROC AUC: {holdout_metrics['roc']['auc']:.4f}\\n\")\n",
    "    \n",
    "    # Save classification reports\n",
    "    pd.DataFrame(test_metrics['report']).to_csv(f\"{results_dir}/test_set_classification_report.csv\")\n",
    "    pd.DataFrame(holdout_metrics['report']).to_csv(f\"{results_dir}/holdout_set_classification_report.csv\")\n",
    "    \n",
    "    # Save confusion matrices\n",
    "    np.savetxt(f\"{results_dir}/test_set_confusion_matrix.csv\", test_metrics['confusion_matrix'], delimiter=\",\")\n",
    "    np.savetxt(f\"{results_dir}/holdout_set_confusion_matrix.csv\", holdout_metrics['confusion_matrix'], delimiter=\",\")\n",
    "    \n",
    "    # Copy holdout videos zip\n",
    "    shutil.copy(zip_filename, results_dir)\n",
    "    \n",
    "    print(f\"All results saved to {results_dir}/\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    best_model, training_history, metrics = main()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 176381,
     "sourceId": 397693,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5764601,
     "sourceId": 9477810,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 2151340,
     "sourceId": 7312675,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
